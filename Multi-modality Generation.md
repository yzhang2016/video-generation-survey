### [Awesome-Multimodal-Large-Language-Models](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)

###  LLM 

[arxiv 2024.12] Training Large Language Models to Reason in a Continuous Latent Space  [[PDF](https://arxiv.org/pdf/2412.06769)]


[arxiv 2025.08]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 



## Feedback 
[arxiv 2025.02] DAMO: Data- and Model-aware Alignment of Multi-modal LLMs  [[PDF](https://arxiv.org/abs/2502.01943),[Page](https://github.com/injadlu/DAMO)] ![Code](https://img.shields.io/github/stars/injadlu/DAMO?style=social&label=Star) 

[arxiv 2025.02]  MM-RLHF: The Next Step Forward in Multimodal LLM Alignment [[PDF](https://arxiv.org/abs/2502.10391),[Page](https://mm-rlhf.github.io/)] ![Code](https://img.shields.io/github/stars/Kwai-YuanQi/MM-RLHF?style=social&label=Star) 

[arxiv 2025.02] RE-ALIGN: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization  [[PDF](https://arxiv.org/abs/2502.13146),[Page](https://github.com/taco-group/Re-Align)] ![Code](https://img.shields.io/github/stars/taco-group/Re-Align?style=social&label=Star) 

[arxiv 2025.02] OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference  [[PDF](https://arxiv.org/abs/2502.18411),[Page](https://github.com/PhoenixZ810/OmniAlign-V)] ![Code](https://img.shields.io/github/stars/PhoenixZ810/OmniAlign-V?style=social&label=Star) 

[arxiv 2025.03] Aligning Multimodal LLM with Human Preference: A Survey [[PDF](https://arxiv.org/abs/2503.14504),[Page](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment)] ![Code](https://img.shields.io/github/stars/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment?style=social&label=Star) 

[arxiv 2025.05] Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning  [[PDF](https://arxiv.org/abs/2505.03318),[Page](https://codegoat24.github.io/UnifiedReward/think)] ![Code](https://img.shields.io/github/stars/CodeGoat24/UnifiedReward?style=social&label=Star) 

[arxiv 2025.05] Skywork-VL Reward: An Effective Reward Model for Multimodal Understanding and Reasoning  [[PDF](https://arxiv.org/abs/2505.07263),[Page](https://huggingface.co/Skywork/Skywork-VL-Reward-7B)] 

[arxiv 2025.05]  One RL to See Them All: Visual Triple Unified Reinforcement Learning [[PDF](https://arxiv.org/abs/2505.18129),[Page](https://github.com/MiniMax-AI/One-RL-to-See-Them-All)] ![Code](https://img.shields.io/github/stars/MiniMax-AI/One-RL-to-See-Them-All?style=social&label=Star) 

[arxiv 2025.06] LeanPO: Lean Preference Optimization for Likelihood Alignment in Video-LLMs  [[PDF](https://arxiv.org/abs/2506.05260),[Page](https://github.com/Wang-Xiaodong1899/LeanPO)] ![Code](https://img.shields.io/github/stars/Wang-Xiaodong1899/LeanPO?style=social&label=Star) 

[arxiv 2025.06]  Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of LLMs [[PDF](https://arxiv.org/abs/2506.10054)] ![Code](https://img.shields.io/github/stars/pspdada/Omni-DPO?style=social&label=Star) 

[arxiv 2025.08]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 



## Agent

[arxiv 2024.10] Agent S: An Open Agentic Framework that Uses Computers Like a Human[[PDF](https://arxiv.org/abs/2410.08164), [Page](https://github.com/simular-ai/Agent-S)]

[arxiv 2024.12]  SPAgent: Adaptive Task Decomposition and Model Selection for General Video Generation and Editing [[PDF](https://arxiv.org/abs/2411.18983)]

[arxiv 2024.12] TeamCraft: A Benchmark for Multi-Modal Multi-Agent Systems in Minecraft  [[PDF](https://arxiv.org/abs/2412.05255),[Page](https://teamcraft-bench.github.io/)] ![Code](https://img.shields.io/github/stars/teamcraft-bench/teamcraft?style=social&label=Star) 

[arxiv 2025.01] UnrealZoo: Enriching Photo-realistic Virtual Worlds for Embodied AI  [[PDF](https://arxiv.org/abs/2412.20977),[Page](http://unrealzoo.site/)] ![Code](https://img.shields.io/github/stars/UnrealZoo/unrealzoo-gym?style=social&label=Star) 

[arxiv 2025.02] Magma: A Foundation Model for Multimodal AI Agents  [[PDF](https://www.arxiv.org/pdf/2502.13130),[Page](https://microsoft.github.io/Magma/)] ![Code](https://img.shields.io/github/stars/microsoft/Magma?style=social&label=Star) 

[arxiv 2025.02] PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC  [[PDF](https://arxiv.org/pdf/2502.14282)] 

[arxiv 2025.03]  STEVE: AStep Verification Pipeline for Computer-use Agent Training [[PDF](https://arxiv.org/abs/2503.12532),[Page](https://github.com/FanbinLu/STEVE-R1)] ![Code](https://img.shields.io/github/stars/FanbinLu/STEVE-R1?style=social&label=Star) 

[arxiv 2025.08]  TurboTrain: Towards Efficient and Balanced Multi-Task Learning for Multi-Agent Perception and Prediction [[PDF](https://arxiv.org/abs/2508.04682),[Page](https://github.com/ucla-mobility/TurboTrain)] ![Code](https://img.shields.io/github/stars/ucla-mobility/TurboTrain?style=social&label=Star) 

[arxiv 2025.08]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 


## Understanding 

[arxiv 2023.07]Generative Pretraining in Multimodality [[PDF](https://arxiv.org/abs/2307.05222),[Page](https://github.com/baaivision/Emu)]

[arxiv 2023.07]Generating Images with Multimodal Language Models [[PDF](https://arxiv.org/abs/2305.17216),[Page](https://jykoh.com/gill)]

[arxiv 2023.07]3D-LLM: Injecting the 3D World into Large Language Models[[PDF] (https://arxiv.org/abs/2307.12981),[Page](https://vis-www.cs.umass.edu/3dllm/)]

[arxiv 2023.10]Making LLaMA SEE and Draw with SEED Tokenizer [[PDF](https://arxiv.org/abs/2310.01218),[Page](https://github.com/AILab-CVC/SEED)]

[arxiv 2023.10]Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation [[PDf](https://arxiv.org/abs/2310.08541),[Page](https://idea2img.github.io/)]

[arxiv 2023.12]CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation[[PDF](https://arxiv.org/abs/2311.18775),[Page](https://codi-2.github.io/)]

[arxiv 2023.12]Massively Multimodal Masked Modeling [[PDF](https://arxiv.org/abs/2312.06647),[Page](https://4m.epfl.ch/)]

[arxiv 2023.12]Gemini: A Family of Highly Capable Multimodal Models [[PDF](https://arxiv.org/abs/2312.11805)]

[arxiv 2023.12]Generative Multimodal Models are In-Context Learners [[PDF](https://arxiv.org/abs/2312.13286),[Page](https://baaivision.github.io/emu2)]

[arxiv 2024.03]LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models [[PDF](https://arxiv.org/abs/2403.15388),[Page](https://llava-prumerge.github.io/)]

[arxiv 2024.06]The Evolution of Multimodal Model Architectures [[PDF](https://arxiv.org/pdf/2405.17927)]

[arxiv 2024.09]EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions[[PDF](https://arxiv.org/abs/2409.18042), [Page](https://emova-ollm.github.io/)]

[arxiv 2024.09] Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models[[PDF](https://arxiv.org/abs/2409.17146),)]

[arxiv 2024.09] Visual Prompting in Multimodal Large Language Models: A Survey[[PDF](https://arxiv.org/abs/2409.15310)]

[arxiv 2024.10]Baichuan-Omni Technical Report [[PDF](https://arxiv.org/abs/2410.08565), [Page](https://github.com/westlake-baichuan-mllm/bc-omni)]

[arxiv 2024.10]TemporalBench Benchmarking Fine-grained Temporal Understanding for Multimodal Video Models [[PDF](https://arxiv.org/abs/2410.10818), [Page](https://temporalbench.github.io/)]

[arxiv 2024.10] γ−MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models[[PDF](https://arxiv.org/abs/2410.13859), [Page](https://github.com/Yaxin9Luo/Gamma-MOD)]

[arxiv 2024.10]Remember, Retrieve and Generate: Understanding Infinite Visual Concepts as Your Personalized Assistant [[PDF](https://arxiv.org/abs/2410.13360), [Page](https://github.com/Hoar012/RAP-MLLM)]

[arxiv 2024.11] Multimodal Alignment and Fusion: A Survey [[PDF](https://arxiv.org/abs/2411.17040)]

[arxiv 2024.11] LLaVA-o1: Let Vision Language Models Reason Step-by-Step[[PDF](https://arxiv.org/abs/2411.10440), [Page]()]

[arxiv 2024.11] CATCH: Complementary Adaptive Token-level Contrastive Decoding to Mitigate Hallucinations in LVLMs[[PDF](https://arxiv.org/abs/2411.12713)]

[arxiv 2024.12]  VisionZip: Longer is Better but Not Necessary in Vision Language Models [[PDF](https://arxiv.org/abs/2412.04467),[Page](https://github.com/dvlab-research/VisionZip)] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 

[arxiv 2024.12]  NVILA: Efficient Frontier Visual Language Models [[PDF](https://arxiv.org/abs/2412.04468)] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 

[arxiv 2024.12]  Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling [[PDF](https://arxiv.org/abs/2412.05271),[Page](https://internvl.github.io/blog/)] ![Code](https://img.shields.io/github/stars//OpenGVLab/InternVL?style=social&label=Star) 

[arxiv 2024.12]  Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces [[PDF](https://arxiv.org/abs/2412.14171),[Page](https://vision-x-nyu.github.io/thinking-in-space.github.io/)] ![Code](https://img.shields.io/github/stars/vision-x-nyu/thinking-in-space?style=social&label=Star) 

[arxiv 2024.12] OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving  [[PDF](https://arxiv.org/abs/2412.15208),[Page](https://github.com/taco-group/OpenEMMA)] ![Code](https://img.shields.io/github/stars/taco-group/OpenEMMA?style=social&label=Star) 

[arxiv 2024.12]  VLM-AD: End-to-End Autonomous Driving through Vision-Language Model Supervision [[PDF](https://arxiv.org/abs/2412.14446)]

[arxiv 2024.12]  HoVLE: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding [[PDF](https://arxiv.org/abs/2412.16158),[Page](https://huggingface.co/OpenGVLab/HoVLE)] 

[arxiv 2024.12] Multimodal Latent Language Modeling with Next-Token Diffusion  [[PDF](https://arxiv.org/abs/2412.08635),[Page](https://aka.ms/GeneralAI)] 

[arxiv 2024.12] InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions  [[PDF](https://arxiv.org/abs/2412.09596),[Page](https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-OmniLive)] ![Code](https://img.shields.io/github/stars/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-OmniLive?style=social&label=Star) 


[arxiv 2025.01]  VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM [[PDF](http://arxiv.org/abs/2501.00599),[Page](https://damo-nlp-sg.github.io/VideoRefer/)] ![Code](https://img.shields.io/github/stars/DAMO-NLP-SG/VideoRefer?style=social&label=Star) 

[arxiv 2025.01] VideoChat-Flash: Hierarchical Compression for Long-Context Video Modeling  [[PDF](https://arxiv.org/abs/2501.00574),[Page](https://github.com/OpenGVLab/VideoChat-Flash)] ![Code](https://img.shields.io/github/stars/OpenGVLab/VideoChat-Flash?style=social&label=Star) 

[arxiv 2025.01] VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction  [[PDF](https://arxiv.org/abs/2501.01957),[Page](https://github.com/VITA-MLLM/VITA)] ![Code](https://img.shields.io/github/stars/VITA-MLLM/VITA?style=social&label=Star) 

[arxiv 2025.01] Virgo: A Preliminary Exploration on Reproducing o1-like MLLM  [[PDF](https://arxiv.org/abs/2501.01904),[Page](https://github.com/RUCAIBox/Virgo)] ![Code](https://img.shields.io/github/stars/RUCAIBox/Virgo?style=social&label=Star) 

[arxiv 2025.01]  Scaling of Search and Learning: A Roadmap to Reproduce o1from Reinforcement Learning Perspective [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 

[arxiv 2025.01]  Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction [[PDF](https://arxiv.org/abs/2501.03218),[Page](https://github.com/Mark12Ding/Dispider)] ![Code](https://img.shields.io/github/stars/Mark12Ding/Dispider?style=social&label=Star) 

[arxiv 2025.01] LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs  [[PDF](https://arxiv.org/abs/2501.06186),[Page](https://mbzuai-oryx.github.io/LlamaV-o1/)] ![Code](https://img.shields.io/github/stars/mbzuai-oryx/LlamaV-o1?style=social&label=Star) 

[arxiv 2025.01] VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding  [[PDF](https://arxiv.org/abs/2501.13106),[Page](https://github.com/DAMO-NLP-SG/VideoLLaMA3)] ![Code](https://img.shields.io/github/stars/DAMO-NLP-SG/VideoLLaMA3?style=social&label=Star) 

[arxiv 2025.01]  Next Token Prediction Towards Multimodal Intelligence[[PDF](https://arxiv.org/abs/2412.18619),[Page](https://github.com/LMM101/Awesome-Multimodal-Next-Token-Prediction)] ![Code](https://img.shields.io/github/stars/LMM101/Awesome-Multimodal-Next-Token-Prediction?style=social&label=Star) 

[arxiv 2025.01] MiniMax-01: Scaling Foundation Models with Lightning Attention  [[PDF](https://arxiv.org/abs/2501.08313),[Page](https://github.com/MiniMax-AI/MiniMax-01)] ![Code](https://img.shields.io/github/stars/MiniMax-AI/MiniMax-01?style=social&label=Star) 

[arxiv 2025.02] MINT: Mitigating Hallucinations in Large Vision-Language Models via Token Reduction  [[PDF](https://arxiv.org/abs/2502.00717)]

[arxiv 2025.02] Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with Leading Short-Context Accuray  [[PDF](https://arxiv.org/abs/2502.05177),[Page](https://github.com/VITA-MLLM/Long-VITA)] ![Code](https://img.shields.io/github/stars/VITA-MLLM/Long-VITA?style=social&label=Star) 

[arxiv 2025.02]  PixelWorld: Towards Perceiving Everything as Pixels [[PDF](https://arxiv.org/abs/2501.19339),[Page](https://tiger-ai-lab.github.io/PixelWorld/)] ![Code](https://img.shields.io/github/stars/TIGER-AI-Lab/PixelWorld?style=social&label=Star) 

[arxiv 2025.02] Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment  [[PDF](https://arxiv.org/abs/2502.04328),[Page](https://ola-omni.github.io/)] ![Code](https://img.shields.io/github/stars/Ola-Omni/Ola?style=social&label=Star) 

[arxiv 2025.02] video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model  [[PDF](https://arxiv.org/pdf/2502.11775),[Page](https://github.com/BriansIDP/video-SALMONN-o1)] ![Code](https://img.shields.io/github/stars/BriansIDP/video-SALMONN-o1?style=social&label=Star) 

[arxiv 2025.02] Qwen2.5-VL Technical Report  [[PDF](https://arxiv.org/abs/2502.13923),[Page](https://github.com/QwenLM/Qwen2.5-VL)] ![Code](https://img.shields.io/github/stars/QwenLM/Qwen2.5-VL?style=social&label=Star) 

[arxiv 2025.02]  Introducing Visual Perception Token into Multimodal Large Language Model [[PDF](https://arxiv.org/abs/2502.17425),[Page](https://github.com/yu-rp/VisualPerceptionToken)] ![Code](https://img.shields.io/github/stars/yu-rp/VisualPerceptionToken?style=social&label=Star) 

[arxiv 2025.02] MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs  [[PDF](https://arxiv.org/abs/2502.17422),[Page](https://github.com/saccharomycetes/mllms_know)] ![Code](https://img.shields.io/github/stars/saccharomycetes/mllms_know?style=social&label=Star) 

[arxiv 2025.03]  Visual-RFT: Visual Reinforcement Fine-Tuning [[PDF](https://arxiv.org/pdf/2503.01785),[Page](https://github.com/Liuziyu77/Visual-RFT)] ![Code](https://img.shields.io/github/stars/Liuziyu77/Visual-RFT?style=social&label=Star) 

[arxiv 2025.03]  NEXUS-O: AN OMNI-PERCEPTIVE AND -INTERACTIVE MODEL FOR LANGUAGE, AUDIO, AND VISION [[PDF](https://arxiv.org/pdf/2503.01879)]

[arxiv 2025.03] Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models  [[PDF](https://arxiv.org/pdf/2503.06749),[Page](https://github.com/Osilly/Vision-R1)] ![Code](https://img.shields.io/github/stars/Osilly/Vision-R1?style=social&label=Star) 

[arxiv 2025.03] Qwen2.5-Omni Technical Report  [[PDF](https://arxiv.org/abs/2503.20215)]

[arxiv 2025.04]  Shot-by-Shot: Film-Grammar-Aware Training-Free Audio Description Generation [[PDF](https://arxiv.org/abs/2504.01020),[Page](https://www.robots.ox.ac.uk/vgg/research/shot-by-shot/)] ![Code](https://img.shields.io/github/stars/Jyxarthur/shot-by-shot?style=social&label=Star) 

[arxiv 2025.04]  Aligned Better, Listen Better for Audio-Visual Large Language Models [[PDF](https://arxiv.org/abs/2504.02061),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 

[arxiv 2025.04] TokenFLEX: Unified VLM Training for Flexible Visual Tokens Inference  [[PDF](https://arxiv.org/abs/2504.03154)]

[arxiv 2025.04]  Kimi-VL Technical Report [[PDF](https://arxiv.org/abs/2504.07491),[Page](https://github.com/MoonshotAI/Kimi-VL)] ![Code](https://img.shields.io/github/stars//MoonshotAI/Kimi-VL?style=social&label=Star) 

[arxiv 2025.04]  InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models [[PDF](https://arxiv.org/abs/2504.10479),[Page](https://github.com/OpenGVLab/InternVL)] ![Code](https://img.shields.io/github/stars/OpenGVLab/InternVL?style=social&label=Star) 

[arxiv 2025.04]  VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference Optimization for Large Video Models [[PDF](https://arxiv.org/abs/2504.13122),[Page](https://github.com/HaroldChen19/VistaDPO)] ![Code](https://img.shields.io/github/stars/HaroldChen19/VistaDPO?style=social&label=Star) 

[arxiv 2025.04]  Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models [[PDF](https://arxiv.org/pdf/2504.15271),[Page](https://nvlabs.github.io/EAGLE/)] ![Code](https://img.shields.io/github/stars/NVlabs/EAGLE?style=social&label=Star) 

[arxiv 2025.05]  Seed1.5-VL Technical Report [[PDF](https://arxiv.org/abs/2505.07062)]

[arxiv 2025.06] HaploVL - A Single-Transformer Baseline for Multi-Modal Understanding  [[PDF](http://arxiv.org/abs/2503.14694),[Page](https://github.com/Tencent/HaploVLM)] ![Code](https://img.shields.io/github/stars/Tencent/HaploVLM?style=social&label=Star) 

[arxiv 2025.07] Kwai Keye-VL Technical Report  [[PDF](https://arxiv.org/abs/2507.01949),[Page](https://github.com/Kwai-Keye/Keye)] ![Code](https://img.shields.io/github/stars/Kwai-Keye/Keye?style=social&label=Star) 

[arxiv 2025.07]  Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal Large Language Models [[PDF](https://arxiv.org/abs/2507.12566),[Page](https://github.com/OpenGVLab/Mono-InternVL)] ![Code](https://img.shields.io/github/stars/OpenGVLab/Mono-InternVL?style=social&label=Star) 

[arxiv 2025.07] Meta CLIP 2: A Worldwide Scaling Recipe  [[PDF](https://arxiv.org/abs/2507.22062),[Page](https://github.com/facebookresearch/MetaCLIP)] ![Code](https://img.shields.io/github/stars/facebookresearch/MetaCLIP?style=social&label=Star) 

[arxiv 2025.08]  Ovis2.5 Technical Report [[PDF](https://arxiv.org/pdf/2508.11737),[Page](https://github.com/AIDC-AI/Ovis)] ![Code](https://img.shields.io/github/stars/AIDC-AI/Ovis?style=social&label=Star) 

[arxiv 2025.08] InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency  [[PDF](https://arxiv.org/abs/2508.18265),[Page](https://github.com/OpenGVLab/InternVL)] ![Code](https://img.shields.io/github/stars/OpenGVLab/InternVL?style=social&label=Star) 


[arxiv 2025.08]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 


## downstream
[arxiv 2025.05]  Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models [[PDF](https://arxiv.org/abs/2505.17015),[Page](https://runsenxu.com/projects/Multi-SpatialMLLM)] ![Code](https://img.shields.io/github/stars/facebookresearch/Multi-SpatialMLLM?style=social&label=Star) 


## Long Video Understanding
[arxiv 2025.02] CoS: Chain-of-Shot Prompting for Long Video Understanding  [[PDF](https://arxiv.org/abs/2502.06428),[Page](https://lwpyh.github.io/CoS/)] ![Code](https://img.shields.io/github/stars/lwpyh/CoS_codes?style=social&label=Star) 

[arxiv 2025.03]  VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning [[PDF](https://arxiv.org/abs/2503.13444),[Page](https://videomind.github.io/)] ![Code](https://img.shields.io/github/stars/yeliudev/VideoMind?style=social&label=Star) 

[arxiv 2025.04]  Chapter-Llama: Efficient Chaptering in Hour-Long Videos with LLMs [[PDF](https://arxiv.org/abs/2504.00072),[Page](https://imagine.enpc.fr/~lucas.ventura/chapter-llama/)] ![Code](https://img.shields.io/github/stars/lucas-ventura/chapter-llama/?style=social&label=Star) 

[arxiv 2025.04]  Slow-Fast Architecture for Video Multi-Modal Large Language Models [[PDF](https://arxiv.org/abs/2504.01328),[Page](https://github.com/SHI-Labs/Slow-Fast-Video-Multimodal-LLM)] ![Code](https://img.shields.io/github/stars/SHI-Labs/Slow-Fast-Video-Multimodal-LLM?style=social&label=Star) 

[arxiv 2025.04] TimeSearch: Hierarchical Video Search with Spotlight and Reflection for Human-like Long Video Understanding  [[PDF](https://arxiv.org/abs/2504.01407)]

[arxiv 2025.04] Re-thinking Temporal Search for Long-Form Video Understanding  [[PDF](https://arxiv.org/abs/2504.02259)]

[arxiv 2025.04] VideoAgent2: Enhancing the LLM-Based Agent System for Long-Form Video Understanding by Uncertainty-Aware CoT  [[PDF](https://arxiv.org/abs/2504.04471)]

[arxiv 2025.04] Vidi: Large Multimodal Models for Video Understanding and Editing  [[PDF](https://arxiv.org/pdf/2504.15681),[Page](https://bytedance.github.io/vidi-website/)] ![Code](https://img.shields.io/github/stars/bytedance/vidi?style=social&label=Star) 

[arxiv 2025.05]  CrossLMM: Decoupling Long Video Sequences from LMMs via Dual Cross-Attention Mechanisms [[PDF](https://arxiv.org/abs/2505.17020),[Page](https://github.com/shilinyan99/CrossLMM)] ![Code](https://img.shields.io/github/stars/shilinyan99/CrossLMM?style=social&label=Star) 

[arxiv 2025.05]  QuickVideo: Real-Time Long Video Understanding with System Algorithm Co-Design[[PDF](https://arxiv.org/abs/2505.16175),[Page](https://github.com/TIGER-AI-Lab/QuickVideo)] ![Code](https://img.shields.io/github/stars/TIGER-AI-Lab/QuickVideo?style=social&label=Star) 

[arxiv 2025.05] VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation  [[PDF](https://arxiv.org/abs/2505.14640),[Page](https://tiger-ai-lab.github.io/VideoEval-Pro/home_page.html)] ![Code](https://img.shields.io/github/stars/TIGER-AI-Lab/VideoEval-Pro?style=social&label=Star) 

[arxiv 2025.05] Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding  [[PDF](https://arxiv.org/abs/2505.18079)]

[arxiv 2025.06]  Movie Facts and Fibs (MF2): A Benchmark for Long Movie Understanding [[PDF](https://arxiv.org/abs/2506.06275)]

[arxiv 2025.06] VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos  [[PDF](https://arxiv.org/abs/2506.10857),[Page](https://vrbench.github.io/)] ![Code](https://img.shields.io/github/stars/OpenGVLab/VRBench?style=social&label=Star) 

[arxiv 2025.06]  Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning [[PDF](https://arxiv.org/abs/2506.13654),[Page](https://egolife-ai.github.io/Ego-R1/)] ![Code](https://img.shields.io/github/stars/egolife-ai/Ego-R1?style=social&label=Star) 

[arxiv 2025.06]  Task-Aware KV Compression For Cost-Effective Long Video Understanding [[PDF](https://arxiv.org/abs/2506.21184),[Page](https://github.com/UnableToUseGit/VideoX22L)] ![Code](https://img.shields.io/github/stars/UnableToUseGit/VideoX22L?style=social&label=Star) 

[arxiv 2025.06]  Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV Sparsification [[PDF](https://arxiv.org/abs/2506.19225),[Page](https://unabletousegit.github.io/video-xl2.github.io/)] ![Code](https://img.shields.io/github/stars/VectorSpaceLab/Video-XL?style=social&label=Star) 

[arxiv 2025.07] Flash-VStream: Efficient Real-Time Understanding for Long Video Streams  [[PDF](https://arxiv.org/abs/2506.23825),[Page](https://github.com/IVGSZ/Flash-VStream)] ![Code](https://img.shields.io/github/stars/IVGSZ/Flash-VStream?style=social&label=Star) 

[arxiv 2025.07] ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World Shorts  [[PDF](https://arxiv.org/abs/2507.20939),[Page](https://github.com/TencentARC/ARC-Hunyuan-Video-7B)] ![Code](https://img.shields.io/github/stars/TencentARC/ARC-Hunyuan-Video-7B?style=social&label=Star) 

[arxiv 2025.08] AVATAR: Reinforcement Learning to See, Hear, and Reason Over Video  [[PDF](https://arxiv.org/abs/2508.03100),[Page](https://github.com/yogkul2000/AVATAR)] ![Code](https://img.shields.io/github/stars/yogkul2000/AVATAR?style=social&label=Star) 

[arxiv 2025.08] Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning for Long Video Reasoning  [[PDF](https://arxiv.org/abs/2508.04416)]

[arxiv 2025.08]  StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding [[PDF](https://arxiv.org/pdf/2508.15717),[Page](https://yangyanl.ai/streammem/)] 


[arxiv 2025.08]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 


## Generation 
[arxiv 2023.12]SmartEdit: Exploring Complex Instruction-based Image Editing with Multimodal Large Language Models [[PDF](https://arxiv.org/abs/2312.06739),[Page](https://yuzhou914.github.io/SmartEdit/)]

[arxiv 2023.12]InstructAny2Pix: Flexible Visual Editing via Multimodal Instruction Following [[PDF](https://arxiv.org/abs/2312.06738), [Page](https://github.com/jacklishufan/InstructAny2Pix.git)]

[arxiv 2024.1]DiffusionGPT: LLM-Driven Text-to-Image Generation System [[PDF](https://arxiv.org/abs/2401.10061)]

[arxiv 2024.1]Image Anything: Towards Reasoning-coherent and Training-free Multi-modal Image Generation [[PDF](https://arxiv.org/abs/2401.17664),[Page](https://vlislab22.github.io/ImageAnything/)]

[arxiv 2024.03]3D-VLA 3D Vision-Language-Action Generative World Model[[PDF](https://arxiv.org/abs/2403.09631),[Page](https://vis-www.cs.umass.edu/3dvla/)]

[arxiv 2024.04]SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation [[PDF](https://arxiv.org/abs/2404.14396)]

[arxiv 2024.08] Show-o: One Single Transformer to Unify Multimodal Understanding and Generation[[PDF](https://arxiv.org/abs/2408.12528), [Page](https://github.com/showlab/Show-o)]

[arxiv 2024.09] VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation[[PDF](https://arxiv.org/abs/2409.04429)]


[arxiv 2024.09] Emu3: Next-Token Prediction is All You Need[[PDF](https://arxiv.org/abs/2409.18869), [Page](https://emu.baai.ac.cn/)]

[arxiv 2024.09]MonoFormer: One Transformer for Both Diffusion and Autoregression [[PDF](https://arxiv.org/abs/2409.16280), [Page](https://monoformer.github.io/)]

[arxiv 2024.10] PUMA: Empowering Unified MLLM with Multi-granular Visual Generation[[PDF](https://arxiv.org/abs/2410.13861), [Page](https://rongyaofang.github.io/puma/)]

[arxiv 2024.10]ACE: All-round Creator and Editor Following Instructions via Diffusion Transformer [[PDF](https://arxiv.org/abs/2410.00086), [Page](https://ali-vilab.github.io/ace-page/)]

[arxiv 2024.10]Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation [[PDF](https://arxiv.org/abs/2410.13848), [Page](https://github.com/deepseek-ai/Janus)]

[arxiv 2024.11]Spider: Any-to-Many Multimodal LLM [[PDF](https://arxiv.org/abs/2411.09439)]

[arxiv 2024.12] TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation  [[PDF](https://arxiv.org/abs/2412.03069),[Page](https://byteflow-ai.github.io/TokenFlow/)] ![Code](https://img.shields.io/github/stars/ByteFlow-AI/TokenFlow?style=social&label=Star) 

[arxiv 2025.02]  HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and Generation [[PDF](https://arxiv.org/pdf/2502.12148),[Page](https://github.com/Gen-Verse/HermesFlow)] ![Code](https://img.shields.io/github/stars/Gen-Verse/HermesFlow?style=social&label=Star) 

[arxiv 2025.02]  UniTok: A Unified Tokenizer for Visual Generation and Understanding [[PDF](https://arxiv.org/abs/2502.20321),[Page](https://github.com/FoundationVision/UniTok)] ![Code](https://img.shields.io/github/stars/FoundationVision/UniTok?style=social&label=Star) 

[arxiv 2025.03]  WeGen: A Unified Model for Interactive Multimodal Generation as We Chat [[PDF](https://arxiv.org/pdf/2503.01115),[Page](https://github.com/hzphzp/WeGen)] ![Code](https://img.shields.io/github/stars/hzphzp/WeGen?style=social&label=Star) 

[arxiv 2025.03]  Unified Autoregressive Visual Generation and Understanding with Continuous Tokens [[PDF](https://arxiv.org/pdf/2503.13436)]

[arxiv 2025.03]  GoT: Unleashing Reasoning Capability of Multimodal Large Language Model for Visual Generation and Editing [[PDF](https://arxiv.org/abs/2503.10639),[Page](https://github.com/rongyaofang/GoT)] ![Code](https://img.shields.io/github/stars/rongyaofang/GoT?style=social&label=Star) 

[arxiv 2025.03]  Harmonizing Visual Representations for Unified Multimodal Understanding and Generation [[PDF](https://arxiv.org/abs/2503.21979),[Page](https://github.com/wusize/Harmon)] ![Code](https://img.shields.io/github/stars/wusize/Harmon?style=social&label=Star) 

[arxiv 2025.04]  Transfer between Modalities with MetaQueries [[PDF](https://arxiv.org/abs/2504.06256),[Page](https://xichenpan.com/metaquery/)] 

[arxiv 2025.05] Nexus-Gen: A Unified Model for Image Understanding, Generation, and Editing  [[PDF](https://arxiv.org/abs/2504.21356)]

[arxiv 2025.05]  YoChameleon: Personalized Vision and Language Generation [[PDF](https://arxiv.org/abs/2504.20998),[Page](https://thaoshibe.github.io/YoChameleon)] ![Code](https://img.shields.io/github/stars/WisconsinAIVision/YoChameleon?style=social&label=Star) 

[arxiv 2025.05] X-Fusion: Introducing New Modality to Frozen Large Language Models  [[PDF](https://arxiv.org/abs/2504.20996),[Page](https://sichengmo.github.io/XFusion/)] 

[arxiv 2025.05] Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities  [[PDF](https://arxiv.org/pdf/2505.02567)]

[arxiv 2025.05] Mogao: An Omni Foundation Model for Interleaved Multi-Modal Generation  [[PDF](https://arxiv.org/pdf/2505.05472)]

[arxiv 2025.05]  TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation [[PDF](https://arxiv.org/abs/2505.05422),[Page](https://arxiv.org/abs/2505.05422)] ![Code](https://img.shields.io/github/stars/TencentARC/TokLIP?style=social&label=Star) 

[arxiv 2025.05] Selftok: Discrete Visual Tokens of Autoregression, by Diffusion, and for Reasoning  [[PDF](https://arxiv.org/pdf/2505.07538),[Page](https://selftok-team.github.io/report/)] 

[arxiv 2025.05]  GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning[[PDF](https://arxiv.org/abs/2505.17022),[Page](https://github.com/gogoduan/GoT-R1)] ![Code](https://img.shields.io/github/stars/gogoduan/GoT-R1?style=social&label=Star) 

[arxiv 2025.05] Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO  [[PDF](https://arxiv.org/abs/2505.17017),[Page](https://github.com/ZiyuGuo99/Image-Generation-CoT)] ![Code](https://img.shields.io/github/stars/ZiyuGuo99/Image-Generation-CoT?style=social&label=Star) 

[arxiv 2025.05]  UniGen: Enhanced Training & Test-Time Strategies for Unified Multimodal Understanding and Generation [[PDF](https://arxiv.org/abs/2505.14682)]

[arxiv 2025.05] Co-Reinforcement Learning for Unified Multimodal Understanding and Generation  [[PDF](https://arxiv.org/abs/2505.17534)]

[arxiv 2025.06]  Ming-Omni: A Unified Multimodal Model for Perception and Generation [[PDF](https://arxiv.org/abs/2506.09344),[Page](https://lucaria-academy.github.io/Ming-Omni/)] ![Code](https://img.shields.io/github/stars/inclusionAI/Ming?style=social&label=Star) 

[arxiv 2025.06] Pisces: An Auto-regressive Foundation Model for Image Understanding and Generation  [[PDF](https://arxiv.org/abs/2506.10395)]

[arxiv 2025.06] Show-o2: Improved Native Unified Multimodal Models  [[PDF](https://arxiv.org/abs/2506.15564),[Page](https://github.com/showlab/Show-o)] ![Code](https://img.shields.io/github/stars/showlab/Show-o?style=social&label=Star) 

[arxiv 2025.06]  UniFork: Exploring Modality Alignment for Unified Multimodal Understanding and Generation [[PDF](https://arxiv.org/abs/2506.17202),[Page](https://github.com/tliby/UniFork)] ![Code](https://img.shields.io/github/stars/tliby/UniFork?style=social&label=Star) 

[arxiv 2025.06] OmniGen2: Exploration to Advanced Multimodal Generation  [[PDF](https://arxiv.org/abs/2506.18871),[Page](https://github.com/VectorSpaceLab/OmniGen2)] ![Code](https://img.shields.io/github/stars/VectorSpaceLab/OmniGen2?style=social&label=Star) 

[arxiv 2025.07]  MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation [[PDF](https://arxiv.org/abs/2507.09574),[Page](https://haozhezhao.github.io/MENTOR.page)] ![Code](https://img.shields.io/github/stars/HaozheZhao/MENTOR?style=social&label=Star) 

[arxiv 2025.07]  X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image Generative Models Great Again [[PDF](https://arxiv.org/abs/2507.22058),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 

[arxiv 2025.07]  UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation and Editing [[PDF](https://arxiv.org/abs/2507.23278)]

[arxiv 2025.08] Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding and Generation  [[PDF](https://arxiv.org/abs/2508.03320),[Page](https://github.com/SkyworkAI/UniPic)] ![Code](https://img.shields.io/github/stars/SkyworkAI/UniPic?style=social&label=Star) 

[arxiv 2025.08]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 


## diffusion LLM
[arxiv 2025.05]  MMaDA: Multimodal Large Diffusion Language Models [[PDF](https://arxiv.org/abs/2505.15809),[Page](https://github.com/Gen-Verse/MMaDA)] ![Code](https://img.shields.io/github/stars/Gen-Verse/MMaDA?style=social&label=Star) 

[arxiv 2025.08]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 


## evaluation 
[arxiv 2024.10] The Curse of Multi-Modalities: Evaluating Hallucinations of Large Multimodal Models across Language, Visual, and Audio[[PDF](https://arxiv.org/abs/2410.12787)]


## VLA 
[arxiv 2025.07] EmbRACE-3K: Embodied Reasoning and Action in Complex Environments  [[PDF](https://arxiv.org/pdf/2507.10548),[Page](https://mxllc.github.io/EmbRACE-3K/)] ![Code](https://img.shields.io/github/stars/mxllc/EmbRACE-3K?style=social&label=Star) 

[arxiv 2025.07] EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos  [[PDF](https://arxiv.org/pdf/2507.12440),[Page](https://rchalyang.github.io/EgoVLA/)] 

[arxiv 2025.07] ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning  [[PDF](https://arxiv.org/abs/2507.16815),[Page](https://jasper0314-huang.github.io/thinkact-vla/)] 

[arxiv 2025.08]  Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies [[PDF](https://arxiv.org/abs/2508.20072)】

[arxiv 2025.08]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 


## reasoning

[arxiv 2025.03] Video-R1: Reinforcing Video Reasoning in MLLMs  [[PDF](https://arxiv.org/abs/2503.21776),[Page](https://github.com/tulerfeng/Video-R1)] ![Code](https://img.shields.io/github/stars/tulerfeng/Video-R1?style=social&label=Star) 

[arxiv 2025.04] Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought  [[PDF](https://arxiv.org/abs/2504.05599)]

[arxiv 2025.04]  VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model [[PDF](https://arxiv.org/abs/2504.07615),[Page](https://github.com/om-ai-lab/VLM-R1)] ![Code](https://img.shields.io/github/stars/om-ai-lab/VLM-R1?style=social&label=Star) 

[arxiv 2025.04] VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning  [[PDF](https://arxiv.org/abs/2504.06958),[Page](https://github.com/OpenGVLab/VideoChat-R1)] ![Code](https://img.shields.io/github/stars/OpenGVLab/VideoChat-R1?style=social&label=Star) 

[arxiv 2025.04] Relation-R1: Cognitive Chain-of-Thought Guided Reinforcement Learning for Unified Relational Comprehension  [[PDF](https://arxiv.org/pdf/2504.14642),[Page](https://github.com/HKUST-LongGroup/Relation-R1)] ![Code](https://img.shields.io/github/stars/HKUST-LongGroup/Relation-R1?style=social&label=Star) 

[arxiv 2025.04]  Unsupervised Visual Chain-of-Thought Reasoning via Preference Optimization [[PDF](https://arxiv.org/pdf/2504.18397),[Page](https://kesenzhao.github.io/my_project/projects/UV-CoT.html)] ![Code](https://img.shields.io/github/stars/kesenzhao/UV-CoT?style=social&label=Star) 

[arxiv 2025.05]  R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning [[PDF](https://arxiv.org/abs/2505.02835),[Page](https://arxiv.org/abs/2505.02835)] ![Code](https://img.shields.io/github/stars/yfzhang114/r1_reward?style=social&label=Star) 

[arxiv 2025.05]  EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via Reinforcement Learning [[PDF](https://arxiv.org/abs/2505.04623),[Page](https://github.com/HarryHsing/EchoInk)] ![Code](https://img.shields.io/github/stars/HarryHsing/EchoInk?style=social&label=Star) 

[arxiv 2025.05] SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward  [[PDF](https://arxiv.org/abs/2505.17018),[Page](https://github.com/kxfan2002/SophiaVL-R1)] ![Code](https://img.shields.io/github/stars/kxfan2002/SophiaVL-R1?style=social&label=Star) 

[arxiv 2025.05] R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO  [[PDF](https://arxiv.org/abs/2505.16673),[Page](https://github.com/HJYao00/R1-ShareVL)] ![Code](https://img.shields.io/github/stars/HJYao00/R1-ShareVL?style=social&label=Star) 

[arxiv 2025.05]  STAR-R1: Spacial TrAnsformation Reasoning by Reinforcing Multimodal LLMs [[PDF](https://arxiv.org/abs/2505.15804),[Page](https://github.com/zongzhao23/STAR-R1)] ![Code](https://img.shields.io/github/stars/zongzhao23/STAR-R1?style=social&label=Star) 

[arxiv 2025.05]  Visual Thoughts: A Unified Perspective of Understanding Multimodal Chain-of-Thought [[PDF](https://arxiv.org/abs/2505.15510)]

[arxiv 2025.05] Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL  [[PDF](https://arxiv.org/abs/2505.15436)]

[arxiv 2025.05]  Visionary-R1: Mitigating Shortcuts in Visual Reasoning with Reinforcement Learning [[PDF](https://arxiv.org/abs/2505.14677),[Page](https://github.com/maifoundations/Visionary-R1)] ![Code](https://img.shields.io/github/stars/maifoundations/Visionary-R1?style=social&label=Star) 

[arxiv 2025.05]  VisionReasoner: Unified Visual Perception and Reasoning via Reinforcement Learning [[PDF](https://arxiv.org/abs/2505.12081),[Page](https://github.com/dvlab-research/VisionReasoner)] ![Code](https://img.shields.io/github/stars/dvlab-research/VisionReasoner?style=social&label=Star) 

[arxiv 2025.05]  Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner [[PDF](https://arxiv.org/abs/2505.11404),[Page](https://github.com/Wenchuan-Zhang/Patho-R1)] ![Code](https://img.shields.io/github/stars/Wenchuan-Zhang/Patho-R1?style=social&label=Star) 

[arxiv 2025.06]  Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks [[PDF](https://arxiv.org/abs/2505.24876),[Page](https://github.com/mbzuai-oryx/Agent-X)] ![Code](https://img.shields.io/github/stars/mbzuai-oryx/Agent-X?style=social&label=Star) 

[arxiv 2025.06]  SiLVR: A Simple Language-based Video Reasoning Framework [[PDF](https://arxiv.org/abs/2505.24869),[Page](https://sites.google.com/cs.unc.edu/silvr)] ![Code](https://img.shields.io/github/stars/CeeZh/SILVR?style=social&label=Star) 

[arxiv 2025.06] Video-Skill-CoT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video Reasoning  [[PDF](https://arxiv.org/abs/2506.03525),[Page](https://video-skill-cot.github.io/)] ![Code](https://img.shields.io/github/stars/daeunni/Video-Skill-CoT?style=social&label=Star) 

[arxiv 2025.06]  MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical Chain-of-Thought Reasoning [[PDF](https://arxiv.org/abs/2506.05331),[Page](https://github.com/xinyan-cxy/MINT-CoT)] ![Code](https://img.shields.io/github/stars/xinyan-cxy/MINT-CoT?style=social&label=Star) 

[arxiv 2025.06] VideoChat-A1: Thinking with Long Videos by Chain-of-Shot Reasoning  [[PDF](https://arxiv.org/abs/2506.06097)]

[arxiv 2025.06] Play to Generalize: Learning to Reason Through Game Play  [[PDF](https://arxiv.org/abs/2506.08011),[Page](https://github.com/yunfeixie233/ViGaL)] ![Code](https://img.shields.io/github/stars/yunfeixie233/ViGaL?style=social&label=Star) 

[arxiv 2025.06]  DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO [[PDF](https://arxiv.org/abs/2506.07464)]

[arxiv 2025.06] MMSearch-R1: Incentivizing LMMs to Search  [[PDF](https://arxiv.org/abs/2506.20670),[Page](https://github.com/EvolvingLMMs-Lab/multimodal-search-r1)] ![Code](https://img.shields.io/github/stars/EvolvingLMMs-Lab/multimodal-search-r1?style=social&label=Star) 

[arxiv 2025.06]  MiCo: Multi-image Contrast for Reinforcement Visual Reasoning [[PDF](https://arxiv.org/pdf/2506.22434)]

[arxiv 2025.07]  GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning [[PDF](https://arxiv.org/abs/2507.01006),[Page](https://github.com/THUDM/GLM-4.1V-Thinking)] ![Code](https://img.shields.io/github/stars/THUDM/GLM-4.1V-Thinking?style=social&label=Star) 

[arxiv 2025.07] Skywork-R1V3 Technical Report  [[PDF](https://arxiv.org/abs/2507.06167),[Page](https://github.com/SkyworkAI/Skywork-R1V)] ![Code](https://img.shields.io/github/stars/SkyworkAI/Skywork-R1V?style=social&label=Star) 

[arxiv 2025.08]  Thyme: Think Beyond Images [[PDF](https://arxiv.org/abs/2508.11630),[Page](https://thyme-vl.github.io/)] ![Code](https://img.shields.io/github/stars/yfzhang114/Thyme?style=social&label=Star) 

[arxiv 2025.08]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 



## Compression
[arxiv 2025.02]  AdaSVD: Adaptive Singular Value Decomposition for Large Language Models [[PDF](https://arxiv.org/abs/2502.01403),[Page](https://github.com/ZHITENGLI/AdaSVD)] ![Code](https://img.shields.io/github/stars/ZHITENGLI/AdaSVD?style=social&label=Star) 

[arxiv 2025.02]  Vision-centric Token Compression in Large Language Model [[PDF](https://arxiv.org/pdf/2502.00791)]

[arxiv 2025.02] From 16-Bit to 1-Bit: Visual KV Cache Quantization for Memory-Efficient Multimodal Large Language Models  [[PDF](https://arxiv.org/abs/2502.14882)]

[arxiv 2025.03]  Token-Efficient Long Video Understanding for Multimodal LLMs [[PDF](https://arxiv.org/pdf/2503.04130)]

[arxiv 2025.08]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 

## few-shot
[arxiv 2025.02]  Efficient Few-Shot Continual Learning in Vision-Language Models [[PDF](https://arxiv.org/pdf/2502.04098)]


## tokenzier
[arxiv 2025.04] UniViTAR: Unified Vision Transformer with Native Resolution  [[PDF](https://arxiv.org/abs/2504.01792)]

[arxiv 2025.04]  UniToken: Harmonizing Multimodal Understanding and Generation through Unified Visual Encoding [[PDF](https://arxiv.org/abs/2504.04423),[Page](https://github.com/SxJyJay/UniToken)] ![Code](https://img.shields.io/github/stars/SxJyJay/UniToken?style=social&label=Star) 

[arxiv 2025.05]  OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision Encoders for Multimodal Learning [[PDF](https://arxiv.org/abs/2505.04601),[Page](https://ucsc-vlaa.github.io/OpenVision/)] ![Code](https://img.shields.io/github/stars/UCSC-VLAA/OpenVision?style=social&label=Star) 

[arxiv 2025.05] ALTo: Adaptive-Length Tokenizer for Autoregressive Mask Generation  [[PDF](https://arxiv.org/pdf/2505.16495),[Page](https://github.com/yayafengzi/ALToLLM)] ![Code](https://img.shields.io/github/stars/yayafengzi/ALToLLM?style=social&label=Star) 

[arxiv 2025.05] UniCTokens: Boosting Personalized Understanding and Generation via Unified Concept Tokens  [[PDF](https://arxiv.org/abs/2505.14671),[Page](https://github.com/arctanxarc/UniCTokens)] ![Code](https://img.shields.io/github/stars/arctanxarc/UniCTokens?style=social&label=Star) 

[arxiv 2025.05] Slot-MLLM: Object-Centric Visual Tokenization for Multimodal LLM  [[PDF](https://arxiv.org/abs/2505.17726),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 

[arxiv 2025.06] UniWorld-V1: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation  [[PDF](https://arxiv.org/abs/2506.03147),[Page](https://github.com/PKU-YuanGroup/UniWorld-V1)] ![Code](https://img.shields.io/github/stars/PKU-YuanGroup/UniWorld-V1?style=social&label=Star) 

[arxiv 2025.07]  When Tokens Talk Too Much: A Survey of Multimodal Long-Context Token Compression across Images, Videos, and Audios [[PDF](https://arxiv.org/abs/2507.20198),[Page](https://github.com/cokeshao/Awesome-Multimodal-Token-Compression)] ![Code](https://img.shields.io/github/stars/cokeshao/Awesome-Multimodal-Token-Compression?style=social&label=Star) 

[arxiv 2025.08]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 

## RoPE
[arxiv 2025.05] Circle-RoPE: Cone-like Decoupled Rotary Positional Embedding for Large Vision-Language Models  [[PDF](https://arxiv.org/abs/2505.16416),[Page](https://github.com/lose4578/CircleRoPE)] ![Code](https://img.shields.io/github/stars/lose4578/CircleRoPE?style=social&label=Star) 

[arxiv 2025.08]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 


## audio 
[arxiv 2024.10] MuVi: Video-to-Music Generation with Semantic Alignment and Rhythmic Synchronization[[PDF](https://arxiv.org/abs/2410.12957)]

[arxiv 2024.11]Video-Guided Foley Sound Generation with Multimodal Controls [[PDF](https://arxiv.org/abs/2411.17698), [Page](https://ificl.github.io/MultiFoley/)]


[arxiv 2025.08]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 


## Study 
[arxiv 2025.03] Should VLMs be Pre-trained with Image Data?  [[PDF](https://arxiv.org/abs/2503.07603)]

[arxiv 2025.08]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 


## agent
[arxiv 2025.08]  OPENCUA: Open Foundations for Computer-Use Agents [[PDF](https://arxiv.org/abs/2508.09123),[Page](https://opencua.xlang.ai/)] ![Code](https://img.shields.io/github/stars/xlang-ai/OpenCUA?style=social&label=Star) 

[arxiv 2025.08]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 

## memory
[arxiv 2025.08]  Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory [[PDF](https://arxiv.org/abs/2508.09736),[Page](https://github.com/bytedance-seed/m3-agent)] ![Code](https://img.shields.io/github/stars/bytedance-seed/m3-agent?style=social&label=Star) 

[arxiv 2025.08]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 


## speed 
[arxiv 2024.10]PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction[[PDF](https://arxiv.org/abs/2410.17247), [Page]()]

[arxiv 2024.12]  [CLS] Attention is All You Need for Training-Free Visual Token Pruning: Make VLM Inference Faster
 [[PDF](https://arxiv.org/abs/2412.01818),[Page](https://theia-4869.github.io/FasterVLM)] ![Code](https://img.shields.io/github/stars/Theia-4869/FasterVLM?style=social&label=Star) 


[arxiv 2025.03]  Dynamic Pyramid Network for Efficient Multimodal Large Language Model [[PDF](https://arxiv.org/abs/2503.20322)]

[arxiv 2025.03]  Mobile-VideoGPT Fast and Accurate Video Understanding Language Model [[PDF](https://arxiv.org/pdf/2503.21782),[Page](https://amshaker.github.io/Mobile-VideoGPT/)] ![Code](https://img.shields.io/github/stars/Amshaker/Mobile-VideoGPT?style=social&label=Star) 

[arxiv 2025.03]  InternVL-X: Advancing and Accelerating InternVL Series with Efficient Visual Token Compression [[PDF](https://arxiv.org/abs/2503.21307),[Page](https://github.com/ludc506/InternVL-X)] ![Code](https://img.shields.io/github/stars/ludc506/InternVL-X?style=social&label=Star) 

[arxiv 2025.04]  Memory-efficient Streaming VideoLLMs for Real-time Procedural Video Understanding [[PDF](https://arxiv.org/abs/2504.13915),[Page](https://dibschat.github.io/ProVideLLM/)] 

[arxiv 2025.08] MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMs  [[PDF](https://arxiv.org/abs/2508.18264),[Page](https://cv.ironieser.cc/projects/mmtok.html)] ![Code](https://img.shields.io/github/stars/Ironieser/MMTok/?style=social&label=Star) 

[arxiv 2025.08]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 

## Evaluation

[arxiv 2025.04]  MME-Unify: A Comprehensive Benchmark for Unified Multimodal Understanding and Generation Models [[PDF](https://arxiv.org/abs/2504.03641),[Page](https://mme-unify.github.io/)] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 

[arxiv 2025.05] On Path to Multimodal Generalist: General-Level and General-Bench  [[PDF](https://arxiv.org/abs/2505.04620),[Page](https://generalist.top/)] ![Code](https://img.shields.io/github/stars/path2generalist/General-Level?style=social&label=Star) 

[arxiv 2025.05] SpatialScore: Towards Unified Evaluation for Multimodal Spatial Understanding  [[PDF](https://arxiv.org/abs/2505.17012),[Page](https://haoningwu3639.github.io/SpatialScore)] ![Code](https://img.shields.io/github/stars/haoningwu3639/SpatialScore/?style=social&label=Star) 

[arxiv 2025.05]  LENS: Multi-level Evaluation of Multimodal Reasoning with Large Language Models [[PDF](https://arxiv.org/abs/2505.15616),[Page](https://lens4mllms.github.io/mars2-workshop-iccv2025/)] 

[arxiv 2025.05] HumaniBench: A Human-Centric Framework for Large Multimodal Models Evaluation  [[PDF](https://arxiv.org/abs/2505.11454)]

[arxiv 2025.05] Human-Aligned Bench: Fine-Grained Assessment of Reasoning Ability in MLLMs vs. Humans  [[PDF](https://arxiv.org/abs/2505.11141),[Page](https://yansheng-qiu.github.io/human-aligned-bench.github.io/)] ![Code](https://img.shields.io/github/stars/yansheng-qiu/Human_Aligned_Bench?style=social&label=Star) 


[arxiv 2025.08]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 


