


###  LLM 

[arxiv 2024.12] Training Large Language Models to Reason in a Continuous Latent Space  [[PDF](https://arxiv.org/pdf/2412.06769)]

[arxiv 2025.01]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 


### Multi-modality Generation 
[arxiv 2024.11] Multimodal Alignment and Fusion: A Survey [[PDF](https://arxiv.org/abs/2411.17040)]

[arxiv 2025.01]  Next Token Prediction Towards Multimodal Intelligence[[PDF](https://arxiv.org/abs/2412.18619),[Page](https://github.com/LMM101/Awesome-Multimodal-Next-Token-Prediction)] ![Code](https://img.shields.io/github/stars/LMM101/Awesome-Multimodal-Next-Token-Prediction?style=social&label=Star) 


### [Awesome-Multimodal-Large-Language-Models](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)

[arxiv 2023.07]Generative Pretraining in Multimodality [[PDF](https://arxiv.org/abs/2307.05222),[Page](https://github.com/baaivision/Emu)]

[arxiv 2023.07]Generating Images with Multimodal Language Models [[PDF](https://arxiv.org/abs/2305.17216),[Page](https://jykoh.com/gill)]

[arxiv 2023.07]3D-LLM: Injecting the 3D World into Large Language Models[[PDF] (https://arxiv.org/abs/2307.12981),[Page](https://vis-www.cs.umass.edu/3dllm/)]

[arxiv 2023.10]Making LLaMA SEE and Draw with SEED Tokenizer [[PDF](https://arxiv.org/abs/2310.01218),[Page](https://github.com/AILab-CVC/SEED)]

[arxiv 2023.10]Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation [[PDf](https://arxiv.org/abs/2310.08541),[Page](https://idea2img.github.io/)]

[arxiv 2023.12]CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation[[PDF](https://arxiv.org/abs/2311.18775),[Page](https://codi-2.github.io/)]

[arxiv 2023.12]SmartEdit: Exploring Complex Instruction-based Image Editing with Multimodal Large Language Models [[PDF](https://arxiv.org/abs/2312.06739),[Page](https://yuzhou914.github.io/SmartEdit/)]

[arxiv 2023.12]InstructAny2Pix: Flexible Visual Editing via Multimodal Instruction Following [[PDF](https://arxiv.org/abs/2312.06738), [Page](https://github.com/jacklishufan/InstructAny2Pix.git)]

[arxiv 2023.12]Massively Multimodal Masked Modeling [[PDF](https://arxiv.org/abs/2312.06647),[Page](https://4m.epfl.ch/)]

[arxiv 2023.12]Gemini: A Family of Highly Capable Multimodal Models [[PDF](https://arxiv.org/abs/2312.11805)]

[arxiv 2023.12]Generative Multimodal Models are In-Context Learners [[PDF](https://arxiv.org/abs/2312.13286),[Page](https://baaivision.github.io/emu2)]

[arxiv 2024.1]DiffusionGPT: LLM-Driven Text-to-Image Generation System [[PDF](https://arxiv.org/abs/2401.10061)]

[arxiv 2024.1]Image Anything: Towards Reasoning-coherent and Training-free Multi-modal Image Generation [[PDF](https://arxiv.org/abs/2401.17664),[Page](https://vlislab22.github.io/ImageAnything/)]

[arxiv 2024.03]3D-VLA 3D Vision-Language-Action Generative World Model[[PDF](https://arxiv.org/abs/2403.09631),[Page](https://vis-www.cs.umass.edu/3dvla/)]

[arxiv 2024.03]LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models [[PDF](https://arxiv.org/abs/2403.15388),[Page](https://llava-prumerge.github.io/)]

[arxiv 2024.04]SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation [[PDF](https://arxiv.org/abs/2404.14396)]

[arxiv 2024.06]The Evolution of Multimodal Model Architectures [[PDF](https://arxiv.org/pdf/2405.17927)]

[arxiv 2024.08] Show-o: One Single Transformer to Unify Multimodal Understanding and Generation[[PDF](https://arxiv.org/abs/2408.12528), [Page](https://github.com/showlab/Show-o)]

[arxiv 2024.09] VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation[[PDF](https://arxiv.org/abs/2409.04429)]

[arxiv 2024.09]EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions[[PDF](https://arxiv.org/abs/2409.18042), [Page](https://emova-ollm.github.io/)]


[arxiv 2024.09] Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models[[PDF](https://arxiv.org/abs/2409.17146),)]


[arxiv 2024.09]MonoFormer: One Transformer for Both Diffusion and Autoregression [[PDF](https://arxiv.org/abs/2409.16280), [Page](https://monoformer.github.io/)]


[arxiv 2024.09] Visual Prompting in Multimodal Large Language Models: A Survey[[PDF](https://arxiv.org/abs/2409.15310)]


[arxiv 2024.09] Emu3: Next-Token Prediction is All You Need[[PDF](https://arxiv.org/abs/2409.18869), [Page](https://emu.baai.ac.cn/)]


[arxiv 2024.10]ACE: All-round Creator and Editor Following Instructions via Diffusion Transformer [[PDF](https://arxiv.org/abs/2410.00086), [Page](https://ali-vilab.github.io/ace-page/)]

[arxiv 2024.10]Baichuan-Omni Technical Report [[PDF](https://arxiv.org/abs/2410.08565), [Page](https://github.com/westlake-baichuan-mllm/bc-omni)]

[arxiv 2024.10] PUMA: Empowering Unified MLLM with Multi-granular Visual Generation[[PDF](https://arxiv.org/abs/2410.13861), [Page](https://rongyaofang.github.io/puma/)]

[arxiv 2024.10]Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation [[PDF](https://arxiv.org/abs/2410.13848), [Page](https://github.com/deepseek-ai/Janus)]

[arxiv 2024.11]Spider: Any-to-Many Multimodal LLM [[PDF](https://arxiv.org/abs/2411.09439)]

[arxiv 2024.12] TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation  [[PDF](https://arxiv.org/abs/2412.03069),[Page](https://byteflow-ai.github.io/TokenFlow/)] ![Code](https://img.shields.io/github/stars/ByteFlow-AI/TokenFlow?style=social&label=Star) 

[arxiv 2024.12] Multimodal Latent Language Modeling with Next-Token Diffusion  [[PDF](https://arxiv.org/abs/2412.08635),[Page](https://aka.ms/GeneralAI)] 


[arxiv 2024.12] InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions  [[PDF](https://arxiv.org/abs/2412.09596),[Page](https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-OmniLive)] ![Code](https://img.shields.io/github/stars/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-OmniLive?style=social&label=Star) 

[arxiv 2025.01]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 




## Agent

[arxiv 2024.10] Agent S: An Open Agentic Framework that Uses Computers Like a Human[[PDF](https://arxiv.org/abs/2410.08164), [Page](https://github.com/simular-ai/Agent-S)]

[arxiv 2024.12]  SPAgent: Adaptive Task Decomposition and Model Selection for General Video Generation and Editing [[PDF](https://arxiv.org/abs/2411.18983)]

[arxiv 2024.12] TeamCraft: A Benchmark for Multi-Modal Multi-Agent Systems in Minecraft  [[PDF](https://arxiv.org/abs/2412.05255),[Page](https://teamcraft-bench.github.io/)] ![Code](https://img.shields.io/github/stars/teamcraft-bench/teamcraft?style=social&label=Star) 

[arxiv 2025.01] UnrealZoo: Enriching Photo-realistic Virtual Worlds for Embodied AI  [[PDF](https://arxiv.org/abs/2412.20977),[Page](http://unrealzoo.site/)] ![Code](https://img.shields.io/github/stars/UnrealZoo/unrealzoo-gym?style=social&label=Star) 


[arxiv 2025.01]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 


## Multi-modality Understanding 

[arxiv 2024.10]TemporalBench Benchmarking Fine-grained Temporal Understanding for Multimodal Video Models [[PDF](https://arxiv.org/abs/2410.10818), [Page](https://temporalbench.github.io/)]

[arxiv 2024.10] γ−MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models[[PDF](https://arxiv.org/abs/2410.13859), [Page](https://github.com/Yaxin9Luo/Gamma-MOD)]

[arxiv 2024.10]Remember, Retrieve and Generate: Understanding Infinite Visual Concepts as Your Personalized Assistant [[PDF](https://arxiv.org/abs/2410.13360), [Page](https://github.com/Hoar012/RAP-MLLM)]

[arxiv 2024.11] LLaVA-o1: Let Vision Language Models Reason Step-by-Step[[PDF](https://arxiv.org/abs/2411.10440), [Page]()]

[arxiv 2024.11] CATCH: Complementary Adaptive Token-level Contrastive Decoding to Mitigate Hallucinations in LVLMs[[PDF](https://arxiv.org/abs/2411.12713)]

[arxiv 2024.12]  VisionZip: Longer is Better but Not Necessary in Vision Language Models [[PDF](https://arxiv.org/abs/2412.04467),[Page](https://github.com/dvlab-research/VisionZip)] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 

[arxiv 2024.12]  NVILA: Efficient Frontier Visual Language Models [[PDF](https://arxiv.org/abs/2412.04468)] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 

[arxiv 2024.12]  Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling [[PDF](https://arxiv.org/abs/2412.05271),[Page](https://internvl.github.io/blog/)] ![Code](https://img.shields.io/github/stars//OpenGVLab/InternVL?style=social&label=Star) 

[arxiv 2024.12]  Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces [[PDF](https://arxiv.org/abs/2412.14171),[Page](https://vision-x-nyu.github.io/thinking-in-space.github.io/)] ![Code](https://img.shields.io/github/stars/vision-x-nyu/thinking-in-space?style=social&label=Star) 

[arxiv 2024.12] OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving  [[PDF](https://arxiv.org/abs/2412.15208),[Page](https://github.com/taco-group/OpenEMMA)] ![Code](https://img.shields.io/github/stars/taco-group/OpenEMMA?style=social&label=Star) 

[arxiv 2024.12]  VLM-AD: End-to-End Autonomous Driving through Vision-Language Model Supervision [[PDF](https://arxiv.org/abs/2412.14446)]

[arxiv 2024.12]  HoVLE: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding [[PDF](https://arxiv.org/abs/2412.16158),[Page](https://huggingface.co/OpenGVLab/HoVLE)] 

[arxiv 2025.01]  VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM [[PDF](http://arxiv.org/abs/2501.00599),[Page](https://damo-nlp-sg.github.io/VideoRefer/)] ![Code](https://img.shields.io/github/stars/DAMO-NLP-SG/VideoRefer?style=social&label=Star) 

[arxiv 2025.01] VideoChat-Flash: Hierarchical Compression for Long-Context Video Modeling  [[PDF](https://arxiv.org/abs/2501.00574),[Page](https://github.com/OpenGVLab/VideoChat-Flash)] ![Code](https://img.shields.io/github/stars/OpenGVLab/VideoChat-Flash?style=social&label=Star) 

[arxiv 2025.01] VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction  [[PDF](https://arxiv.org/abs/2501.01957),[Page](https://github.com/VITA-MLLM/VITA)] ![Code](https://img.shields.io/github/stars/VITA-MLLM/VITA?style=social&label=Star) 

[arxiv 2025.01] Virgo: A Preliminary Exploration on Reproducing o1-like MLLM  [[PDF](https://arxiv.org/abs/2501.01904),[Page](https://github.com/RUCAIBox/Virgo)] ![Code](https://img.shields.io/github/stars/RUCAIBox/Virgo?style=social&label=Star) 


[arxiv 2025.01]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 



## multi-modality evaluation 
[arxiv 2024.10] The Curse of Multi-Modalities: Evaluating Hallucinations of Large Multimodal Models across Language, Visual, and Audio[[PDF](https://arxiv.org/abs/2410.12787)]


## audio 
[arxiv 2024.10] MuVi: Video-to-Music Generation with Semantic Alignment and Rhythmic Synchronization[[PDF](https://arxiv.org/abs/2410.12957)]

[arxiv 2024.11]Video-Guided Foley Sound Generation with Multimodal Controls [[PDF](https://arxiv.org/abs/2411.17698), [Page](https://ificl.github.io/MultiFoley/)]


[arxiv 2025.01]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 

## speed 
[arxiv 2024.10]PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction[[PDF](https://arxiv.org/abs/2410.17247), [Page]()]

[arxiv 2024.12]  [CLS] Attention is All You Need for Training-Free Visual Token Pruning: Make VLM Inference Faster
 [[PDF](https://arxiv.org/abs/2412.01818),[Page](https://theia-4869.github.io/FasterVLM)] ![Code](https://img.shields.io/github/stars/Theia-4869/FasterVLM?style=social&label=Star) 



[arxiv 2025.01]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star) 
