# Video Generation Survey
A reading list of video generation

## Repo for open-sora

[2024.03] [HPC-AI Open-Sora](https://github.com/hpcaitech/Open-Sora) 

[2024.03] [PKU Open-Sora Plan](https://github.com/PKU-YuanGroup/Open-Sora-Plan)


## Related surveys 
[Awesome-Video-Diffusion-Models](https://github.com/ChenHsing/Awesome-Video-Diffusion-Models?tab=readme-ov-file)

[Awesome-Text-to-Image](https://github.com/Yutong-Zhou-cv/Awesome-Text-to-Image?tab=readme-ov-file#head-ti2i)

## :point_right: Models to play with

### Open source

* **VideoCrafter/Floor33** [[Page](http://floor33.tech/)], [[Discord](https://discord.gg/rrayYqZ4tf)], [[Code & Models](https://github.com/AILab-CVC/VideoCrafter)]

* **ModelScope** [[Page](https://modelscope.cn/models/damo/text-to-video-synthesis/summary), [i2v](https://modelscope.cn/models/damo/Image-to-Video/summary)], [[Code & Models](https://modelscope.cn/models/damo/text-to-video-synthesis/summary)]

* **Hotshot-XL** [[Page](https://hotshot.co/)], [[Code & Models](https://github.com/hotshotco/Hotshot-XL)]

* **AnimeDiff** [[Page](https://animatediff.github.io/), [Code & Models](https://github.com/guoyww/AnimateDiff)]

* **Zeroscope V2 XL** [[Page](https://huggingface.co/cerspense/zeroscope_v2_XL)]

* **MuseV** [[Page](https://github.com/TMElyralab/MuseV)] 

* **opensora plan** [[Page](https://github.com/PKU-YuanGroup/Open-Sora-Plan)]

* **opensora** [[Page](https://github.com/hpcaitech/Open-Sora)]

*  **easyanimate** [[Page](https://github.com/aigc-apps/EasyAnimate)] 

### Non-open source

* **Gen-1/Gen-2** [[Page](https://research.runwayml.com/gen2)]

* **Pika Lab** [[Page](https://www.pika.art/)], [[Discord](http://discord.gg/pika)]

* **Moonvalley** [[Page](https://moonvalley.ai/)], [[Discord](https://discord.gg/vk3aaH7r)]

* **Leonard Ai** [[Page](https://leonardo.ai/)]

* **Morph Studio** [[Page](https://www.morphstudio.xyz/)], [[Discord](https://discord.gg/hjd9JvXTU5)]
  
* **Lensgo** [[Page](https://lensgo.ai/), [Discord]()]

* **Genmo** [[Page](https://www.genmo.ai/)]

* **PlaiDay** [[Discord](https://discord.gg/6f6Q9pWb)]

* **Nerverends** [[Page](https://neverends.life/create)]

* **HiDream.ai/Pixeling** [[Page](https://hidream.ai/#/Pixeling)]

* **Assistant++** [[Page](https://assistive.chat/video)]

* **PixVerse**[[Page](https://pixverse.ai/)]

* **ltx.studio**[[Page](https://ltx.studio/)]

  
* **Haiper** [[Page](https://app.haiper.ai/explore)]

* **vivago.ai**[[Page](https://vivago.ai/video)]

* **智普AI**[[Page](https://chatglm.cn/video)]

### Translation 
* **Goenhance.ai**[[Page](https://www.goenhance.ai/)]

* **ViggleAI**[[Page](https://t.co/2GMBpUOyHL)]



## :point_right: Databases

* **HowTo100M**

  [ICCV 2019] Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips \[[PDF](https://arxiv.org/pdf/1906.03327.pdf), [Project](https://www.di.ens.fr/willow/research/howto100m/) \]

* **HD-VILA-100M**
  
  [CVPR 2022]Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions [[PDF](https://openaccess.thecvf.com/content/CVPR2022/papers/Xue_Advancing_High-Resolution_Video-Language_Representation_With_Large-Scale_Video_Transcriptions_CVPR_2022_paper.pdf), [Page](https://github.com/microsoft/XPretrain/blob/main/hd-vila-100m/README.md)]
  
* **Web10M**

  [ICCV 2021]Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval \[[PDF](https://arxiv.org/pdf/2104.00650.pdf), [Project](https://github.com/m-bain/webvid) \]
  
* **UCF-101**

  [arxiv 2012] Ucf101: A dataset of 101 human actions classes from videos in the wild \[[PDF](https://arxiv.org/pdf/1212.0402.pdf), [Project](https://www.crcv.ucf.edu/data/UCF101.php) \]
  
* **Sky Time-lapse** 

  [CVPR 2018] Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks \[[PDF](https://openaccess.thecvf.com/content_cvpr_2018/papers/Xiong_Learning_to_Generate_CVPR_2018_paper.pdf), [Project](https://github.com/weixiong-ur/mdgan) \]
  
* **TaiChi** 

  [NIPS 2019] First order motion model for image animation \[ [PDF](https://papers.nips.cc/paper/2019/file/31c0b36aef265d9221af80872ceb62f9-Paper.pdf), [Project](https://github.com/AliaksandrSiarohin/first-order-model) \]

* **Celebv-text**
  
  [arxiv ]CelebV-Text: A Large-Scale Facial Text-Video Dataset [[PDF](), [Page](https://celebv-text.github.io/)]

* **Youku-mPLUG**
  
  [arxiv 2023.06]Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks [[PDF](https://arxiv.org/abs/2306.04362)]

* **InternVid**
  
  [arxiv 2023.07]InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation [[PDF](https://arxiv.org/abs/2307.06942)]

* **DNA-Rendering**
  
  [arxiv 2023.07] DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-centric Rendering [[PDF](https://arxiv.org/abs/2307.10173)]

* **Vimeo25M** (not open-source)
  
  [arxiv 2023.09] LAVIE: HIGH-QUALITY VIDEO GENERATION WITH CASCADED LATENT DIFFUSION MODELS [[PDF](https://arxiv.org/pdf/2309.15103.pdf)]

* **HD-VG-130M**

  [arxiv 2023.06]VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation [[PDF](https://arxiv.org/abs/2305.10874), [Page](https://github.com/daooshee/HD-VG-130M)]

* **Panda-70M**

[arxiv 2024.06]ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of Text-to-Time-lapse Video Generation
 [[PDF](https://arxiv.org/abs/2406.18522), [Page](https://github.com/PKU-YuanGroup/ChronoMagic-Bench)]


* **ChronoMagic-Pro**
  

* **OpenVid-1M**
  [arxiv 2024.07] A Large-Scale Dataset for High-Quality Text-to-Video Generation  [[PDF](http://export.arxiv.org/pdf/2407.02371),[Page](https://nju-pcalab.github.io/projects/openvid/)]


## Video VAE 
[arxiv 2024.05]CV-VAE: A Compatible Video VAE for Latent Generative Video Models [[PDF](https://arxiv.org/abs/2405.20279),[Page](https://ailab-cvc.github.io/cvvae/index.html)]

[arxiv 2024.06]OmniTokenizer: A Joint Image-Video Tokenizer for Visual Generation[[PDF](https://arxiv.org/abs/2406.09399),[Page](https://github.com/FoundationVision/OmniTokenizer)]

[arxiv 2024.07] [[PDF](),[Page]()]



## GAN/VAE-based methods 
[NIPS 2016] **---VGAN---** Generating Videos with Scene Dynamics \[[PDF](https://proceedings.neurips.cc/paper/2016/file/04025959b191f8f9de3f924f0940515f-Paper.pdf), [code](https://github.com/cvondrick/videogan) \]

[ICCV 2017] **---TGAN---** Temporal Generative Adversarial Nets with Singular Value Clipping \[[PDF](https://arxiv.org/pdf/1611.06624.pdf), [code](https://github.com/pfnet-research/tgan) \]

[CVPR 2018] **---MoCoGAN---** MoCoGAN: Decomposing Motion and Content for Video Generation \[[PDF](https://arxiv.org/pdf/1707.04993.pdf), [code](https://github.com/sergeytulyakov/mocogan) \]

[NIPS 2018] **---SVG---** Stochastic Video Generation with a Learned Prior \[[PDF](https://proceedings.mlr.press/v80/denton18a/denton18a.pdf), [code](https://github.com/edenton/svg) \]

[ECCV 2018] Probabilistic Video Generation using
Holistic Attribute Control \[[PDF](https://openaccess.thecvf.com/content_ECCV_2018/papers/Jiawei_He_Probabilistic_Video_Generation_ECCV_2018_paper.pdf), code\]

[CVPR 2019; CVL ETH] **---SWGAN---** Sliced Wasserstein Generative Models \[[PDF](https://openaccess.thecvf.com/content_CVPR_2019/papers/Wu_Sliced_Wasserstein_Generative_Models_CVPR_2019_paper.pdf), [code](https://github.com/skolouri/swae) \]

[NIPS 2019; NVLabs] **---vid2vid---** Few-shot Video-to-Video Synthesis \[[PDF](https://nvlabs.github.io/few-shot-vid2vid/main.pdf), [code](https://github.com/NVlabs/few-shot-vid2vid) \]

[arxiv 2020; Deepmind] **---DVD-GAN---** ADVERSARIAL VIDEO GENERATION ON COMPLEX DATASETS \[[PDF](https://arxiv.org/pdf/1907.06571.pdf), [code](https://github.com/Harrypotterrrr/DVD-GAN) \]

[IJCV 2020] **---TGANv2---** Train Sparsely, Generate Densely: Memory-efficient Unsupervised Training of High-resolution Temporal GAN \[[PDF](https://arxiv.org/pdf/1811.09245.pdf), [code](https://github.com/pfnet-research/tgan2) \]

[PMLR 2021] **---TGANv2-ODE---** Latent Neural Differential Equations for Video Generation \[[PDF](https://arxiv.org/pdf/2011.03864.pdf), [code](https://github.com/Zasder3/Latent-Neural-Differential-Equations-for-Video-Generation) \]

[ICLR 2021 ] **---DVG---** Diverse Video Generation using a Gaussian Process Trigger \[[PDF](https://openreview.net/pdf?id=Qm7R_SdqTpT), [code](https://github.com/shgaurav1/DVG) \]

[Arxiv 2021; MRSA] **---GODIVA---** GODIVA: Generating Open-DomaIn Videos from nAtural Descriptions \[[PDF]([https://arxiv.org/pdf/2205.15868.pdf](https://arxiv.org/pdf/2104.14806.pdf)), [code](https://github.com/sihyun-yu/digan) \]

*[CVPR 2022 ] **---StyleGAN-V--** StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2 \[[PDF](https://arxiv.org/pdf/2112.14683.pdf), [code](https://github.com/universome/stylegan-v) \]

*[NeurIPs 2022] **---MCVD---** MCVD: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation [[PDF](https://arxiv.org/abs/2205.09853), [code](https://github.com/voletiv/mcvd-pytorch)]

## :point_right: Implicit Neural Representations
[ICLR 2022] Generating videos with dynamics-aware implicit generative adversarial networks \[[PDF](https://openreview.net/pdf?id=Czsdv-S4-w9), [code]() \]

## Transformer-based 
[arxiv 2021] **---VideoGPT--** VideoGPT: Video Generation using VQ-VAE and Transformers \[[PDF](https://arxiv.org/pdf/2104.10157.pdf), [code](https://github.com/wilson1yan/VideoGPT) \]

[ECCV 2022; Microsoft] **---NÜWA--** NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion \[[PDF](https://arxiv.org/pdf/2111.12417.pdf), code \]

[NIPS 2022; Microsoft] **---NÜWA-Infinity--** NUWA-Infinity: Autoregressive over Autoregressive Generation for Infinite Visual Synthesis \[[PDF](https://arxiv.org/pdf/2207.09814.pdf), code \]

[Arxiv 2020; Tsinghua] **---CogVideo--** CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers \[[PDF](https://arxiv.org/pdf/2205.15868.pdf), [code](https://github.com/THUDM/CogVideo) \]

*[ECCV 2022] **---TATS--** Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer \[[PDF](https://arxiv.org/pdf/2204.03638.pdf), [code](https://github.com/SongweiGe/TATS)\]


*[arxiv 2022; Google] **---PHENAKI--** PHENAKI: VARIABLE LENGTH VIDEO GENERATION FROM OPEN DOMAIN TEXTUAL DESCRIPTIONS \[[PDF](https://arxiv.org/pdf/2210.02399.pdf), code \]

[arxiv 2022.12]MAGVIT: Masked Generative Video Transformer[[PDF](https://arxiv.org/pdf/2212.05199.pdf)]

[arxiv 2023.11]Optimal Noise pursuit for Augmenting Text-to-Video Generation [[PDF](https://arxiv.org/abs/2311.00949)]

[arxiv 2024.01]WorldDreamer: Towards General World Models for Video Generation via Predicting Masked Tokens [[PDF](https://arxiv.org/abs/2401.09985),[Page](https://world-dreamer.github.io/)]

## Diffusion-based methods 
*[NIPS 2022; Google] **---VDM--**  Video Diffusion Models \[[PDF](https://arxiv.org/pdf/2204.03458.pdf), [code](https://github.com/lucidrains/video-diffusion-pytorch) \]

*[arxiv 2022; Meta] **---MAKE-A-VIDEO--** MAKE-A-VIDEO: TEXT-TO-VIDEO GENERATION WITHOUT TEXT-VIDEO DATA \[[PDF](https://arxiv.org/pdf/2209.14792.pdf), code \]

*[arxiv 2022; Google] **---IMAGEN VIDEO--** IMAGEN VIDEO: HIGH DEFINITION VIDEO GENERATION WITH DIFFUSION MODELS \[[PDF](https://arxiv.org/pdf/2210.02303.pdf), code \]

*[arxiv 2022; ByteDace] ***MAGIC VIDEO***:Efficient Video Generation With Latent Diffusion Models \[[PDF](https://arxiv.org/pdf/2211.11018.pdf), code\]

*[arxiv 2022; Tencent] ***LVDM*** Latent Video Diffusion Models for High-Fidelity Video Generation with Arbitrary Lengths  \[[PDF](https://arxiv.org/pdf/2211.13221.pdf), code\]

[AAAI 2022; JHU ] VIDM: Video Implicit Diffusion Model \[[PDF](https://kfmei.page/vidm/Video_implicit_diffusion_models.pdf)\]

[arxiv 2023.01; Meta] Text-To-4D Dynamic Scene Generation [[PDF](https://arxiv.org/pdf/2301.11280.pdf), [Page](https://make-a-video3d.github.io/)]

[arxiv 2023.03]Video Probabilistic Diffusion Models in Projected Latent Space [[PDF](https://arxiv.org/abs/2302.07685), [Page](https://sihyun.me/PVDM/)]

[arxiv 2023.03]Controllable Video Generation by Learning the Underlying Dynamical System with Neural ODE [[PDF](https://arxiv.org/abs/2303.05323)]

[arxiv 2023.03]Decomposed Diffusion Models for High-Quality Video Generation [[PDF](https://arxiv.org/pdf/2303.08320.pdf)]

[arxiv 2023.03]NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation [[PDF](https://arxiv.org/abs/2303.12346)]

*[arxiv 2023.04]Latent-Shift: Latent Diffusion with Temporal Shift for Efficient Text-to-Video Generation [[PDF](https://arxiv.org/abs/2304.08477)]

*[arxiv 2023.04]Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models [[PDF](https://arxiv.org/abs/2304.08818), [Page](https://research.nvidia.com/labs/toronto-ai/VideoLDM/)]

[arxiv 2023.04]LaMD: Latent Motion Diffusion for Video Generation [[PDF](https://arxiv.org/abs/2304.11603)]

*[arxiv 2023.05]Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models[[PDF](https://arxiv.org/pdf/2305.10474.pdf), [Page](https://research.nvidia.com/labs/dir/pyoco/)]

[arxiv 2023.05]VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation [[PDF](https://arxiv.org/pdf/2305.10874.pdf)]

[arxiv 2023.08]ModelScope Text-to-Video Technical Report [[PDF](https://arxiv.org/pdf/2308.06571.pdf)]

[arxiv 2023.08]Dual-Stream Diffusion Net for Text-to-Video Generation [[PDF](https://huggingface.co/papers/2308.08316)]

[arxiv 2023.08]SimDA: Simple Diffusion Adapter for Efficient Video Generation [[PDF](https://arxiv.org/abs/2308.09710), [Page](https://chenhsing.github.io/SimDA/)]

[arxiv 2023.08]Dysen-VDM: Empowering Dynamics-aware Text-to-Video Diffusion with Large Language Models [[PDF](https://arxiv.org/pdf/2308.13812.pdf), [Page](https://haofei.vip/Dysen-VDM/)]

[arxiv 2023.09]Reuse and Diffuse: Iterative Denoising for Text-to-Video Generation[[PDF](https://arxiv.org/pdf/2309.03549.pdf),[Page](https://anonymous0x233.github.io/ReuseAndDiffuse/)]

[arxiv 2023.09]LAVIE: HIGH-QUALITY VIDEO GENERATION WITH CASCADED LATENT DIFFUSION MODELS [[PDF](https://arxiv.org/pdf/2309.15103.pdf), [Page](https://vchitect.github.io/LaVie-project/)]

[arxiv 2023.09]VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning [[PDF](https://arxiv.org/abs/2309.15091), [Page](https://videodirectorgpt.github.io/)]

[arxiv 2023.10]Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation [[PDF](https://arxiv.org/abs/2309.15818), [Page](https://showlab.github.io/Show-1)]

[arxiv 2023.10]LLM-grounded Video Diffusion Models [[PDF](https://arxiv.org/abs/2309.17444),[Page](https://llm-grounded-video-diffusion.github.io/)]

[arxiv 2023.10]VideoCrafter1: Open Diffusion Models for High-Quality Video Generation [[PDF](https://arxiv.org/abs/2310.19512),[Page](https://github.com/AILab-CVC/VideoCrafter)]

[arxiv 2023.11]Make Pixels Dance: High-Dynamic Video Generation [[PDF](https://arxiv.org/abs/2311.10982), [Page](https://makepixelsdance.github.io/)]

[arxiv 2023.11]Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets[[PDF](https://static1.squarespace.com/static/6213c340453c3f502425776e/t/655ce779b9d47d342a93c890/1700587395994/stable_video_diffusion.pdf), [Page](https://t.co/P2lmq343cf)]

[arxiv 2023.11]Kandinsky Video [[PDF](https://arxiv.org/abs/2311.13073),[Page](https://ai-forever.github.io/kandinsky-video/)]

[arxiv 2023.12]GenDeF: Learning Generative Deformation Field for Video Generation [[PDF](https://arxiv.org/abs/2312.04561),[Page](https://aim-uofa.github.io/GenDeF/)]

[arxiv 2023.12]GenTron: Delving Deep into Diffusion Transformers for Image and Video Generation [[PDF](https://arxiv.org/abs/2312.04557),[Page](https://www.shoufachen.com/gentron_website/)]

[arxiv 2023.12]Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation [[PDF](https://arxiv.org/abs/2312.04483), [Page](https://higen-t2v.github.io/)]

[arxiv 2023.12]AnimateZero:Video Diffusion Models are Zero-Shot Image Animators [[PDF](https://arxiv.org/abs/2312.03793),[Page](https://vvictoryuki.github.io/animatezero.github.io/)]

[arxiv 2023.12]Photorealistic Video Generation with Diffusion Models [[PDF](https://arxiv.org/abs/2312.06662),[Page](https://walt-video-diffusion.github.io/)]

[arxiv 2023.12]A Recipe for Scaling up Text-to-Video Generation with Text-free Videos [[PDF](https://arxiv.org/abs/2312.15770),[Page](https://tf-t2v.github.io/)]

[arxiv 2023.12]MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation [[PDF](https://arxiv.org/pdf/2401.04468.pdf), [Page](https://magicvideov2.github.io/)]

[arxiv 2024.1]Latte: Latent Diffusion Transformer for Video Generation [[PDF](https://arxiv.org/abs/2401.03048),[Page](https://maxin-cn.github.io/latte_project)]

[arxiv 2024.1]VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models [[PDF](https://arxiv.org/abs/2401.09047),[Page](https://ailab-cvc.github.io/videocrafter2/)]

[arxiv 2024.1]Lumiere: A Space-Time Diffusion Model for Video Generation [[PDF](https://arxiv.org/abs/2401.12945), [Page](https://lumiere-video.github.io/)]

[arxiv 2024.02]Hybrid Video Diffusion Models with 2D Triplane and 3D Wavelet Representation [[PDF](https://arxiv.org/abs/2402.13729)]

[arxiv 2024.02]Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis[[PDF](https://arxiv.org/abs/2402.14797),[Page](https://snap-research.github.io/snapvideo/)]

[arxiv 2024.03]Mora: Enabling Generalist Video Generation via A Multi-Agent Framework[[PDF](https://arxiv.org/html/2403.13248v1)]

[arxiv 2024.03]Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition [[PDF](https://arxiv.org/abs/2403.14148),[Page](https://arxiv.org/abs/2403.14148)]

[arxiv 2024.04]Grid Diffusion Models for Text-to-Video Generation [[PDF](https://arxiv.org/abs/2404.00234)]

[arxiv 2024.04]MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators [[PDF](https://arxiv.org/abs/2404.05014)]

[arxiv 2024.05]Matten: Video Generation with Mamba-Attention [[PDF](https://arxiv.org/abs/2405.03025)]

[arxiv 2024.05]Vidu: a Highly Consistent, Dynamic and Skilled Text-to-Video Generator with Diffusion Models [[PDF](https://arxiv.org/abs/2405.04233),[Page](https://www.shengshu-ai.com/vidu)]

[arxiv 2024.05]Lumina-T2X: Transforming Text into Any Modality, Resolution, and Duration via Flow-based Large Diffusion Transformers [[PDF](https://arxiv.org/abs/2405.05945),[Page](https://github.com/Alpha-VLLM/Lumina-T2X)]

[arxiv 2024.05] Scaling Diffusion Mamba with Bidirectional SSMs for Efficient Image and Video Generation [[PDF](https://arxiv.org/abs/2405.15881)]

[arxiv 2024.06]Hierarchical Patch Diffusion Models for High-Resolution Video Generation [[PDF](https://arxiv.org/pdf/2406.07792), [Page](https://snap-research.github.io/hpdm)]

[arxiv 2024.08] xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed Representations[[PDF](https://arxiv.org/abs/2408.12590), [Page](https://github.com/SalesforceAIResearch/xgen-videosyn)]

[arxiv 2024.08] [[PDF](), [Page]()]

## LLMs-based 
[arxiv 2023.12]VideoPoet: A Large Language Model for Zero-Shot Video Generation [[PDF](https://arxiv.org/abs/2312.14125),[Page](http://sites.research.google/videopoet/)]

[arxiv 2024.08] [[PDF](), [Page]()]



## DiT 
[arxiv 2024.05]  EasyAnimate: A High-Performance Long Video Generation Method based on Transformer Architecture [[PDF](https://arxiv.org/abs/2405.18991),[Page](https://github.com/aigc-apps/EasyAnimate)]

[arxiv 2024.08] CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer  [[PDF](https://arxiv.org/abs/2408.06072),[Page](https://github.com/THUDM/CogVideo)]

[arxiv 2024.08]   [[PDF](),[Page]()]

## State Space-based 
[arxiv 2024.03]SSM Meets Video Diffusion Models: Efficient Video Generation with Structured State Spaces [[PDF](https://arxiv.org/abs/2403.07711),[Page](https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models)]


[arxiv 2024.08] [[PDF](), [Page]()]



## improve Video Diffusion models 
[arxiv 2023.10]ScaleCrafter: Tuning-free Higher-Resolution Visual Generation with Diffusion Models [[PDF](https://arxiv.org/abs/2310.07702), [Page](https://yingqinghe.github.io/scalecrafter/)]

[arxiv 2023.10]FreeU: Free Lunch in Diffusion U-Net [[PDF](https://arxiv.org/pdf/2309.11497.pdf), [Page](https://chenyangsi.top/FreeU/)]

[arxiv 2023.12]FreeInit: Bridging Initialization Gap in Video Diffusion Models [[PDF](https://arxiv.org/abs/2312.07537),[Page](https://tianxingwu.github.io/pages/FreeInit/)]

[arxiv 2024.07] Video Diffusion Alignment via Reward Gradients [[PDF](https://arxiv.org/abs/2407.08737), [Page](https://github.com/mihirp1998/VADER)]

[arxiv 2024.08] FancyVideo: Towards Dynamic and Consistent Video Generation via Cross-frame Textual Guidance[[PDF](https://arxiv.org/abs/2408.08189)]


[arxiv 2024.08] [[PDF](), [Page]()]

## composition 
[arxiv 2024.07]VideoTetris: Towards Compositional Text-To-Video Generation[[PDF](https://arxiv.org/abs/2406.04277), [Page](https://videotetris.github.io/)]


[arxiv 2024.07]GVDIFF: Grounded Text-to-Video Generation with Diffusion Models[[PDF](https://arxiv.org/abs/2407.01921)]

[arxiv 2024.07]Compositional Video Generation as Flow Equalization [[PDF](https://arxiv.org/abs/2407.06182), [Page](https://adamdad.github.io/vico/)]

[arxiv 2024.07] InVi: Object Insertion In Videos Using Off-the-Shelf Diffusion Models[[PDF](https://arxiv.org/abs/2407.10958)]



[arxiv 2024.08] [[PDF](), [Page]()]


## multi-prompt 
[arxiv 2023.12]MTVG : Multi-text Video Generation with Text-to-Video Models [[PDF](https://arxiv.org/abs/2312.04086)]

[arxiv 2024.05]TALC: Time-Aligned Captions for Multi-Scene Text-to-Video Generation [[PDF](https://arxiv.org/abs/2405.04682),[Page](https://talc-mst2v.github.io/)]

[arxiv 2024.06]VideoTetris: Towards Compositional Text-To-Video Generation[[PDF](https://arxiv.org/abs/2406.04277), [Page](https://videotetris.github.io/)]

[arxiv 2024.08] [[PDF](), [Page]()]


[arxiv 2024.08] [[PDF](), [Page]()]


## long video generation 
[arxiv 2023.]Gen-L-Video: Long Video Generation via Temporal Co-Denoising [[PDF](https://arxiv.org/abs/2305.18264), [Page](https://g-u-n.github.io/projects/gen-long-video/index.html)]

[arxiv 2023.10]FreeNoise: Tuning-Free Longer Video Diffusion Via Noise Rescheduling [[PDF](https://arxiv.org/abs/2310.15169),[Page](http://haonanqiu.com/projects/FreeNoise.html)]

[arxiv 2023.12]VIDiff: Translating Videos via Multi-Modal Instructions with Diffusion Models[[PDF](https://arxiv.org/abs/2311.18837),[Page](https://chenhsing.github.io/VIDiff)]

[arxiv 2023.12]AVID: Any-Length Video Inpainting with Diffusion Model [[PDF](https://arxiv.org/abs/2312.03816),[Page](https://zhang-zx.github.io/AVID/)]

[arxiv 2023.12]RealCraft: Attention Control as A Solution for Zero-shot Long Video Editing [[PDF](https://arxiv.org/abs/2312.12635)]

[arxiv 2024.03]VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis [[PDF](https://arxiv.org/pdf/2403.13501.pdf),[Page](https://yumengli007.github.io/VSTAR/)]

[arxiv 2024.03]StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text [[PDF](https://arxiv.org/abs/2403.14773)]

[arxiv 2024.04]FlexiFilm: Long Video Generation with Flexible Conditions [[PDF](https://arxiv.org/abs/2404.18620)]

[arxiv 2024.05] FIFO-Diffusion: Generating Infinite Videos from Text without Training [[PDF](https://arxiv.org/abs/2405.11473),[Page](https://jjihwan.github.io/projects/FIFO-Diffusion)]

[arxiv 2024.05]Controllable Long Image Animation with Diffusion Models[[PDF](https://arxiv.org/pdf/2405.17306),[Page](https://wangqiang9.github.io/Controllable.github.io/)]

[arxiv 2024.06]CoNo: Consistency Noise Injection for Tuning-free Long Video Diffusion [[PDF](https://arxiv.org/abs/2406.05082), [Page](https://wxrui182.github.io/CoNo.github.io/)]

[arxiv 2024.06]Video-Infinity: Distributed Long Video Generation [[PDF](https://arxiv.org/abs/2406.16260), [Page](https://video-infinity.tanzhenxiong.com/)]

[arxiv 2024.07]Multi-sentence Video Grounding for Long Video Generation[[PDF](https://arxiv.org/abs/2407.13219)]

[arxiv 2024.08] [[PDF](), [Page]()]


## sound generation 
[arxiv 2024.07] Read, Watch and Scream! Sound Generation from Text and Video
[[PDF](https://arxiv.org/abs/2407.05551), [Page](https://naver-ai.github.io/rewas)]

[arxiv 2024.08] [[PDF](), [Page]()]


## Higher Resolution 
[arxiv 2023.10] ScaleCrafter: Tuning-free Higher-Resolution Visual Generation with Diffusion Models [[PDF](https://arxiv.org/abs/2310.07702), [Page](https://yingqinghe.github.io/scalecrafter/)]



## infinity scene /360
[arxiv 2023.12]Going from Anywhere to Everywhere[[PDF](https://arxiv.org/abs/2312.03884),[Page](https://kovenyu.com/wonderjourney/)]

[arxiv 2024.1]360DVD: Controllable Panorama Video Generation with 360-Degree Video Diffusion Model [[PDF](https://arxiv.org/abs/2401.06578)]

## Video Story /  Concept 
[arxiv 2023.05]TaleCrafter: Interactive Story Visualization with Multiple Characters [[PDF](https://arxiv.org/abs/2305.18247), [Page](https://videocrafter.github.io/TaleCrafter/)]

[arxiv 2023.07]Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation [[PDF](https://arxiv.org/abs/2307.06940), [Page](https://videocrafter.github.io/Animate-A-Story)]

[arxiv 2024.01]VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM [[PDF](https://arxiv.org/abs/2401.01256), [Page](https://videodrafter.github.io/)]

[arxiv 2024.01]Vlogger: Make Your Dream A Vlog [[PDF](https://arxiv.org/abs/2401.09414),[Page](https://github.com/zhuangshaobin/Vlogger)]

[arxiv 2024.03]AesopAgent: Agent-driven Evolutionary System on Story-to-Video Production [[PDF](https://arxiv.org/abs/2403.07952),[Page](https://aesopai.github.io/)]

[arxiv 2024.04]StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation [[PDF](https://arxiv.org/abs/2405.01434),[Page](https://github.com/HVision-NKU/StoryDiffusion)]

[arxiv 2024.05]The Lost Melody: Empirical Observations on Text-to-Video Generation From A Storytelling Perspective [[PDF](https://arxiv.org/abs/2405.08720)]

[arxiv 2024.05]DisenStudio: Customized Multi-subject Text-to-Video Generation with Disentangled Spatial Control [[PDF](https://arxiv.org/abs/2405.12796),[Page](https://forchchch.github.io/disenstudio.github.io/)]

[arxiv 2024.05]  [[PDF](),[Page]()]


## Controllable Video Generation 

*[arxiv 2023.04]Motion-Conditioned Diffusion Model for Controllable Video Synthesis [[PDF](https://arxiv.org/abs/2304.14404), [Page](https://tsaishien-chen.github.io/MCDiff/)]

[arxiv 2023.06]Video Diffusion Models with Local-Global Context Guidance [[PDF](https://arxiv.org/abs/2306.02562)]

[arxiv 2023.06]VideoComposer: Compositional Video Synthesis with Motion Controllability [[PDF](https://arxiv.org/abs/2306.02018)]

[arxiv 2023.07]Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation [[PDF](https://arxiv.org/abs/2307.06940), [Page](https://videocrafter.github.io/Animate-A-Story)]

[arxiv 2023.10]MotionDirector: Motion Customization of Text-to-Video Diffusion Models [[PDF](https://arxiv.org/abs/2310.08465),[Page](https://showlab.github.io/MotionDirector/)]

[arxiv 2023.11]MagicDance: Realistic Human Dance Video Generation with Motions & Facial Expressions Transfer [[PDF](https://arxiv.org/abs/2311.12052), [Page](https://boese0601.github.io/magicdance/)]

[arxiv 2023.11]Space-Time Diffusion Features for Zero-Shot Text-Driven Motion Transfer[[PDF](https://arxiv.org/abs/2311.17009),[Page](https://diffusion-motion-transfer.github.io/)]

[arxiv 2023.11]SparseCtrl: Adding Sparse Controls to Text-to-Video Diffusion Models[[PDF](https://arxiv.org/abs/2311.16933), [Page](https://guoyww.github.io/projects/SparseCtrl)]

[arxiv 2023.11]MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model [[PDF](https://arxiv.org/abs/2311.16498), [Page](https://showlab.github.io/magicanimate)]

[arxiv 2023.12]DreaMoving: A Human Dance Video Generation Framework based on Diffusion Models [[PDF](https://arxiv.org/abs/2312.05107), [Page](https://dreamoving.github.io/dreamoving)]


[arxiv 2023.12]Fine-grained Controllable Video Generation via Object Appearance and Context [[PDF](https://arxiv.org/abs/2312.02919),[Page](https://hhsinping.github.io/factor)]

[arxiv 2023.12]Drag-A-Video: Non-rigid Video Editing with Point-based Interaction [[PDF](https://arxiv.org/abs/2312.02936),[Page](https://drag-a-video.github.io/)]

[arxiv 2023.12]Peekaboo: Interactive Video Generation via Masked-Diffusion [[PDF](https://arxiv.org/abs/2312.07509),[Page](https://jinga-lala.github.io/projects/Peekaboo/)]

[arxiv 2023.12]InstructVideo: Instructing Video Diffusion Models with Human Feedback [[PDF](https://arxiv.org/abs/2312.12490),[Page](https://instructvideo.github.io/)]

[arxiv 2024.01]Motion-Zero: Zero-Shot Moving Object Control Framework for Diffusion-Based Video Generation[[PDF](https://arxiv.org/abs/2401.10150)]

[arxiv 2024.01]Synthesizing Moving People with 3D Control [[PDF](https://arxiv.org/abs/2401.10889),[PDF](https://boyiliee.github.io/3DHM.github.io/)]

[arxiv 2024.02]Boximator: Generating Rich and Controllable Motions for Video Synthesis [[PDF](https://arxiv.org/abs/2402.01566),[Page](https://boximator.github.io/)]

[arxiv 2024.02]InteractiveVideo: User-Centric Controllable Video Generation with Synergistic Multimodal Instructions [[PDF](https://arxiv.org/abs/2402.03040),[Page](https://github.com/invictus717/InteractiveVideo)]

[arxiv 2024.02]EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions[[PDF](https://arxiv.org/abs/2402.17485),[Page](https://humanaigc.github.io/emote-portrait-alive/)]

[arxiv 2024.03]Animate Your Motion: Turning Still Images into Dynamic Videos [[PDF](https://arxiv.org/abs/2403.10179),[Page](https://mingxiao-li.github.io/smcd/)]

[arxiv 2024.03]Champ: Controllable and Consistent Human Image Animation with 3D Parametric Guidance [[PDF](https://arxiv.org/pdf/2403.14781.pdf),[Page](https://fudan-generative-vision.github.io/champ/#/)]

[arxiv 2024.03]Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework [[PDF](https://arxiv.org/abs/2403.16510), [Page](https://github.com/ICTMCG/Make-Your-Anchor)]

[arxiv 2024.04]Motion Inversion for Video Customization [[PDF](https://arxiv.org/abs/2403.20193),[Page](https://wileewang.github.io/MotionInversion/)]

[arxiv 2023.12]Generative Rendering: Controllable 4D-Guided Video Generation with 2D Diffusion Models [[PDF](https://arxiv.org/abs/2312.01409),[Page](https://primecai.github.io/generative_rendering/)]

[arxiv 2024.05]MOFA-Video: Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model [[PDF](https://arxiv.org/abs/2405.20222),[Page](https://myniuuu.github.io/MOFA_Video/)]


[arxiv 2024.06] FreeTraj: Tuning-Free Trajectory Control in Video Diffusion Models [[PDF](https://arxiv.org/abs/2406.16863),[Page](http://haonanqiu.com/projects/FreeTraj.html)]

[arxiv 2024.06] MVOC: a training-free multiple video object composition method with diffusion models [[PDF](https://arxiv.org/abs/2406.15829),[Page](https://sobeymil.github.io/mvoc.com/)]

[arxiv 2024.06] MotionBooth: Motion-Aware Customized Text-to-Video Generation [[PDF](https://arxiv.org/abs/2406.17758),[Page](https://jianzongwu.github.io/projects/motionbooth)]

[arxiv 2024.07]IDOL: Unified Dual-Modal Latent Diffusion for Human-Centric Joint Video-Depth Generation  [[PDF](https://arxiv.org/abs/2407.10937),[Page](https://yhzhai.github.io/idol/)]

[arxiv 2024.07]HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation  [[PDF](https://arxiv.org/abs/2407.17438),[Page](https://github.com/zhenzhiwang/HumanVid/)]

[arxiv 2024.07]Tora: Trajectory-oriented Diffusion Transformer for Video Generation  [[PDF](https://arxiv.org/abs/2407.21705),[Page](https://ali-videoai.github.io/tora_video/)]


[arxiv 2024.08] Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics[[PDF](https://arxiv.org/abs/2408.04631),[Page](https://vgg-puppetmaster.github.io/)]

[arxiv 2024.08] TrackGo: A Flexible and Efficient Method for Controllable Video Generation [[PDF](https://arxiv.org/abs/2408.11475),[Page](https://zhtjtcz.github.io/TrackGo-Page/)]

[arxiv 2024.08]  [[PDF](),[Page]()]

## text 
[arxiv 2024.06]  Text-Animator: Controllable Visual Text Video Generation[[PDF](https://arxiv.org/abs/2406.17777),[Page](https://laulampaul.github.io/text-animator.html)]

[arxiv 2024.08]  [[PDF](),[Page]()]

## Camera 
[arxiv 2023.12]MotionCtrl: A Unified and Flexible Motion Controller for Video Generation [[PDF](https://arxiv.org/abs/2312.03641),[Page](https://wzhouxiff.github.io/projects/MotionCtrl/)]

[arxiv 2024.02]Direct-a-Video: Customized Video Generation with User-Directed Camera Movement and Object Motion [[PDF](https://arxiv.org/pdf/2402.03162.pdf),[Page](https://direct-a-video.github.io/)]

[arxiv 2024.04]CameraCtrl: Enabling Camera Control for Text-to-Video Generation [[PDF](https://arxiv.org/abs/2404.02101),[Page](https://hehao13.github.io/projects-CameraCtrl/)]

[arxiv 2024.04]Customizing Text-to-Image Diffusion with Camera Viewpoint Control [[PDF](https://arxiv.org/abs/2404.12333),[Page](https://customdiffusion360.github.io/)]

[arxiv 2024.04]MotionMaster: Training-free Camera Motion Transfer For Video Generation[[PDF](https://arxiv.org/abs/2404.15789)]

[arxiv 2024.05] Video Diffusion Models are Training-free Motion Interpreter and Controller[[PDF](https://arxiv.org/abs/2405.14864),[Page](https://xizaoqu.github.io/moft/)]

[arxiv 2024.05] VidvidDream Generating 3D Scene with Ambient Dynamics [[PDF](https://arxiv.org/abs/2405.20334),[Page](https://vivid-dream-4d.github.io/)]

[arxiv 2024.06] CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation [[PDF](https://arxiv.org/abs/2406.02509),[Page](https://ir1d.github.io/CamCo/)]

[arxiv 2024.06]Training-free Camera Control for Video Generation[[PDF](https://arxiv.org/abs/2406.10126),[Page](https://lifedecoder.github.io/CamTrol/)]

[arxiv 2024.06] Image Conductor: Precision Control for Interactive Video Synthesis [[PDF](https://liyaowei-stu.github.io/project/ImageConductor/),[Page](https://liyaowei-stu.github.io/project/ImageConductor/)]

[arxiv 2024.07]VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control [[PDF](https://arxiv.org/abs/2407.12781),[Page](https://snap-research.github.io/vd3d/)]

[arxiv 2024.08] DreamCinema: Cinematic Transfer with Free Camera and 3D Character [[PDF](https://arxiv.org/abs/2408.12601),[Page](https://liuff19.github.io/DreamCinema)]


[arxiv 2024.08]  [[PDF](),[Page]()]


## inpainting / outpainting 
[MM 2023.09]Hierarchical Masked 3D Diffusion Model for Video Outpainting [[PDF](https://arxiv.org/abs/2309.02119)]

[arxiv 2023.11]Flow-Guided Diffusion for Video Inpainting [[PDF](https://arxiv.org/abs/2311.15368)]

[arxiv 2024.01]ActAnywhere: Subject-Aware Video Background Generation [[PDF](https://arxiv.org/abs/2401.10822), [Page](https://actanywhere.github.io/)]

[arxiv 2024.03]CoCoCo: Improving Text-Guided Video Inpainting for Better Consistency, Controllability and Compatibility [[PDF](https://arxiv.org/abs/2403.12035),[Page](https://cococozibojia.github.io/)]

[arxiv 2024.03]Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific Adaptation [[PDF](https://arxiv.org/abs/2403.13745),[Page](https://github.com/G-U-N/Be-Your-Outpainter)]

[arxiv 2024.04]AudioScenic: Audio-Driven Video Scene Editing [[PDF](https://arxiv.org/abs/2404.16581)]

[arxiv 2024.05]Semantically Consistent Video Inpainting with Conditional Diffusion Models [[PDF(https://arxiv.org/abs/2405.00251)]

[arxiv 2024.05]ReVideo: Remake a Video with Motion and Content Control [[PDF](https://arxiv.org/abs/2405.13865),[Page](https://mc-e.github.io/project/ReVideo/)]

[arxiv 2024.08]Video Diffusion Models are Strong Video Inpainter  [[PDF](https://arxiv.org/abs/2408.11402)]

[arxiv 2024.08]  [[PDF](),[Page]()]

## Video Quality 
[arxiv 2024.03]VideoElevator : Elevating Video Generation Quality with Versatile Text-to-Image Diffusion Models[[PDF](https://arxiv.org/abs/2403.05438),[Page](https://videoelevator.github.io/)]




## super-resolution
[arxiv 2023.11]Enhancing Perceptual Quality in Video Super-Resolution through Temporally-Consistent Detail Synthesis using Diffusion Models [[PDF](https://arxiv.org/abs/2311.15908)]

[arxiv 2023.12]Upscale-A-Video: Temporal-Consistent Diffusion Model for Real-World Video Super-Resolution [[PDF](https://arxiv.org/abs/2312.06640),[Page](https://shangchenzhou.com/projects/upscale-a-video/)]

[arxiv 2023.12]Video Dynamics Prior: An Internal Learning Approach for Robust Video Enhancements [[PDF](https://arxiv.org/abs/2312.07835),[Page](http://www.cs.umd.edu/~gauravsh/vdp.html)]

[arxiv 2024.03]Learning Spatial Adaptation and Temporal Coherence in Diffusion Models for Video Super-Resolution [[PDF](https://arxiv.org/abs/2403.17000)]

[arxiv 2024.04]VideoGigaGAN: Towards Detail-rich Video Super-Resolution [[PDF](https://videogigagan.github.io/assets/paper.pdf), [Page](https://videogigagan.github.io/)]

[arxiv 2024.06] EvTexture: Event-driven Texture Enhancement for Video Super-Resolution [[PDF](https://arxiv.org/abs/2406.13457),[Page](https://dachunkai.github.io/evtexture.github.io/)]

[arxiv 2024.06]  DiffIR2VR-Zero:Zero-Shot Video Restoration with Diffusion-based Image Restoration Models [[PDF](https://arxiv.org/abs/2304.06706),[Page](https://jimmycv07.github.io/DiffIR2VR_web/)]

[arxiv 2024.07]  DiffIR2VR-Zero: Zero-Shot Video Restoration with Diffusion-based Image Restoration Models [[PDF](https://arxiv.org/abs/2407.01519),[Page](https://jimmycv07.github.io/DiffIR2VR_web/)]

[arxiv 2024.07] Zero-shot Video Restoration and Enhancement Using Pre-Trained Image Diffusion Model  [[PDF](https://arxiv.org/abs/2407.01960)]

[arxiv 2024.07] VEnhancer: Generative Space-Time Enhancement for Video Generation[[PDF](https://arxiv.org/abs/2407.07667),[Page](https://vchitect.github.io/VEnhancer-project/)]


[arxiv 2024.07] Noise Calibration: Plug-and-play Content-Preserving Video Enhancement using Pre-trained Video Diffusion Models [[PDF](https://arxiv.org/abs/2407.10285),[Page](https://yangqy1110.github.io/NC-SDEdit/)]

[arxiv 2024.07]  RealViformer: Investigating Attention for Real-World Video Super-Resolution [[PDF](),[Page]()]

[arxiv 2024.08]Kalman-Inspired Feature Propagation for Video Face Super-Resolution[[PDF](https://arxiv.org/abs/2408.05205),[Page](https://jnjaby.github.io/projects/KEEP/)]

[arxiv 2024.08] Unrolled Decomposed Unpaired Learning for Controllable Low-Light Video Enhancement  [[PDF](https://arxiv.org/abs/2408.12316),[Page](https://github.com/lingyzhu0101/UDU)]

[arxiv 2024.08]   [[PDF](),[Page]()]

## downstream apps
[arxiv 2023.11]Breathing Life Into Sketches Using Text-to-Video Priors [[PDF](https://arxiv.org/abs/2311.13608),[Page](https://livesketch.github.io/)]

[arxiv 2023.11]Flow-Guided Diffusion for Video Inpainting [[PDF](https://arxiv.org/abs/2311.15368)]

[arxiv 2024.02]Animated Stickers: Bringing Stickers to Life with Video Diffusion [[PDF](https://arxiv.org/abs/2402.06088)]

[arxiv 2024.03]DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation [[PDF](https://arxiv.org/abs/2403.06845),[Page](https://drivedreamer2.github.io/)]

[arxiv 2024.03]Intention-driven Ego-to-Exo Video Generation [[PDF](https://arxiv.org/abs/2403.09194)]

[arxiv 2024.04]PhysDreamer: Physics-Based Interaction with 3D Objects via Video Generation [[PDF](https://arxiv.org/abs/2404.13026),[Page](https://physdreamer.github.io/)]

[arxiv 2024.04]Tunnel Try-on: Excavating Spatial-temporal Tunnels for High-quality Virtual Try-on in Videos [[PDF](https://arxiv.org/abs/2404.17571),[Page](https://mengtingchen.github.io/tunnel-try-on-page/)]

[arxiv 2024.04]Dance Any Beat: Blending Beats with Visuals in Dance Video Generation [[PDF](https://arxiv.org/abs/2405.09266), [Page](https://dabfusion.github.io/)]

[arxiv 2024.05] ViViD: Video Virtual Try-on using Diffusion Models  [[PDF](https://arxiv.org/abs/2405.11794),[Page](https://becauseimbatman0.github.io/ViViD)]

[arxiv 2024.05] VITON-DiT: Learning In-the-Wild Video Try-On from Human Dance Videos via Diffusion Transformers[[PDF](https://arxiv.org/abs/2405.18326),[Page](https://zhengjun-ai.github.io/viton-dit-page/)]

[arxiv 2024.07]WildVidFit: Video Virtual Try-On in the Wild via Image-Based Controlled Diffusion Models [[PDF](https://arxiv.org/abs/2407.10625), [Page](https://wildvidfit-project.github.io/)]

[arxiv 2024.07]Streetscapes: Large-scale Consistent Street View Generation Using Autoregressive Video Diffusion
 [[PDF](https://arxiv.org/abs/2407.13759), [Page](https://boyangdeng.com/streetscapes)]

[arxiv 2024.08] Panacea+: Panoramic and Controllable Video Generation for Autonomous Driving[[PDF](https://arxiv.org/abs/2408.07605), [Page](https://panacea-ad.github.io/)]

[arxiv 2024.08] [[PDF](), [Page]()]

## Concept 
[arxiv 2023.07]Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation [[PDF](https://arxiv.org/abs/2307.06940), [Page](https://videocrafter.github.io/Animate-A-Story)]

[arxiv 2023.11]VideoDreamer: Customized Multi-Subject Text-to-Video Generation with Disen-Mix Finetuning[[PDF](https://arxiv.org/pdf/%3CARXIV%20PAPER%20ID%3E.pdf),[Page](https://videodreamer23.github.io/)]

[arxiv 2023.12]VideoAssembler: Identity-Consistent Video Generation with Reference Entities using Diffusion Model [[PDF](https://arxiv.org/abs/2311.17338),[Page](https://gulucaptain.github.io/videoassembler/)]

[arxiv 2023.12]VideoBooth: Diffusion-based Video Generation with Image Prompts [[PDF](https://arxiv.org/abs/2312.00777),[Page](https://vchitect.github.io/VideoBooth-project/)]

[arxiv 2023.12]DreamVideo: Composing Your Dream Videos with Customized Subject and Motion [[PDF](https://arxiv.org/abs/2312.04433),[Page](https://dreamvideo-t2v.github.io/)]

[arxiv 2023.12]PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models [[PDF](https://pi-animator.github.io/)]

[arxiv 2024.01]CustomVideo: Customizing Text-to-Video Generation with Multiple Subjects [[PDF](https://arxiv.org/abs/2401.09962)]

[arxiv 2024.02]Magic-Me: Identity-Specific Video Customized Diffusion [[PDf](https://arxiv.org/abs/2402.09368),[Page](https://magic-me-webpage.github.io/)]

[arxiv 2024.03]EVA: Zero-shot Accurate Attributes and Multi-Object Video Editing [[PDF](https://arxiv.org/abs/2403.16111),[Page](https://knightyxp.github.io/EVA/)]

[arxiv 2024.04]AniClipart: Clipart Animation with Text-to-Video Priors [[PDF](https://arxiv.org/abs/2404.12347),[Page](https://aniclipart.github.io/)]

[arxiv 2024.04]ID-Animator: Zero-Shot Identity-Preserving Human Video Generation [[PDF](),[Page](https://id-animator.github.io/)]

[arxiv 2024.07]Still-Moving: Customized Video Generation without Customized Video Data [[PDF](https://arxiv.org/abs/2407.08674),[Page](https://still-moving.github.io/)]

[arxiv 2024.08] CustomCrafter: Customized Video Generation with Preserving Motion and Concept Composition Abilities[[PDF](https://arxiv.org/abs/2408.13239),[Page](https://customcrafter.github.io/)]


[arxiv 2024.08] [[PDF](),[Page]()]


## Talking Face 
[arxiv 2024.02]EMO Emote Portrait Alive: Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions [[PDF](https://arxiv.org/abs/2402.17485),[Page](https://humanaigc.github.io/emote-portrait-alive/)]

[arxiv 2024.04] VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time [[PDF](https://arxiv.org/abs/2404.10667),[Page](https://www.microsoft.com/en-us/research/project/vasa-1/)]

[arxiv 2024.04]MuseTalk: Real-Time High Quality Lip Synchronization with Latent Space Inpainting[[PDF](),[Page](https://github.com/TMElyralab/MuseTalk)]

[arxiv 2024.06]V-Express: Conditional Dropout for Progressive Training of Portrait Video Generation[[PDF](https://arxiv.org/abs/2406.01900),[Page](https://github.com/tencent-ailab/V-Express)]

[arxiv 2024.06]Follow-Your-Emoji: Fine-Controllable and Expressive Freestyle Portrait Animation[[PDF](),[Page](https://follow-your-emoji.github.io/)]


[arxiv 2024.07] [[PDF](),[Page]()]


## Image-to-video Generation 
[arxiv 2023.09]VideoGen: A Reference-Guided Latent Diffusion Approach for High Definition Text-to-Video Generation [[PDF](https://arxiv.org/abs/2309.00398)]

[arxiv 2023.09]Generative Image Dynamics [[PDF](https://arxiv.org/abs/2309.07906),[Page](http://generative-dynamics.github.io/)]

[arxiv 2023.10]DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors [[PDF](https://arxiv.org/abs/2310.12190), [Page](https://github.com/AILab-CVC/VideoCrafter)]

[arxiv 2023.11]SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction [[PDF](https://arxiv.org/abs/2310.20700),[Page](https://vchitect.github.io/SEINE-project/)]

[arxiv 2023.11]I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models
[[PDF](https://arxiv.org/abs/2311.04145),[Page](https://i2vgen-xl.github.io/page04.html)]

[arxiv 2023.11]Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning [[PDF](https://arxiv.org/abs/2311.10709),[Page](https://emu-video.metademolab.com/)]

[arxiv 2023.11]MoVideo: Motion-Aware Video Generation with Diffusion Models[[PDF](https://github.com/JingyunLiang/MoVideo/releases/download/v0.0/MoVideo.pdf),[Page](https://jingyunliang.github.io/MoVideo/)]

[arxiv 2023.11]Make Pixels Dance: High-Dynamic Video Generation[[PDF](),[Page](https://makepixelsdance.github.io/)]

[arxiv 2023.11]Decouple Content and Motion for Conditional Image-to-Video Generation [[PDF](https://arxiv.org/abs/2311.14294)]

[arxiv 2023.12]ART•V: Auto-Regressive Text-to-Video Generation with Diffusion Models [[PDF](https://arxiv.org/abs/2311.18834), [Page](https://warranweng.github.io/art.v)]

[arxiv 2023.12]MicroCinema: A Divide-and-Conquer Approach for Text-to-Video Generation [[PDF](https://arxiv.org/abs/2311.18829), [Page](https://wangyanhui666.github.io/MicroCinema.github.io/)]

[arxiv 2023.12]DreamVideo: High-Fidelity Image-to-Video Generation with Image Retention and Text Guidance [[PDF](https://arxiv.org/abs/2312.03018),[Page](https://anonymous0769.github.io/DreamVideo/)]

[arxiv 2023.12]LivePhoto: Real Image Animation with Text-guided Motion Control [[PDF](https://arxiv.org/abs/2312.02928), [Page](https://xavierchen34.github.io/LivePhoto-Page/)]

[arxiv 2023.12]I2V-Adapter: A General Image-to-Video Adapter for Video Diffusion Models [[PDF](https://arxiv.org/abs/2312.16693)]

[arxiv 2023.11] Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning [[PDF](https://arxiv.org/abs/2311.10709),[Page](https://emu-video.metademolab.com/)]

[arxiv 2024.01]UniVG: Towards UNIfied-modal Video Generation [[PDF](https://arxiv.org/abs/2401.09084),[Page](https://univg-baidu.github.io/)]

[arxiv 2024.03]Tuning-Free Noise Rectification for High Fidelity Image-to-Video Generation [[PDF](https://arxiv.org/abs/2403.02827),[Page](https://noise-rectification.github.io/)]

[arxiv 2024.03]AtomoVideo: High Fidelity Image-to-Video Generation [[PDF](https://arxiv.org/abs/2403.01800),[Page](https://atomo-video.github.io/)]

[arxiv 2024.03]Pix2Gif: Motion-Guided Diffusion for GIF Generation[[PDF](https://arxiv.org/abs/2403.04634),[Page](https://hiteshk03.github.io/Pix2Gif/)]

[arxiv 2024.03]Follow-Your-Click: Open-domain Regional Image Animation via Short Prompts [[PDF](https://arxiv.org/abs/2403.08268),[Page](https://github.com/mayuelala/FollowYourClick)]

[arxiv 2024.03]TimeRewind: Rewinding Time with Image-and-Events Video Diffusion [[PDF](https://arxiv.org/abs/2403.13800),[Page](https://timerewind.github.io/)]

[arxiv 2024.03]TRIP: Temporal Residual Learning with Image Noise Prior for Image-to-Video Diffusion Models [[PDF](https://arxiv.org/abs/2403.17005),[Page](https://trip-i2v.github.io/TRIP/)]

[arxiv 2024.04]LASER: Tuning-Free LLM-Driven Attention Control for Efficient Text-conditioned Image-to-Animation [[PDF](https://arxiv.org/abs/2404.13558)]

[arxiv 2024.04]TI2V-Zero: Zero-Shot Image Conditioning for Text-to-Video Diffusion Models [[PDF](https://arxiv.org/abs/2404.16306),[Page](https://merl.com/research/highlights/TI2V-Zero)]

[arxiv 2024.06] I4VGen: Image as Stepping Stone for Text-to-Video Generation[[PDF](https://arxiv.org/abs/2406.02230),[Page](https://xiefan-guo.github.io/i4vgen/)]

[arxiv 2024.06] AID: Adapting Image2Video Diffusion Models for Instruction-based Video Prediction[[PDF](https://arxiv.org/abs/2406.06465),[Page](https://chenhsing.github.io/AID/)]

[arxiv 2024.06] Identifying and Solving Conditional Image Leakage in Image-to-Video Generation[[PDF](https://arxiv.org/pdf/2406.15735),[Page](https://cond-image-leak.github.io/)]

[arxiv 2024.07]Cinemo: Consistent and Controllable Image Animation with Motion Diffusion Models [[PDF](https://arxiv.org/abs/2407.15642),[Page](https://maxin-cn.github.io/cinemo_project/)]



[arxiv 2024.07] [[PDF](),[Page]()]


## 4D generation 
[arxiv 2023.11]Animate124: Animating One Image to 4D Dynamic Scene [[PDF](https://arxiv.org/abs/2311.14603),[Page](https://animate124.github.io/)]

[arxiv 2023.12]4D-fy: Text-to-4D Generation Using Hybrid Score Distillation Sampling[[PDF](https://arxiv.org/abs/2311.17984), [Page](https://sherwinbahmani.github.io/4dfy)]

[arxiv 2023.12]4DGen: Grounded 4D Content Generation with Spatial-temporal Consistency [[PDF](https://arxiv.org/abs/2312.17225),[Page](https://vita-group.github.io/4DGen/)]

[arxiv 2023.12]DreamGaussian4D: Generative 4D Gaussian Splatting [[PDF](https://arxiv.org/abs/2312.17142), [Page](https://jiawei-ren.github.io/projects/dreamgaussian4d)]


[arxiv 2024.07] [[PDF](),[Page]()]


## Audio-to-video Generation
[arxiv 2023.09]Diverse and Aligned Audio-to-Video Generation via Text-to-Video Model Adaptation [[PDF](https://arxiv.org/abs/2309.16429)]

[arxiv 2024.02]Seeing and Hearing Open-domain Visual-Audio Generation with Diffusion Latent Aligners [[PDF](https://arxiv.org/abs/2402.17723),[Page](https://yzxing87.github.io/Seeing-and-Hearing/)]


[arxiv 2024.04]TAVGBench: Benchmarking Text to Audible-Video Generation [[PDF](https://arxiv.org/abs/2404.14381),[Page](https://github.com/OpenNLPLab/TAVGBench)]

[arxiv 2024.07] [[PDF](),[Page]()]




## editing with video models 
[arxiv 2023.12]VIDiff: Translating Videos via Multi-Modal Instructions with Diffusion Models[[PDF](https://arxiv.org/abs/2311.18837),[Page](https://chenhsing.github.io/VIDiff)]

[arxiv 2023.12]Neutral Editing Framework for Diffusion-based Video Editing [[PDF](https://arxiv.org/abs/2312.06708),[Page](https://neuedit.github.io/)]

[arxiv 2024.01]FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis[[PDF](https://arxiv.org/abs/2312.17681),[Page](https://jeff-liangf.github.io/projects/flowvid/)]

[arxiv 2024.02]UniEdit: A Unified Tuning-Free Framework for Video Motion and Appearance Editing [[PDF](https://arxiv.org/abs/2402.13185),[Page](https://jianhongbai.github.io/UniEdit/)]

[arxiv 2024.02]Customize-A-Video: One-Shot Motion Customization of Text-to-Video Diffusion Models [[PDF](https://arxiv.org/abs/2402.14780),[Page](https://anonymous-314.github.io/)]

[arxiv 2024.03]FastVideoEdit: Leveraging Consistency Models for Efficient Text-to-Video Editing[[PDF](https://arxiv.org/abs/2403.06269)]

[arxiv 2024.03]DreamMotion: Space-Time Self-Similarity Score Distillation for Zero-Shot Video Editing [[PDF](https://arxiv.org/abs/2403.12002),[Page](https://hyeonho99.github.io/dreammotion/)]

[arxiv 2024.03]EffiVED:Efficient Video Editing via Text-instruction Diffusion Models [[PDF](https://arxiv.org/abs/2403.11568)]

[arxiv 2024.03]Videoshop: Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion [[PDF](https://arxiv.org/abs/2403.14617),[Page](https://videoshop-editing.github.io/)]

[arxiv 2024.03]AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks  [[PDF](https://arxiv.org/pdf/2403.14468.pdf),[Page](https://tiger-ai-lab.github.io/AnyV2V/)]

[arxiv 2024.04]Investigating the Effectiveness of Cross-Attention to Unlock Zero-Shot Editing of Text-to-Video Diffusion Models [[PDF](https://arxiv.org/abs/2404.05519)]

[arxiv 2024.05]I2VEdit: First-Frame-Guided Video Editing via Image-to-Video Diffusion Models[[PDF](https://arxiv.org/abs/2405.16537),[Page](https://i2vedit.github.io/)]

[arxiv 2024.05] Streaming Video Diffusion: Online Video Editing with Diffusion Models[[PDF](https://arxiv.org/abs/2405.1972),[Page](https://github.com/Chenfeng1271/SVDiff)]

[arxiv 2024.06]Zero-Shot Video Editing through Adaptive Sliding Score Distillation[[PDF](https://arxiv.org/abs/2406.04888),[Page](https://nips24videoedit.github.io/zeroshot_videoedit/)]

[arxiv 2024.06]FRAG: Frequency Adapting Group for Diffusion Video Editing[[PDF](https://arxiv.org/abs/2406.06044)]

[arxiv 2024.07] Fine-gained Zero-shot Video Sampling[[PDF](https://arxiv.org/pdf/2407.21475),[Page](https://densechen.github.io/zss/)]

[arxiv 2024.07] [[PDF](),[Page]()]


## Editing with image model 
*[arxiv 2022.12]Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation [[PDF](https://arxiv.org/abs/2212.11565), [Page](https://tuneavideo.github.io/)]

[arxiv 2023.03]Video-P2P: Video Editing with Cross-attention Control [[PDF](https://arxiv.org/abs/2303.04761), [Page](https://video-p2p.github.io/)]

[arxiv 2023.03]Edit-A-Video: Single Video Editing with Object-Aware Consistency [[PDF](https://arxiv.org/abs/2303.07945), [Page](https://edit-a-video.github.io/)]

[arxiv 2023.03]FateZero: Fusing Attentions for Zero-shot Text-based Video Editing [[PDF](https://arxiv.org/abs/2303.09535), [Page](https://github.com/ChenyangQiQi/FateZero)]

[arxiv 2023.03]Pix2Video: Video Editing using Image Diffusion [[PDF](https://arxiv.org/abs/2303.12688)]

->[arxiv 2023.03]Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators [[PDF](https://arxiv.org/abs/2303.13439), [code](https://github.com/Picsart-AI-Research/Text2Video-Zero)]

[arxiv 2023.03]Zero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models[[PDF](https://arxiv.org/abs/2303.17599),[code](https://github.com/baaivision/vid2vid-zero)]

[arxiv 2023.04]Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free Videos[[PDF](https://arxiv.org/abs/2304.01186)]

[arxiv 2023.05]ControlVideo: Training-free Controllable Text-to-Video Generation [[PDF](https://arxiv.org/abs/2305.13077), [Page](https://github.com/YBYBZhang/ControlVideo)]

[arxiv 2023.05]Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models[[PDF](https://arxiv.org/abs/2305.13840), [Page](https://controlavideo.github.io/)]

[arxiv-2023.05]Large Language Models are Frame-level Directors for Zero-shot Text-to-Video Generation [[PDF](https://arxiv.org/abs/2305.14330), [Page](https://github.com/KU-CVLAB/DirecT2V)]

[arxiv 2023.05]Video ControlNet: Towards Temporally Consistent Synthetic-to-Real Video Translation Using Conditional Image Diffusion Models [[PDF](https://arxiv.org/abs/2305.19193)]

[arxiv 2023.05]SAVE: Spectral-Shift-Aware Adaptation of Image Diffusion Models for Text-guided Video Editing [[PDF](https://arxiv.org/abs/2305.18670)]

[arxiv 2023.05]InstructVid2Vid: Controllable Video Editing with Natural Language Instructions [[PDF](https://arxiv.org/abs/2305.12328)]

[arxiv 2023.05] ControlVideo: Adding Conditional Control for One Shot Text-to-Video Editing [[PDF](https://arxiv.org/pdf/2305.17098.pdf), [Page](https://ml.cs.tsinghua.edu.cn/controlvideo/)]

[arxiv 2023.05]Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising [[PDF](https://arxiv.org/abs/2305.18264),[Page](https://g-u-n.github.io/projects/gen-long-video/index.html)]

[arxiv 2023.06]Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance [[PDF](https://arxiv.org/abs/2306.00943), [Page](https://doubiiu.github.io/projects/Make-Your-Video/)]

[arxiv 2023.06]VidEdit: Zero-Shot and Spatially Aware Text-Driven Video Editing [[PDF](https://arxiv.org/abs/2306.08707),[Page](https://videdit.github.io/)]

*[arxiv 2023.06]Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation [[PDF](https://arxiv.org/abs/2306.07954), [Page](https://anonymous-31415926.github.io/)]

*[arxiv 2023.07]AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning [[PDF](https://arxiv.org/abs/2307.04725),  [Page](https://animatediff.github.io/)]

*[arxiv 2023.07]TokenFlow: Consistent Diffusion Features for Consistent Video Editing [[PDF](https://arxiv.org/pdf/2307.10373.pdf),[Page](https://diffusion-tokenflow.github.io/)]

[arxiv 2023.07]VideoControlNet: A Motion-Guided Video-to-Video Translation Framework by Using Diffusion Model with ControlNet [[PDF](https://arxiv.org/pdf/2307.14073.pdf), [Page](https://vcg-aigc.github.io/)]

[arxiv 2023.08]CoDeF: Content Deformation Fields for Temporally Consistent Video Processing [[PDF](https://arxiv.org/pdf/2308.07926.pdf), [Page](https://qiuyu96.github.io/CoDeF/)]

[arxiv 2023.08]DragNUWA: Fine-grained Control in Video Generation by Integrating Text, Image, and Trajectory [[PDF](https://arxiv.org/abs/2308.08089), [Page](https://www.microsoft.com/en-us/research/project/dragnuwa/)]

[arxiv 2023.08]StableVideo: Text-driven Consistency-aware Diffusion Video Editing [[PDF](https://arxiv.org/abs/2308.09592), [Page](https://github.com/rese1f/StableVideo)]

[arxiv 2023.08]Edit Temporal-Consistent Videos with Image Diffusion Model [[PDF](https://arxiv.org/abs/2308.09091)]

[arxiv 2023.08]EVE: Efficient zero-shot text-based Video Editing with Depth Map Guidance and Temporal Consistency Constraints [[PDF](https://arxiv.org/pdf/2308.10648.pdf)]

[arxiv 2023.08]MagicEdit: High-Fidelity and Temporally Coherent Video Editing [[PDF](https://arxiv.org/pdf/2308.14749), [Page](https://magic-edit.github.io/)]

[arxiv 2023.09]MagicProp: Diffusion-based Video Editing via Motionaware Appearance Propagation[[PDF](https://arxiv.org/pdf/2309.00908.pdf)]

[arxiv 2023.09]Free-Bloom: Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator[[PDF](https://arxiv.org/abs/2309.14494), [Page](https://github.com/SooLab/Free-Bloom)]

[arxiv 2023.09]CCEdit: Creative and Controllable Video Editing via Diffusion Models [[PDF](https://arxiv.org/abs/2309.16496)]

[arxiv 2023.10]Ground-A-Video: Zero-shot Grounded Video Editing using Text-to-image Diffusion Models [[PDF](https://arxiv.org/abs/2310.01107),[Page](https://ground-a-video.github.io/)]

[arxiv 2023.10]FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video editing [[PDF](https://arxiv.org/abs/2310.05922),[Page](https://flatten-video-editing.github.io/)]

[arxiv 2023.10]ConditionVideo: Training-Free Condition-Guided Text-to-Video Generation [[PDF](https://arxiv.org/abs/2310.07697),[Page](https://pengbo807.github.io/conditionvideo-website/)]

[arxiv 2023.10, nerf] DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and View-Change Human-Centric Video Editing [[PDF](https://arxiv.org/abs/2310.10624), [Page](https://showlab.github.io/DynVideo-E/)]

[arxiv 2023.10]LAMP: Learn A Motion Pattern for Few-Shot-Based Video Generation [[PDF](https://arxiv.org/abs/2310.10769),[Page](https://rq-wu.github.io/projects/LAMP/index.html)]

[arxiv 2023.11]LATENTWARP: CONSISTENT DIFFUSION LATENTS FOR ZERO-SHOT VIDEO-TO-VIDEO TRANSLATION [[PDF](https://arxiv.org/pdf/2311.00353.pdf)]

[arxiv 2023.11]Cut-and-Paste: Subject-Driven Video Editing with Attention Control[[PDF](https://arxiv.org/abs/2311.11697)]

[arxiv 2023.11]MotionZero:Exploiting Motion Priors for Zero-shot Text-to-Video Generation [[PDF](https://arxiv.org/abs/2311.16635)]

[arxiv 2023.12]Motion-Conditioned Image Animation for Video Editing [[PDF](https://arxiv.org/pdf/2311.18827.pdf), [Page](https://facebookresearch.github.io/MoCA/)]

[arxiv 2023.12]RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models [[PDF](https://arxiv.org/abs/2312.04524),[Page](https://rave-video.github.io/)]

[arxiv 2023.12]DiffusionAtlas: High-Fidelity Consistent Diffusion Video Editing [[PDF](https://arxiv.org/abs/2312.03772)]

[arxiv 2023.12]MagicStick: Controllable Video Editing via Control Handle Transformations [[PDF](https://arxiv.org/abs/2312.03047),[Page](https://github.com/mayuelala/MagicStick)]

[arxiv 2023.12]SAVE: Protagonist Diversification with Structure Agnostic Video Editing [[PDF](https://arxiv.org/abs/2312.02503),[Page](https://ldynx.github.io/SAVE/)]

[arxiv 2023.12]VidToMe: Video Token Merging for Zero-Shot Video Editing [[PDF](https://arxiv.org/abs/2312.10656),[Page](https://vidtome-diffusion.github.io/)]

[arxiv 2023.12]Fairy: Fast Parallelized Instruction-Guided Video-to-Video Synthesis [[PDF](https://arxiv.org/abs/2312.13834),[Page](https://fairy-video2video.github.io/)]

[arxiv 2024.1]Object-Centric Diffusion for Efficient Video Editing [[PDF](https://arxiv.org/abs/2401.05735)]

[arxiv 2024.1]VASE: Object-Centric Shape and Appearance Manipulation of Real Videos [[PDF](https://arxiv.org/abs/2401.02473),[Page](https://helia95.github.io/vase-website/)]

[arxiv 2024.03]FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation [[PDF](https://arxiv.org/abs/2403.12962),[Page](https://www.mmlab-ntu.com/project/fresco/)]

[arxiv 2024.04]GenVideo: One-shot Target-image and Shape Aware Video Editing using T2I Diffusion Models [[PDF](https://arxiv.org/abs/2404.12541)]

[arxiv 2024.05]Edit-Your-Motion: Space-Time Diffusion Decoupling Learning for Video Motion Editing [[PDF](https://arxiv.org/abs/2405.04496),[Page](https://github.com/yiiizuo/Edit-Your-Motion)]

[arxiv 2024.05] Slicedit: Zero-Shot Video Editing With Text-to-Image Diffusion Models Using Spatio-Temporal Slices  [[PDF](https://arxiv.org/abs/2405.12211),[Page](https://matankleiner.github.io/slicedit/)]

[arxiv 2024.05] Looking Backward: Streaming Video-to-Video Translation with Feature Banks [[PDF](https://arxiv.org/abs/2405.15757),[Page](https://jeff-liangf.github.io/projects/streamv2v)]

[arxiv 2024.06]Follow-Your-Pose v2: Multiple-Condition Guided Character Image Animation for Stable Pose Control [[PDF](https://arxiv.org/abs/2406.03035)]

[arxiv 2024.06]NaRCan: Natural Refined Canonical Image with Integration of Diffusion Prior for Video Editing[[PDF](https://arxiv.org/abs/2406.06523),[Page](https://koi953215.github.io/NaRCan_page/)]

[arxiv 2024.06]VIA: A Spatiotemporal Video Adaptation Framework for Global and Local Video Editing [[PDF](https://arxiv.org/abs/2406.12831),[Page](https://via-video.github.io/)]

[arxiv 2024.06]  [[PDF](),[Page]()] 





## :point_right: Video Completion (animation, interpolation, prediction)
[arxiv 2022; Meta] Tell Me What Happened: Unifying Text-guided Video Completion via Multimodal Masked Video Generation \[[PDF](https://arxiv.org/pdf/2211.12824.pdf), code]

[arxiv 2023.03]LDMVFI: Video Frame Interpolation with Latent Diffusion Models[[PDF](https://arxiv.org/abs/2303.09508)]

*[arxiv 2023.03]Seer: Language Instructed Video Prediction with Latent Diffusion Models [[PDF](https://arxiv.org/abs/2303.14897)]

[arxiv 2023.10]DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors [[PDF](https://arxiv.org/abs/2310.12190), [Page](https://github.com/AILab-CVC/VideoCrafter)]


[arxiv 2024.03]Explorative Inbetweening of Time and Space [[PDF](https://time-reversal.github.io/),[Page](https://time-reversal.github.io/)]

[arxiv 2024.04]Video Interpolation With Diffusion Models [[PDF](https://arxiv.org/abs/2404.01203),[Page](https://vidim-interpolation.github.io/)]

[arxiv 2024.04]Sparse Global Matching for Video Frame Interpolation with Large Motion [[PDF](https://arxiv.org/abs/2404.06913),[Page](https://sgm-vfi.github.io/)]

[arxiv 2024.04]LADDER: An Efficient Framework for Video Frame Interpolation [[PDF](https://arxiv.org/abs/2404.11108)]

[arxiv 2024.04]Motion-aware Latent Diffusion Models for Video Frame Interpolation [[PDF](https://arxiv.org/abs/2404.13534)]

[arxiv 2024.04]Event-based Video Frame Interpolation with Edge Guided Motion Refinement [[PDF](https://arxiv.org/abs/2404.18156)]

[arxiv 2024.04]StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation [[PDF](https://arxiv.org/abs/2405.01434),[Page](https://github.com/HVision-NKU/StoryDiffusion)]

[arxiv 2024.04]Frame Interpolation with Consecutive Brownian Bridge Diffusion[[PDF](https://arxiv.org/abs/2405.05953),[Page](https://zonglinl.github.io/videointerp/)]

[arxiv 2024.05]ToonCrafter: Generative Cartoon Interpolation [[PDF](https://arxiv.org/abs/2405.17933),[Page](https://doubiiu.github.io/projects/ToonCrafter/)]

[arxiv 2024.06]Disentangled Motion Modeling for Video Frame Interpolation [[PDF](https://arxiv.org/abs/2406.17256),[Page](https://github.com/JHLew/MoMo)]

[arxiv 2024.07] VFIMamba: Video Frame Interpolation with State Space Models [[PDF](https://arxiv.org/abs/2407.02315),[Page](https://github.com/MCG-NJU/VFIMamba)]


[arxiv 2024.07] [[PDF](),[Page]()]


## motion transfer 
[arxiv 2023.05]LEO: Generative Latent Image Animator for Human Video Synthesis [[PDF](https://arxiv.org/abs/2305.03989),[Page](https://wyhsirius.github.io/LEO-project/)]

*[arxiv 2023.03]Conditional Image-to-Video Generation with Latent Flow Diffusion Models [[PDF](https://arxiv.org/abs/2303.13744)]

[arxiv 2023.07]DisCo: Disentangled Control for Referring Human Dance Generation in Real World
[[PDF](https://arxiv.org/abs/2307.00040), [Page](https://disco-dance.github.io/)]

[arxiv 2023.12]MotionEditor: Editing Video Motion via Content-Aware Diffusion [[PDF](https://arxiv.org/abs/2311.18830),[Page](https://francis-rings.github.io/MotionEditor/)]

[arxiv 2023.12]Customizing Motion in Text-to-Video Diffusion Models [[PDF](https://arxiv.org/abs/2312.04966),[Page](https://joaanna.github.io/customizing_motion/)]

[arxiv 2023.12]MotionCrafter: One-Shot Motion Customization of Diffusion Models [[PDF](https://arxiv.org/abs/2312.05288)]

[arxiv 2023.11]MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model[[PDF](https://arxiv.org/abs/2311.16498),[Page](https://showlab.github.io/magicanimate)]

[arxiv 2023.12] Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation[[PDF](https://arxiv.org/abs/2311.17117),[Page](https://humanaigc.github.io/animate-anyone/)]

[arxiv 2024.01]Motion-Zero: Zero-Shot Moving Object Control Framework for Diffusion-Based Video Generation[[PDF](https://arxiv.org/abs/2401.10150)]

[arxiv 2024.03]Spectral Motion Alignment for Video Motion Transfer using Diffusion Models[[PDF](https://arxiv.org/abs/2403.15249),[Page](https://geonyeong-park.github.io/spectral-motion-alignment/)]

[arxiv 2024.05]ReVideo: Remake a Video with Motion and Content Control [[PDF](https://arxiv.org/abs/2405.13865),[Page](https://mc-e.github.io/project/ReVideo/)]

[arxiv 2024.05]VividPose: Advancing Stable Video Diffusion for Realistic Human Image Animation  [[PDF](https://arxiv.org/abs/2405.18156)]

[arxiv 2024.05]Disentangling Foreground and Background Motion for Enhanced Realism in Human Video Generation [[PDF](https://arxiv.org/abs/2405.16393),[Page](https://liujl09.github.io/humanvideo_movingbackground/)]

[arxiv 2024.05] MusePose: a Pose-Driven Image-to-Video Framework for Virtual Human Generation. [[PDF](),[Page](https://github.com/TMElyralab/MusePose?tab=readme-ov-file)]

[arxiv 2024.05]MotionFollower: Editing Video Motion via Lightweight Score-Guided Diffusion [[PDF](https://arxiv.org/abs/2405.20325),[Page](https://francis-rings.github.io/MotionFollower/)]

[arxiv 2024.06] UniAnimate: Taming Unified Video Diffusion Models for Consistent Human Image Animation[[PDF](https://arxiv.org/abs/2406.01188),[Page](https://unianimate.github.io/)]

[arxiv 2024.06] Searching Priors Makes Text-to-Video Synthesis Better[[PDF](https://arxiv.org/abs/2406.03215),[Page](https://hrcheng98.github.io/Search_T2V/)]

[arxiv 2024.06]MotionClone: Training-Free Motion Cloning for Controllable Video Generation[[PDF](https://arxiv.org/pdf/2406.05338)]

[arxiv 2024.07] [[PDF](),[Page]()]



## style transfer 
[arxiv 2023.06]Probabilistic Adaptation of Text-to-Video Models [[PDF](https://arxiv.org/abs/2306.01872)]

[arxiv 2023.11]Highly Detailed and Temporal Consistent Video Stylization via Synchronized Multi-Frame Diffusion[[PDF](https://arxiv.org/abs/2311.14343)]

[arxiv 2023.12]StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter[[PDF](https://arxiv.org/abs/2312.00330),[Page](https://gongyeliu.github.io/StyleCrafter.github.io/)]

[arxiv 2023.12]DragVideo: Interactive Drag-style Video Editing [[PDF](https://arxiv.org/abs/2312.02216)]

[arxiv 2024.03]FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation [[PDF](https://arxiv.org/abs/2403.12962),[Page](https://github.com/williamyang1991/fresco)]

[arxiv 2024.07] [[PDF](),[Page]()]

## Evaluation 
[arxiv 2023.10]EvalCrafter: Benchmarking and Evaluating Large Video Generation Models [[PDF](https://arxiv.org/abs/2310.11440),[Page](https://evalcrafter.github.io/)]

[arxiv 2023.11]FETV: A Benchmark for Fine-Grained Evaluation of Open-Domain Text-to-Video Generation [[PDF](https://arxiv.org/abs/2311.01813)]

[arxiv 2023.11]Online Video Quality Enhancement with Spatial-Temporal Look-up Tables [[PDF](https://arxiv.org/abs/2311.13616)]

[arxiv 2024.03]STREAM: Spatio-TempoRal Evaluation and Analysis Metric for Video Generative Models [[PDF](https://arxiv.org/abs/2403.09669)]

[arxiv 2024.03]Subjective-Aligned Dateset and Metric for Text-to-Video Quality Assessment [[PDF](https://arxiv.org/abs/2403.11956)]

[arxiv 2024.06] GenAI Arena: An Open Evaluation Platform for Generative Models[[PDF](https://arxiv.org/abs/2406.04485),[Page](https://huggingface.co/spaces/TIGER-Lab/GenAI-Arena)]

[arxiv 2024.06]VideoPhy: Evaluating Physical Commonsense for Video Generation [[PDF](http://arxiv.org/abs/2406.03520),[Page](https://videophy.github.io/)]

[arxiv 2024.07]T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video Generation [[PDF](https://arxiv.org/abs/2407.14505),[Page](https://t2v-compbench.github.io/)]

[arxiv 2024.07]Fr\'echet Video Motion Distance: A Metric for Evaluating Motion Consistency in Videos [[PDF](https://arxiv.org/abs/2407.16124)]


[arxiv 2024.07] [[PDF](),[Page]()]

## Survey
[arxiv 2023.03]A Survey on Video Diffusion Models [[PDF](https://arxiv.org/abs/2310.10647)]

[arxiv 2024.05]Video Diffusion Models: A Survey [[PDF](https://arxiv.org/abs/2405.03150)]

[arxiv 2024.07]Diffusion Model-Based Video Editing: A Survey [[PDF](https://arxiv.org/abs/2407.07111),[Page](https://github.com/wenhao728/awesome-diffusion-v2v)]

[ResearchGate 2024.07]Conditional Video Generation Guided by Multimodal Inputs: A Comprehensive Survey [[PDF](https://www.researchgate.net/publication/382443305_Conditional_Video_Generation_Guided_by_Multimodal_Inputs_A_Comprehensive_Survey)]


## Evaluation 
[ICCV 2023]Exploring Video Quality Assessment on User Generated Contents from Aesthetic and Technical Perspectives [[PDF](https://arxiv.org/abs/2211.04894),[Page](https://github.com/VQAssessment/DOVER)]

[arxiv 2023.10]EvalCrafter: Benchmarking and Evaluating Large Video Generation Models[[PDF](https://arxiv.org/abs/2310.11440), [Page](https://arxiv.org/abs/2310.11440)]

[arxiv 2023.11]HIDRO-VQA: High Dynamic Range Oracle for Video Quality Assessment [[PDF](https://arxiv.org/abs/2311.11059)]

[arxiv 2023.12]VBench: Comprehensive Benchmark Suite for Video Generative Models [[PDF](https://arxiv.org/abs/2311.17982), [Page](https://vchitect.github.io/VBench-project/)]

[arxiv 2024.02]Perceptual Video Quality Assessment: A Survey [[PDF](https://arxiv.org/abs/2402.03413)]

[arxiv 2024.02]KVQ: Kaleidoscope Video Quality Assessment for Short-form Videos [[PDf](https://arxiv.org/abs/2402.07220)]

[arxiv 2024.03]Modular Blind Video Quality Assessment [[PDF](https://arxiv.org/abs/2402.19276)]




## Speed 
[arxiv 2023.12]F3-Pruning: A Training-Free and Generalized Pruning Strategy towards Faster and Finer Text-to-Video Synthesis [[PDF](https://arxiv.org/abs/2312.03459)]

[arxiv 2023.12]VideoLCM: Video Latent Consistency Model [[PDF](https://arxiv.org/abs/2312.09109)]

[arxiv 2024.01]FlashVideo: A Framework for Swift Inference in Text-to-Video Generation [[PDF](https://arxiv.org/abs/2401.00869)]

[arxiv 2024.01]AnimateLCM: Accelerating the Animation of Personalized Diffusion Models and Adapters with Decoupled Consistency Learning [[PDF](https://arxiv.org/abs/2402.00769),[Page](https://animatelcm.github.io/)]

[arxiv 2024.03]AnimateDiff-Lightning: Cross-Model Diffusion Distillation [[PDF](https://arxiv.org/abs/2403.12706)]

[arxiv 2024.05] T2V-Turbo: Breaking the Quality Bottleneck of Video Consistency Model with Mixed Reward Feedback[[PDF](https://arxiv.org/abs/2405.18750),[Page](https://t2v-turbo.github.io/)]

[arxiv 2024.05] PCM : Phased Consistency Model[[PDF](https://arxiv.org/abs/2405.18407),[Page](https://g-u-n.github.io/projects/pcm/)]

[arxiv 2024.06]SF-V: Single Forward Video Generation Model [[PDF](https://arxiv.org/abs/2406.04324),[Page](https://snap-research.github.io/SF-V/)]

[arxiv 2024.07]QVD: Post-training Quantization for Video Diffusion Models [[PDF](https://arxiv.org/abs/2407.11585),[Page]()]

[arxiv 2024.08]Real-Time Video Generation with Pyramid Attention Broadcast [[PDF](https://arxiv.org/abs/2408.12588),[Page](https://github.com/NUS-HPC-AI-Lab/VideoSys)]

[arxiv 2024.08] [[PDF](),[Page]()]

## Others 
[arxiv 2023.05]AADiff: Audio-Aligned Video Synthesis with Text-to-Image Diffusion [[PDF](https://arxiv.org/abs/2305.04001)]

[arxiv 2023.05]Multi-object Video Generation from Single Frame Layouts [[PDF](https://arxiv.org/abs/2305.03983)]

[arxiv 2023.06]Learn the Force We Can: Multi-Object Video Generation from Pixel-Level Interactions [[PDF](https://arxiv.org/abs/2306.03988)]

[arxiv 2023.08]DiffSynth: Latent In-Iteration Deflickering for Realistic Video Synthesis [[PDF](https://arxiv.org/abs/2308.03463)]



## CV Related 
[arxiv 2022.12; ByteDace]PV3D: A 3D GENERATIVE MODEL FOR PORTRAIT VIDEO GENERATION [[PDF](https://arxiv.org/pdf/2212.06384.pdf)]

[arxiv 2022.12]MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation[[PDF](https://arxiv.org/pdf/2212.09478.pdf)]

[arxiv 2022.12]Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation [[PDF](https://arxiv.org/pdf/2212.11565.pdf), [Page](https://tuneavideo.github.io/)]

[arxiv 2023.01]Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation [[PDF](https://arxiv.org/pdf/2301.03396.pdf), [Page](https://mstypulkowski.github.io/diffusedheads)]

[arxiv 2023.01]DiffTalk: Crafting Diffusion Models for Generalized Talking Head Synthesis [[PDF](https://arxiv.org/pdf/2301.03786.pdf), [Page](https://sstzal.github.io/DiffTalk/)]

[arxiv 2023.02 Google]Scaling Vision Transformers to 22 Billion Parameters [[PDF](https://arxiv.org/abs/2302.05442)]

[arxiv 2023.05]VDT: An Empirical Study on Video Diffusion with Transformers [[PDF](https://arxiv.org/abs/2305.13311), [code](https://github.com/RERV/VDT)]

[arxiv 2024] MAGVIT-V2 : Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation [[PDF](https://arxiv.org/abs/2310.05737)]

[arxiv 2024.08]Sapiens: Foundation for Human Vision Models [[PDF](https://arxiv.org/abs/2408.12569),[Page](https://about.meta.com/realitylabs/codecavatars/sapiens)]


## NLP related
[arxiv 2022.10]DIFFUSEQ: SEQUENCE TO SEQUENCE TEXT GENERATION WITH DIFFUSION MODELS [[PDF](https://arxiv.org/pdf/2210.08933.pdf)]

[arxiv 2023.02]The Flan Collection: Designing Data and Methods for Effective Instruction Tuning [[PDF](https://arxiv.org/pdf/2301.13688.pdf)]





## Speech 
[arxiv 2023.01]Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers[[PDF](https://arxiv.org/abs/2301.02111), [Page](https://valle-demo.github.io/)]

