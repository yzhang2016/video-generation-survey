# Video Generation Survey
A reading list of video generation

## Repo for open-sora

[2024.03] [HPC-AI Open-Sora](https://github.com/hpcaitech/Open-Sora) 

[2024.03] [PKU Open-Sora Plan](https://github.com/PKU-YuanGroup/Open-Sora-Plan)


## Related surveys 
[Awesome-Video-Diffusion-Models](https://github.com/ChenHsing/Awesome-Video-Diffusion-Models?tab=readme-ov-file)

[Awesome-Text-to-Image](https://github.com/Yutong-Zhou-cv/Awesome-Text-to-Image?tab=readme-ov-file#head-ti2i)

## :point_right: Models to play with

### Open source

* **VideoCrafter/Floor33** [[Page](http://floor33.tech/)], [[Discord](https://discord.gg/rrayYqZ4tf)], [[Code & Models](https://github.com/AILab-CVC/VideoCrafter)]

* **ModelScope** [[Page](https://modelscope.cn/models/damo/text-to-video-synthesis/summary), [i2v](https://modelscope.cn/models/damo/Image-to-Video/summary)], [[Code & Models](https://modelscope.cn/models/damo/text-to-video-synthesis/summary)]

* **Hotshot-XL** [[Page](https://hotshot.co/)], [[Code & Models](https://github.com/hotshotco/Hotshot-XL)]

* **AnimeDiff** [[Page](https://animatediff.github.io/), [Code & Models](https://github.com/guoyww/AnimateDiff)]

* **Zeroscope V2 XL** [[Page](https://huggingface.co/cerspense/zeroscope_v2_XL)]

* **MuseV** [[Page](https://github.com/TMElyralab/MuseV)] 

* **opensora plan** [[Page](https://github.com/PKU-YuanGroup/Open-Sora-Plan)]

* **opensora** [[Page](https://github.com/hpcaitech/Open-Sora)]

*  **easyanimate** [[Page](https://github.com/aigc-apps/EasyAnimate)]

*  **Cogvideo X** [[Page](https://github.com/THUDM/CogVideo)]

*  **Mochi from Genmo** [[Page](https://huggingface.co/genmo/mochi-1-preview#running)]

*  **Hunyuan Video** [[Page](https://github.com/Tencent/HunyuanVideo)]



### Non-open source

* **Gen-1/Gen-2** [[Page](https://research.runwayml.com/gen2)]

* **Pika Lab** [[Page](https://www.pika.art/)], [[Discord](http://discord.gg/pika)]

* **Moonvalley** [[Page](https://moonvalley.ai/)], [[Discord](https://discord.gg/vk3aaH7r)]

* **Leonard Ai** [[Page](https://leonardo.ai/)]

* **Morph Studio** [[Page](https://www.morphstudio.xyz/)], [[Discord](https://discord.gg/hjd9JvXTU5)]
  
* **Lensgo** [[Page](https://lensgo.ai/), [Discord]()]

* **Genmo** [[Page](https://www.genmo.ai/)]

* **PlaiDay** [[Discord](https://discord.gg/6f6Q9pWb)]

* **Nerverends** [[Page](https://neverends.life/create)]

* **HiDream.ai/Pixeling** [[Page](https://hidream.ai/#/Pixeling)]

* **Assistant++** [[Page](https://assistive.chat/video)]

* **PixVerse**[[Page](https://pixverse.ai/)]

* **ltx.studio**[[Page](https://ltx.studio/)]

* **Haiper** [[Page](https://app.haiper.ai/explore)]

* **vivago.ai**[[Page](https://vivago.ai/video)]

* **智谱AI**[[Page](https://chatglm.cn/video)]

### Translation 
* **Goenhance.ai**[[Page](https://www.goenhance.ai/)]

* **ViggleAI**[[Page](https://t.co/2GMBpUOyHL)]



## :point_right: Databases

* **HowTo100M**

  [ICCV 2019] Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips \[[PDF](https://arxiv.org/pdf/1906.03327.pdf), [Project](https://www.di.ens.fr/willow/research/howto100m/) \]

* **HD-VILA-100M**
  
  [CVPR 2022]Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions [[PDF](https://openaccess.thecvf.com/content/CVPR2022/papers/Xue_Advancing_High-Resolution_Video-Language_Representation_With_Large-Scale_Video_Transcriptions_CVPR_2022_paper.pdf), [Page](https://github.com/microsoft/XPretrain/blob/main/hd-vila-100m/README.md)]
  
* **Web10M**

  [ICCV 2021]Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval \[[PDF](https://arxiv.org/pdf/2104.00650.pdf), [Project](https://github.com/m-bain/webvid) \]
  
* **UCF-101**

  [arxiv 2012] Ucf101: A dataset of 101 human actions classes from videos in the wild \[[PDF](https://arxiv.org/pdf/1212.0402.pdf), [Project](https://www.crcv.ucf.edu/data/UCF101.php) \]
  
* **Sky Time-lapse** 

  [CVPR 2018] Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks \[[PDF](https://openaccess.thecvf.com/content_cvpr_2018/papers/Xiong_Learning_to_Generate_CVPR_2018_paper.pdf), [Project](https://github.com/weixiong-ur/mdgan) \]
  
* **TaiChi** 

  [NIPS 2019] First order motion model for image animation \[ [PDF](https://papers.nips.cc/paper/2019/file/31c0b36aef265d9221af80872ceb62f9-Paper.pdf), [Project](https://github.com/AliaksandrSiarohin/first-order-model) \]

* **Celebv-text**
  
  [arxiv ]CelebV-Text: A Large-Scale Facial Text-Video Dataset [[PDF](), [Page](https://celebv-text.github.io/)]

* **Youku-mPLUG**
  
  [arxiv 2023.06]Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks [[PDF](https://arxiv.org/abs/2306.04362)]

* **InternVid**
  
  [arxiv 2023.07]InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation [[PDF](https://arxiv.org/abs/2307.06942)]

* **DNA-Rendering**
  
  [arxiv 2023.07] DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-centric Rendering [[PDF](https://arxiv.org/abs/2307.10173)]

* **Vimeo25M** (not open-source)
  
  [arxiv 2023.09] LAVIE: HIGH-QUALITY VIDEO GENERATION WITH CASCADED LATENT DIFFUSION MODELS [[PDF](https://arxiv.org/pdf/2309.15103.pdf)]

* **HD-VG-130M**

  [arxiv 2023.06]VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation [[PDF](https://arxiv.org/abs/2305.10874), [Page](https://github.com/daooshee/HD-VG-130M)]

* **Panda-70M**

[arxiv 2024.06]ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of Text-to-Time-lapse Video Generation
 [[PDF](https://arxiv.org/abs/2406.18522), [Page](https://github.com/PKU-YuanGroup/ChronoMagic-Bench)]


* **ChronoMagic-Pro**
  
* **OpenVid-1M**
  [arxiv 2024.07] A Large-Scale Dataset for High-Quality Text-to-Video Generation  [[PDF](http://export.arxiv.org/pdf/2407.02371),[Page](https://nju-pcalab.github.io/projects/openvid/)]


* **Koala-36M**
[arxiv 2024.10]Koala-36M: A Large-scale Video Dataset Improving Consistency between Fine-grained Conditions and Video Content[[PDF](https://arxiv.org/abs/2410.08260),[Page](https://koala36m.github.io/)]


  
* **LVD-2M**
  [arxiv 2024.10] LVD-2M: A Long-take Video Dataset with Temporally Dense Captions  [[PDF](https://arxiv.org/abs/2410.10816),[Page](https://github.com/SilentView/LVD-2M)]


* **MovieBench**
  [arxiv 2024.11]MovieBench: A Hierarchical Movie Level Dataset for Long Video Generation  [[PDF](https://weijiawu.github.io/MovieBench/),[Page](https://weijiawu.github.io/MovieBench/)] ![Code](https://img.shields.io/github/stars/showlab/MovieBench?style=social&label=Star)

* **VIVID-10M**
  [arxiv 2024.11]VIVID-10M: A Dataset and Baseline for Versatile and Interactive Video Local Editing [[PDF](https://arxiv.org/abs/2411.15260),[Page](https://inkosizhong.github.io/VIVID/)] 

* **OpenHumanVid**
  [arxiv 2024.12]A Large-Scale High-Quality Dataset for Enhancing Human-Centric Video Generation [[PDF](https://arxiv.org/abs/2412.00115),[Page](https://inkosizhong.github.io/VIVID/)]  ![Code](https://img.shields.io/github/stars/fudan-generative-vision/OpenHumanVid?style=social&label=Star)

* **Se\~norita-2M**
  [arxiv 2025.02]  Se\~norita-2M: A High-Quality Instruction-based Dataset for General Video Editing by Video Specialists [[PDF](https://arxiv.org/abs/2502.06734),[Page](https://senorita.github.io/)] 



* **VideoUFO**
  [arxiv 2025.03]  VideoUFO: A Million-Scale User-Focused Dataset for Text-to-Video Generation [[PDF](https://arxiv.org/pdf/2503.01739),[Page](https://huggingface.co/datasets/WenhaoWang/VideoUFO)] 



[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## VAE
[arxiv 2024.05]CV-VAE: A Compatible Video VAE for Latent Generative Video Models [[PDF](https://arxiv.org/abs/2405.20279),[Page](https://ailab-cvc.github.io/cvvae/index.html)] ![Code](https://img.shields.io/github/stars/AILab-CVC/CV-VAE?style=social&label=Star)

[arxiv 2024.06]OmniTokenizer: A Joint Image-Video Tokenizer for Visual Generation[[PDF](https://arxiv.org/abs/2406.09399),[Page](https://github.com/FoundationVision/OmniTokenizer)] ![Code](https://img.shields.io/github/stars/FoundationVision/OmniTokenizer?style=social&label=Star)

[arxiv 2024.09] OD-VAE: An Omni-dimensional Video Compressor for Improving Latent Video Diffusion Model [[PDF](https://arxiv.org/abs/2409.01199),[Page](https://github.com/PKU-YuanGroup/Open-Sora-Plan)] ![Code](https://img.shields.io/github/stars/PKU-YuanGroup/Open-Sora-Plan?style=social&label=Star)

[arxiv 2024.10] MotionAura: Generating High-Quality and Motion Consistent Videos using Discrete Diffusion [[PDF](https://arxiv.org/abs/2410.07659),[Page](https://researchgroup12.github.io/Abstract_Diagram.html)]

[arxiv 2024.10] Deep Compression Autoencoder for Efficient High-Resolution Diffusion Models [[PDF](https://arxiv.org/abs/2410.10733),[Page](https://github.com/mit-han-lab/efficientvit)] ![Code](https://img.shields.io/github/stars/mit-han-lab/efficientvit?style=social&label=Star)

[arxiv 2024.11] Cosmos Tokenizer: A suite of image and video neural tokenizers. [[PDF](),[Page](https://github.com/NVIDIA/Cosmos-Tokenizer)] ![Code](https://img.shields.io/github/stars/NVIDIA/Cosmos-Tokenizer?style=social&label=Star)

[arxiv 2024.11] Improved Video VAE for Latent Video Diffusion Model [[PDF](https://arxiv.org/abs/2411.06449),[Page](https://wpy1999.github.io/IV-VAE/)] 

[arxiv 2024.11] REDUCIO! Generating 1024×1024 Video within 16 Seconds using Extremely Compressed Motion Latents [[PDF](https://arxiv.org/abs/2411.13552),[Page](https://github.com/microsoft/Reducio-VAE)] [![Code](https://img.shields.io/github/stars/microsoft/Reducio-VAE?style=social&label=Star)](https://github.com/microsoft/Reducio-VAE)

[arxiv 2024.11] Factorized Visual Tokenization and Generation [[PDF](https://arxiv.org/abs/2411.16681),[Page](https://showlab.github.io/FQGAN/)] ![Code](https://img.shields.io/github/stars/showlab/FQGAN?style=social&label=Star)

[arxiv 2024.11] WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model [[PDF](https://arxiv.org/abs/2411.17459),[Page](https://github.com/PKU-YuanGroup/WF-VAE)] ![Code](https://img.shields.io/github/stars/PKU-YuanGroup/WF-VAE?style=social&label=Star)

[arxiv 2024.12] Four-Plane Factorized Video Autoencoders [[PDF](https://arxiv.org/abs/2412.04452),[Page](https://arxiv.org/abs/2412.04452)] 

[arxiv 2024.12] VidTok: A Versatile and Open-Source Video Tokenizer [[PDF](https://arxiv.org/abs/2412.13061),[Page](https://github.com/microsoft/VidTok)] ![Code](https://img.shields.io/github/stars/microsoft/VidTok?style=social&label=Star)

[arxiv 2024.12] Scaling 4D Representations  [[PDF](https://arxiv.org/abs/2412.15212)]

[arxiv 2024.12] Large Motion Video Autoencoding with Cross-modal Video VAE [[PDF](https://arxiv.org/abs/2412.17805),[Page](https://yzxing87.github.io/vae/)] ![Code](https://img.shields.io/github/stars/VideoVAEPlus?style=social&label=Star)

[arxiv 2024.12] VidTwin: Video VAE with Decoupled Structure and Dynamics [[PDF](https://arxiv.org/abs/2412.17726),[Page](https://github.com/microsoft/VidTok)] ![Code](https://img.shields.io/github/stars/microsoft/VidTok?style=social&label=Star)

[arxiv 2025.01] Learnings from Scaling Visual Tokenizers for Reconstruction and Generation [[PDF](https://arxiv.org/abs/2501.09755),[Page](https://vitok.github.io/)] 

[arxiv 2025.02]  DLFR-VAE: Dynamic Latent Frame Rate VAE for Video Generation [[PDF](http://arxiv.org/abs/2502.11897),[Page](https://github.com/thu-nics/DLFR-VAE)] ![Code](https://img.shields.io/github/stars/thu-nics/DLFR-VAE?style=social&label=Star)

[arxiv 2025.03]  Alias-Free Latent Diffusion Models: Improving Fractional Shift Equivariance of Diffusion Latent Space [[PDF](https://arxiv.org/pdf/2503.09419),[Page](https://github.com/SingleZombie/AFLDM)] ![Code](https://img.shields.io/github/stars/SingleZombie/AFLDM?style=social&label=Star)

[arxiv 2025.03] HiTVideo: Hierarchical Tokenizers for Enhancing Text-to-Video Generation with Autoregressive Large Language Models  [[PDF](https://arxiv.org/pdf/2503.11513),[Page](https://ziqinzhou66.github.io/project/HiTVideo)] 

[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## Tokenizer 

[arxiv 2024.12] Divot: Diffusion Powers Video Tokenizer for Comprehension and Generation [[PDF](https://arxiv.org/abs/2412.04432),[Page](https://github.com/TencentARC/Divot)] ![Code](https://img.shields.io/github/stars/TencentARC/Divot?style=social&label=Star)

[arxiv 2024.12] TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation  [[PDF](https://arxiv.org/abs/2412.03069),[Page](https://byteflow-ai.github.io/TokenFlow/)] ![Code](https://img.shields.io/github/stars/ByteFlow-AI/TokenFlow?style=social&label=Star) 

[arxiv 2024.12] V2PE: Improving Multimodal Long-Context Capability of Vision-Language Models with Variable Visual Position Encoding [[PDF](https://arxiv.org/abs/2412.09616),[Page](https://github.com/OpenGVLab/PVC)] ![Code](https://img.shields.io/github/stars/OpenGVLab/PVC?style=social&label=Star)

[arxiv 2024.12] Spectral Image Tokenizer [[PDF](https://arxiv.org/abs/2412.09607)]

[arxiv 2024.12] Dynamic-VLM: Simple Dynamic Visual Token Compression for VideoLLM [[PDF](https://arxiv.org/abs/2412.09530) ]

[arxiv 2025.01] LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token [[PDF](https://arxiv.org/abs/2501.03895),[Page](https://huggingface.co/ICTNLP/llava-mini-llama-3.1-8b)] ![Code](https://img.shields.io/github/stars/LLaVA-Mini?style=social&label=Star)

[arxiv 2025.02] FlexTok: Resampling Images into 1D Token Sequences of Flexible Length  [[PDF](https://arxiv.org/abs/2502.13967),[Page](https://flextok.epfl.ch/)] 


[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## GAN/VAE-based methods 
[NIPS 2016] **---VGAN---** Generating Videos with Scene Dynamics \[[PDF](https://proceedings.neurips.cc/paper/2016/file/04025959b191f8f9de3f924f0940515f-Paper.pdf), [code](https://github.com/cvondrick/videogan) \]

[ICCV 2017] **---TGAN---** Temporal Generative Adversarial Nets with Singular Value Clipping \[[PDF](https://arxiv.org/pdf/1611.06624.pdf), [code](https://github.com/pfnet-research/tgan) \]

[CVPR 2018] **---MoCoGAN---** MoCoGAN: Decomposing Motion and Content for Video Generation \[[PDF](https://arxiv.org/pdf/1707.04993.pdf), [code](https://github.com/sergeytulyakov/mocogan) \]

[NIPS 2018] **---SVG---** Stochastic Video Generation with a Learned Prior \[[PDF](https://proceedings.mlr.press/v80/denton18a/denton18a.pdf), [code](https://github.com/edenton/svg) \]

[ECCV 2018] Probabilistic Video Generation using
Holistic Attribute Control \[[PDF](https://openaccess.thecvf.com/content_ECCV_2018/papers/Jiawei_He_Probabilistic_Video_Generation_ECCV_2018_paper.pdf), code\]

[CVPR 2019; CVL ETH] **---SWGAN---** Sliced Wasserstein Generative Models \[[PDF](https://openaccess.thecvf.com/content_CVPR_2019/papers/Wu_Sliced_Wasserstein_Generative_Models_CVPR_2019_paper.pdf), [code](https://github.com/skolouri/swae) \]

[NIPS 2019; NVLabs] **---vid2vid---** Few-shot Video-to-Video Synthesis \[[PDF](https://nvlabs.github.io/few-shot-vid2vid/main.pdf), [code](https://github.com/NVlabs/few-shot-vid2vid) \]

[arxiv 2020; Deepmind] **---DVD-GAN---** ADVERSARIAL VIDEO GENERATION ON COMPLEX DATASETS \[[PDF](https://arxiv.org/pdf/1907.06571.pdf), [code](https://github.com/Harrypotterrrr/DVD-GAN) \]

[IJCV 2020] **---TGANv2---** Train Sparsely, Generate Densely: Memory-efficient Unsupervised Training of High-resolution Temporal GAN \[[PDF](https://arxiv.org/pdf/1811.09245.pdf), [code](https://github.com/pfnet-research/tgan2) \]

[PMLR 2021] **---TGANv2-ODE---** Latent Neural Differential Equations for Video Generation \[[PDF](https://arxiv.org/pdf/2011.03864.pdf), [code](https://github.com/Zasder3/Latent-Neural-Differential-Equations-for-Video-Generation) \]

[ICLR 2021 ] **---DVG---** Diverse Video Generation using a Gaussian Process Trigger \[[PDF](https://openreview.net/pdf?id=Qm7R_SdqTpT), [code](https://github.com/shgaurav1/DVG) \]

[Arxiv 2021; MRSA] **---GODIVA---** GODIVA: Generating Open-DomaIn Videos from nAtural Descriptions \[[PDF]([https://arxiv.org/pdf/2205.15868.pdf](https://arxiv.org/pdf/2104.14806.pdf)), [code](https://github.com/sihyun-yu/digan) \]

*[CVPR 2022 ] **---StyleGAN-V--** StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2 \[[PDF](https://arxiv.org/pdf/2112.14683.pdf), [code](https://github.com/universome/stylegan-v) \]

*[NeurIPs 2022] **---MCVD---** MCVD: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation [[PDF](https://arxiv.org/abs/2205.09853), [code](https://github.com/voletiv/mcvd-pytorch)]

## :point_right: Implicit Neural Representations
[ICLR 2022] Generating videos with dynamics-aware implicit generative adversarial networks \[[PDF](https://openreview.net/pdf?id=Czsdv-S4-w9), [code]() \]

## Transformer-based 
[arxiv 2021] **---VideoGPT--** VideoGPT: Video Generation using VQ-VAE and Transformers \[[PDF](https://arxiv.org/pdf/2104.10157.pdf), [code](https://github.com/wilson1yan/VideoGPT) \]

[ECCV 2022; Microsoft] **---NÜWA--** NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion \[[PDF](https://arxiv.org/pdf/2111.12417.pdf), code \]

[NIPS 2022; Microsoft] **---NÜWA-Infinity--** NUWA-Infinity: Autoregressive over Autoregressive Genera#tion for Infinite Visual Synthesis \[[PDF](https://arxiv.org/pdf/2207.09814.pdf), code \]

[Arxiv 2020; Tsinghua] **---CogVideo--** CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers \[[PDF](https://arxiv.org/pdf/2205.15868.pdf), [code](https://github.com/THUDM/CogVideo) \]

*[ECCV 2022] **---TATS--** Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer \[[PDF](https://arxiv.org/pdf/2204.03638.pdf), [code](https://github.com/SongweiGe/TATS)\]


*[arxiv 2022; Google] **---PHENAKI--** PHENAKI: VARIABLE LENGTH VIDEO GENERATION FROM OPEN DOMAIN TEXTUAL DESCRIPTIONS \[[PDF](https://arxiv.org/pdf/2210.02399.pdf), code \]

[arxiv 2022.12]MAGVIT: Masked Generative Video Transformer[[PDF](https://arxiv.org/pdf/2212.05199.pdf)]

[arxiv 2023.11]Optimal Noise pursuit for Augmenting Text-to-Video Generation [[PDF](https://arxiv.org/abs/2311.00949)]

[arxiv 2024.01]WorldDreamer: Towards General World Models for Video Generation via Predicting Masked Tokens [[PDF](https://arxiv.org/abs/2401.09985),[Page](https://world-dreamer.github.io/)]

[arxiv 2024.10] Loong: Generating Minute-level Long Videos with Autoregressive Language Models [[PDF](https://arxiv.org/abs/2410.02757), [Page](https://epiphqny.github.io/Loong-video/)]

[arxiv 2024.10] LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior  [[PDF](https://arxiv.org/abs/2410.21264),[Page](https://hywang66.github.io/larp/)]


[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)




## Diffusion-based methods 
*[NIPS 2022; Google] **---VDM--**  Video Diffusion Models \[[PDF](https://arxiv.org/pdf/2204.03458.pdf), [code](https://github.com/lucidrains/video-diffusion-pytorch) \]

*[arxiv 2022; Meta] **---MAKE-A-VIDEO--** MAKE-A-VIDEO: TEXT-TO-VIDEO GENERATION WITHOUT TEXT-VIDEO DATA \[[PDF](https://arxiv.org/pdf/2209.14792.pdf), code \]

*[arxiv 2022; Google] **---IMAGEN VIDEO--** IMAGEN VIDEO: HIGH DEFINITION VIDEO GENERATION WITH DIFFUSION MODELS \[[PDF](https://arxiv.org/pdf/2210.02303.pdf), code \]

*[arxiv 2022; ByteDace] ***MAGIC VIDEO***:Efficient Video Generation With Latent Diffusion Models \[[PDF](https://arxiv.org/pdf/2211.11018.pdf), code\]

*[arxiv 2022; Tencent] ***LVDM*** Latent Video Diffusion Models for High-Fidelity Video Generation with Arbitrary Lengths  \[[PDF](https://arxiv.org/pdf/2211.13221.pdf), code\]

[AAAI 2022; JHU ] VIDM: Video Implicit Diffusion Model \[[PDF](https://kfmei.page/vidm/Video_implicit_diffusion_models.pdf)\]

[arxiv 2023.01; Meta] Text-To-4D Dynamic Scene Generation [[PDF](https://arxiv.org/pdf/2301.11280.pdf), [Page](https://make-a-video3d.github.io/)]

[arxiv 2023.03]Video Probabilistic Diffusion Models in Projected Latent Space [[PDF](https://arxiv.org/abs/2302.07685), [Page](https://sihyun.me/PVDM/)]

[arxiv 2023.03]Controllable Video Generation by Learning the Underlying Dynamical System with Neural ODE [[PDF](https://arxiv.org/abs/2303.05323)]

[arxiv 2023.03]Decomposed Diffusion Models for High-Quality Video Generation [[PDF](https://arxiv.org/pdf/2303.08320.pdf)]

[arxiv 2023.03]NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation [[PDF](https://arxiv.org/abs/2303.12346)]

*[arxiv 2023.04]Latent-Shift: Latent Diffusion with Temporal Shift for Efficient Text-to-Video Generation [[PDF](https://arxiv.org/abs/2304.08477)]

*[arxiv 2023.04]Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models [[PDF](https://arxiv.org/abs/2304.08818), [Page](https://research.nvidia.com/labs/toronto-ai/VideoLDM/)]

[arxiv 2023.04]LaMD: Latent Motion Diffusion for Video Generation [[PDF](https://arxiv.org/abs/2304.11603)]

*[arxiv 2023.05]Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models[[PDF](https://arxiv.org/pdf/2305.10474.pdf), [Page](https://research.nvidia.com/labs/dir/pyoco/)]

[arxiv 2023.05]VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation [[PDF](https://arxiv.org/pdf/2305.10874.pdf)]

[arxiv 2023.08]ModelScope Text-to-Video Technical Report [[PDF](https://arxiv.org/pdf/2308.06571.pdf)]

[arxiv 2023.08]Dual-Stream Diffusion Net for Text-to-Video Generation [[PDF](https://huggingface.co/papers/2308.08316)]

[arxiv 2023.08]SimDA: Simple Diffusion Adapter for Efficient Video Generation [[PDF](https://arxiv.org/abs/2308.09710), [Page](https://chenhsing.github.io/SimDA/)]

[arxiv 2023.08]Dysen-VDM: Empowering Dynamics-aware Text-to-Video Diffusion with Large Language Models [[PDF](https://arxiv.org/pdf/2308.13812.pdf), [Page](https://haofei.vip/Dysen-VDM/)]

[arxiv 2023.09]Reuse and Diffuse: Iterative Denoising for Text-to-Video Generation[[PDF](https://arxiv.org/pdf/2309.03549.pdf),[Page](https://anonymous0x233.github.io/ReuseAndDiffuse/)]

[arxiv 2023.09]LAVIE: HIGH-QUALITY VIDEO GENERATION WITH CASCADED LATENT DIFFUSION MODELS [[PDF](https://arxiv.org/pdf/2309.15103.pdf), [Page](https://vchitect.github.io/LaVie-project/)]

[arxiv 2023.09]VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning [[PDF](https://arxiv.org/abs/2309.15091), [Page](https://videodirectorgpt.github.io/)]

[arxiv 2023.10]Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation [[PDF](https://arxiv.org/abs/2309.15818), [Page](https://showlab.github.io/Show-1)]

[arxiv 2023.10]LLM-grounded Video Diffusion Models [[PDF](https://arxiv.org/abs/2309.17444),[Page](https://llm-grounded-video-diffusion.github.io/)]

[arxiv 2023.10]VideoCrafter1: Open Diffusion Models for High-Quality Video Generation [[PDF](https://arxiv.org/abs/2310.19512),[Page](https://github.com/AILab-CVC/VideoCrafter)]

[arxiv 2023.11]Make Pixels Dance: High-Dynamic Video Generation [[PDF](https://arxiv.org/abs/2311.10982), [Page](https://makepixelsdance.github.io/)]

[arxiv 2023.11]Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets[[PDF](https://static1.squarespace.com/static/6213c340453c3f502425776e/t/655ce779b9d47d342a93c890/1700587395994/stable_video_diffusion.pdf), [Page](https://t.co/P2lmq343cf)]

[arxiv 2023.11]Kandinsky Video [[PDF](https://arxiv.org/abs/2311.13073),[Page](https://ai-forever.github.io/kandinsky-video/)]

[arxiv 2023.12]GenDeF: Learning Generative Deformation Field for Video Generation [[PDF](https://arxiv.org/abs/2312.04561),[Page](https://aim-uofa.github.io/GenDeF/)]

[arxiv 2023.12]GenTron: Delving Deep into Diffusion Transformers for Image and Video Generation [[PDF](https://arxiv.org/abs/2312.04557),[Page](https://www.shoufachen.com/gentron_website/)]

[arxiv 2023.12]Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation [[PDF](https://arxiv.org/abs/2312.04483), [Page](https://higen-t2v.github.io/)]

[arxiv 2023.12]AnimateZero:Video Diffusion Models are Zero-Shot Image Animators [[PDF](https://arxiv.org/abs/2312.03793),[Page](https://vvictoryuki.github.io/animatezero.github.io/)]

[arxiv 2023.12]Photorealistic Video Generation with Diffusion Models [[PDF](https://arxiv.org/abs/2312.06662),[Page](https://walt-video-diffusion.github.io/)]

[arxiv 2023.12]A Recipe for Scaling up Text-to-Video Generation with Text-free Videos [[PDF](https://arxiv.org/abs/2312.15770),[Page](https://tf-t2v.github.io/)]

[arxiv 2023.12]MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation [[PDF](https://arxiv.org/pdf/2401.04468.pdf), [Page](https://magicvideov2.github.io/)]

[arxiv 2024.1]Latte: Latent Diffusion Transformer for Video Generation [[PDF](https://arxiv.org/abs/2401.03048),[Page](https://maxin-cn.github.io/latte_project)]

[arxiv 2024.1]VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models [[PDF](https://arxiv.org/abs/2401.09047),[Page](https://ailab-cvc.github.io/videocrafter2/)]

[arxiv 2024.1]Lumiere: A Space-Time Diffusion Model for Video Generation [[PDF](https://arxiv.org/abs/2401.12945), [Page](https://lumiere-video.github.io/)]

[arxiv 2024.02]Hybrid Video Diffusion Models with 2D Triplane and 3D Wavelet Representation [[PDF](https://arxiv.org/abs/2402.13729)]

[arxiv 2024.02]Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis[[PDF](https://arxiv.org/abs/2402.14797),[Page](https://snap-research.github.io/snapvideo/)]

[arxiv 2024.03]Mora: Enabling Generalist Video Generation via A Multi-Agent Framework[[PDF](https://arxiv.org/html/2403.13248v1)]

[arxiv 2024.03]Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition [[PDF](https://arxiv.org/abs/2403.14148),[Page](https://arxiv.org/abs/2403.14148)]

[arxiv 2024.04]Grid Diffusion Models for Text-to-Video Generation [[PDF](https://arxiv.org/abs/2404.00234)]

[arxiv 2024.04]MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators [[PDF](https://arxiv.org/abs/2404.05014)]

[arxiv 2024.05]Matten: Video Generation with Mamba-Attention [[PDF](https://arxiv.org/abs/2405.03025)]

[arxiv 2024.05]Vidu: a Highly Consistent, Dynamic and Skilled Text-to-Video Generator with Diffusion Models [[PDF](https://arxiv.org/abs/2405.04233),[Page](https://www.shengshu-ai.com/vidu)]

[arxiv 2024.05]Lumina-T2X: Transforming Text into Any Modality, Resolution, and Duration via Flow-based Large Diffusion Transformers [[PDF](https://arxiv.org/abs/2405.05945),[Page](https://github.com/Alpha-VLLM/Lumina-T2X)]

[arxiv 2024.05] Scaling Diffusion Mamba with Bidirectional SSMs for Efficient Image and Video Generation [[PDF](https://arxiv.org/abs/2405.15881)]

[arxiv 2024.06]Hierarchical Patch Diffusion Models for High-Resolution Video Generation [[PDF](https://arxiv.org/pdf/2406.07792), [Page](https://snap-research.github.io/hpdm)]

[arxiv 2024.08] xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed Representations[[PDF](https://arxiv.org/abs/2408.12590), [Page](https://github.com/SalesforceAIResearch/xgen-videosyn)]

[arxiv 2024.10] Movie Gen: A Cast of Media Foundation Models [[PDF](https://ai.meta.com/static-resource/movie-gen-research-paper), [Page]()]

[arxiv 2024.10] MotionAura: Generating High-Quality and Motion Consistent Videos using Discrete Diffusion [[PDF](https://arxiv.org/abs/2410.07659),[Page](https://researchgroup12.github.io/Abstract_Diagram.html)]

[arxiv 2024.10] Redefining Temporal Modeling in Video Diffusion: The Vectorized Timestep Approach [[PDF](https://arxiv.org/abs/2410.03160), [Page](https://github.com/Yaofang-Liu/FVDM)]

[arxiv 2024.10] MarDini: Masked Autoregressive Diffusion for Video Generation at Scale [[PDF](https://arxiv.org/abs/2410.20280), [Page](https://mardini-vidgen.github.io/)]

[arxiv 2024.12] Open-Sora Plan: Open-Source Large Video Generation Model [[PDF](https://arxiv.org/abs/2412.00131),[Page](https://github.com/PKU-YuanGroup/Open-Sora-Plan)] ![Code](https://img.shields.io/github/stars/PKU-YuanGroup/Open-Sora-Plan?style=social&label=Star)

[arxiv 2024.12] HunyuanVideo: A Systematic Framework For Large Video Generation Model [[PDF](https://arxiv.org/abs/2412.03603),[Page](https://github.com/Tencent/HunyuanVideo)] ![Code](https://img.shields.io/github/stars/Tencent/HunyuanVideo?style=social&label=Star)

[arxiv 2025.01] Open-Sora: Democratizing Efficient Video Production for All  [[PDF](https://arxiv.org/pdf/2412.20404),[Page](https://github.com/hpcaitech/Open-Sora)] ![Code](https://img.shields.io/github/stars/hpcaitech/Open-Sora?style=social&label=Star)

[arxiv 2025.02]  FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution Video Generation [[PDF](https://arxiv.org/abs/2502.05179),[Page](https://github.com/FoundationVision/FlashVideo)] ![Code](https://img.shields.io/github/stars/FoundationVision/FlashVideo?style=social&label=Star)

[arxiv 2025.02] Goku: Flow Based Video Generative Foundation Models  [[PDF](https://arxiv.org/abs/2502.04896),[Page](https://saiyan-world.github.io/goku/)] ![Code](https://img.shields.io/github/stars/Saiyan-World/goku?style=social&label=Star)

[arxiv 2025.02] Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT  [[PDF](https://arxiv.org/abs/2502.06782),[Page](https://github.com/Alpha-VLLM/Lumina-Video)] ![Code](https://img.shields.io/github/stars/Alpha-VLLM/Lumina-Video?style=social&label=Star)

[arxiv 2025.02] Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model  [[PDF](https://arxiv.org/abs/2502.10248),[Page](https://yuewen.cn/videos)] ![Code](https://img.shields.io/github/stars/stepfun-ai/Step-Video-T2V?style=social&label=Star)

[arxiv 2025.02] SkyReels V1: Human-Centric Video Foundation Model  [[Page](https://github.com/SkyworkAI/SkyReels-V1)] ![Code](https://img.shields.io/github/stars/SkyworkAI/SkyReels-V1?style=social&label=Star)

[arxiv 2025.03] TPDiff: Temporal Pyramid Video Diffusion Model  [[PDF](https://arxiv.org/abs/2503.09566),[Page](https://showlab.github.io/TPDiff/)] ![Code](https://img.shields.io/github/stars/showlab/TPDiff?style=social&label=Star)

[arxiv 2025.03]  Step-Video-TI2V Technical Report: A State-of-the-Art Text-Driven Image-to-Video Generation Model [[PDF](https://arxiv.org/abs/2503.11251),[Page](https://github.com/stepfun-ai/Step-Video-TI2V)] ![Code](https://img.shields.io/github/stars/stepfun-ai/Step-Video-TI2V?style=social&label=Star)


[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## LLMs-based 
[arxiv 2023.12]VideoPoet: A Large Language Model for Zero-Shot Video Generation [[PDF](https://arxiv.org/abs/2312.14125),[Page](http://sites.research.google/videopoet/)]

[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)



## DiT
[arxiv 2024.05]  EasyAnimate: A High-Performance Long Video Generation Method based on Transformer Architecture [[PDF](https://arxiv.org/abs/2405.18991),[Page](https://github.com/aigc-apps/EasyAnimate)]

[arxiv 2024.08] CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer  [[PDF](https://arxiv.org/abs/2408.06072),[Page](https://github.com/THUDM/CogVideo)]

[arxiv 2024.10] Allegro: Open the Black Box of Commercial-Level Video Generation Model  [[PDF](https://arxiv.org/abs/2410.15458),[Page](https://rhymes.ai/allegro_gallery)]

[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)

## scaling law 
[arxiv 2024.11] Towards Precise Scaling Laws for Video Diffusion Transformers [[PDF](https://arxiv.org/abs/2411.17470)] 


## State Space-based 
[arxiv 2024.03]SSM Meets Video Diffusion Models: Efficient Video Generation with Structured State Spaces [[PDF](https://arxiv.org/abs/2403.07711),[Page](https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models)]


[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)



## improve Video Diffusion models 
[arxiv 2023.10]ScaleCrafter: Tuning-free Higher-Resolution Visual Generation with Diffusion Models [[PDF](https://arxiv.org/abs/2310.07702), [Page](https://yingqinghe.github.io/scalecrafter/)]

[arxiv 2023.10]FreeU: Free Lunch in Diffusion U-Net [[PDF](https://arxiv.org/pdf/2309.11497.pdf), [Page](https://chenyangsi.top/FreeU/)]

[arxiv 2023.12]FreeInit: Bridging Initialization Gap in Video Diffusion Models [[PDF](https://arxiv.org/abs/2312.07537),[Page](https://tianxingwu.github.io/pages/FreeInit/)]

[arxiv 2024.07] Video Diffusion Alignment via Reward Gradients [[PDF](https://arxiv.org/abs/2407.08737), [Page](https://github.com/mihirp1998/VADER)]

[arxiv 2024.08] FancyVideo: Towards Dynamic and Consistent Video Generation via Cross-frame Textual Guidance[[PDF](https://arxiv.org/abs/2408.08189)]

[arxiv 2024.09] S2AG-Vid: Enhancing Multi-Motion Alignment in Video Diffusion Models via Spatial and Syntactic Attention-Based Guidance [[PDF](https://arxiv.org/pdf/2409.15259)]

[arxiv 2024.10] BroadWay: Boost Your Text-to-Video Generation Model in a Training-free Way [[PDF](https://arxiv.org/abs/2410.06241)]

[arxiv 2024.10] Pyramidal Flow Matching for Efficient Video Generative Modeling [[PDF](https://arxiv.org/abs/2410.05954), [Page](https://pyramid-flow.github.io/)]

[arxiv 2024.10] T2V-Turbo-v2: Enhancing Video Generation Model Post-Training through Data, Reward, and Conditional Guidance Design [[PDF](https://arxiv.org/abs/2410.05677), [Page](https://t2v-turbo-v2.github.io/)]

[arxiv 2024.11] Enhancing Motion in Text-to-Video Generation with Decomposed Encoding and Conditioning [[PDF](https://arxiv.org/abs/2410.24219), [Page](https://github.com/PR-Ryan/DEMO)]

[arxiv 2024.11] Optical-Flow Guided Prompt Optimization for Coherent Video Generation [[PDF](https://arxiv.org/abs/2411.15540),[Page](https://motionprompt.github.io/)] ![Code](https://img.shields.io/github/stars/HyelinNAM/MotionPrompt?style=social&label=Star)

[arxiv 2024.11] Free2Guide: Gradient-Free Path Integral Control for Enhancing Text-to-Video Generation with Large Vision-Language Models [[PDF](https://arxiv.org/abs/2411.17041),[Page](https://kjm981995.github.io/free2guide/)] 

[arxiv 2024.12] PhyT2V: LLM-Guided Iterative Self-Refinement for Physics-Grounded Text-to-Video Generation [[PDF](https://arxiv.org/abs/2412.00596),[Page](https://github.com/pittisl/PhyT2V)] ![Code](https://img.shields.io/github/stars/pittisl/PhyT2V?style=social&label=Star)

[arxiv 2024.12] Mimir: Improving Video Diffusion Models for Precise Text Understanding [[PDF](https://arxiv.org/abs/2412.03085),[Page](https://lucaria-academy.github.io/Mimir/)] 

[arxiv 2024.12] Track4Gen: Teaching Video Diffusion Models to Track Points Improves Video Generation [[PDF](https://arxiv.org/abs/2412.06016),[Page](https://aaltoml.github.io/BayesVLM/)] 

[arxiv 2024.12] STIV: Scalable Text and Image Conditioned Video Generation [[PDF](https://arxiv.org/abs/2412.07730)]

[arxiv 2024.12] VideoDPO: Omni-Preference Alignment for Video Diffusion Generation [[PDF](https://arxiv.org/abs/2412.14167),[Page](https://videodpo.github.io/)] 

[arxiv 2025.01] RepVideo: Rethinking Cross-Layer Representation for Video Generation [[PDF](https://arxiv.org/abs/2501.08994),[Page](https://vchitect.github.io/RepVid-Webpage/)] ![Code](https://img.shields.io/github/stars/Vchitect/RepVideo?style=social&label=Star)

[arxiv 2025.02] VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models [[PDF](https://arxiv.org/abs/2502.02492),[Page](https://hila-chefer.github.io/videojam-paper.github.io/)] 

[arxiv 2025.02]  Inference-Time Text-to-Video Alignment with Diffusion Latent Beam Search [[PDF](https://arxiv.org/abs/2501.19252),[Page](https://sites.google.com/view/t2v-dlbs)] 

[arxiv 2025.02]  History-Guided Video Diffusion [[PDF](https://arxiv.org/abs/2502.06764),[Page](https://boyuan.space/history-guidance)] 

[arxiv 2025.02] Enhance-A-Video: Better Generated Video for Free  [[PDF](https://arxiv.org/pdf/2502.07508),[Page](https://github.com/NUS-HPC-AI-Lab/Enhance-A-Video)] ![Code](https://img.shields.io/github/stars/NUS-HPC-AI-Lab/Enhance-A-Video?style=social&label=Star)

[arxiv 2025.03]  Raccoon: Multi-stage Diffusion Training with Coarse-to-Fine Curating Videos [[PDF](https://arxiv.org/pdf/2502.21314)]

[arxiv 2025.03] The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation  [[PDF](https://arxiv.org/pdf/2503.04606),[Page](https://landiff.github.io/)] 

[arxiv 2025.03] MagicComp: Training-free Dual-Phase Refinement for Compositional Video Generation  [[PDF](https://arxiv.org/pdf/2503.14428),[Page](https://hong-yu-zhang.github.io/MagicComp-Page/)] ![Code](https://img.shields.io/github/stars/Hong-yu-Zhang/MagicComp?style=social&label=Star)

[arxiv 2025.03]  Temporal Regularization Makes Your Video Generator Stronger [[PDF](https://arxiv.org/abs/2412.02114),[Page](https://haroldchen19.github.io/FluxFlow/)] 


[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## composition 
[arxiv 2024.07]VideoTetris: Towards Compositional Text-To-Video Generation[[PDF](https://arxiv.org/abs/2406.04277), [Page](https://videotetris.github.io/)]


[arxiv 2024.07]GVDIFF: Grounded Text-to-Video Generation with Diffusion Models[[PDF](https://arxiv.org/abs/2407.01921)]

[arxiv 2024.07]Compositional Video Generation as Flow Equalization [[PDF](https://arxiv.org/abs/2407.06182), [Page](https://adamdad.github.io/vico/)]

[arxiv 2024.07] InVi: Object Insertion In Videos Using Off-the-Shelf Diffusion Models[[PDF](https://arxiv.org/abs/2407.10958)]

[arxiv 2025.01] VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion Control [[PDF](https://arxiv.org/abs/2501.01427),[Page](https://videoanydoor.github.io/)] 

[arxiv 2025.01]BlobGEN-Vid: Compositional Text-to-Video Generation with Blob Video Representations  [[PDF](https://arxiv.org/abs/2501.07647),[Page](https://blobgen-vid2.github.io/)] 

[arxiv 2025.02]  DC-ControlNet: Decoupling Inter- and Intra-Element Conditions in Image Generation with Diffusion Models [[PDF](https://arxiv.org/abs/2502.14779),[Page](https://um-lab.github.io/DC-ControlNet/)] 

[arxiv 2025.03]  Get In Video: Add Anything You Want to the Video [[PDF](https://arxiv.org/abs/2503.06268),[Page](https://zhuangshaobin.github.io/GetInVideo-project/)] ![Code](https://img.shields.io/github/stars/xh9998/DiffVSR?style=social&label=Star)

[arxiv 2025.03] DreamInsert: Zero-Shot Image-to-Video Object Insertion from A Single Image  [[PDF](https://arxiv.org/pdf/2503.10342)]


[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)



## Caption
[arxiv 2024.11] Grounded Video Caption Generation [[PDF](https://arxiv.org/abs/2411.07584)]

[arxiv 2024.12] Progress-Aware Video Frame Captioning [[PDF](https://arxiv.org/abs/2412.02071),[Page](https://vision.cs.utexas.edu/projects/ProgressCaptioner/)] 

[arxiv 2024.12] Mimir: Improving Video Diffusion Models for Precise Text Understanding [[PDF](https://arxiv.org/abs/2412.03085),[Page](https://lucaria-academy.github.io/Mimir/)] 

[arxiv 2024.12] InstanceCap: Improving Text-to-Video Generation via Instance-aware Structured Caption [[PDF](https://arxiv.org/abs/2412.09283),[Page](https://github.com/NJU-PCALab/InstanceCap)] ![Code](https://img.shields.io/github/stars/NJU-PCALab/InstanceCap?style=social&label=Star)


[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)





## multi-prompt 
[arxiv 2023.12]MTVG : Multi-text Video Generation with Text-to-Video Models [[PDF](https://arxiv.org/abs/2312.04086)]

[arxiv 2024.05]TALC: Time-Aligned Captions for Multi-Scene Text-to-Video Generation [[PDF](https://arxiv.org/abs/2405.04682),[Page](https://talc-mst2v.github.io/)]

[arxiv 2024.06]VideoTetris: Towards Compositional Text-To-Video Generation[[PDF](https://arxiv.org/abs/2406.04277), [Page](https://videotetris.github.io/)]

[arxiv 2024.06] Pandora: Towards General World Model with Natural Language Actions and Video States [[PDF](https://arxiv.org/abs/2406.09455), [Page](https://world-model.maitrix.org/)]

[arxiv 2024.12] Mind the Time: Temporally-Controlled Multi-Event Video Generation [[PDF](https://arxiv.org/abs/2412.05263),[Page](https://mint-video.github.io/)]

[arxiv 2025.02] Object-Centric Image to Video Generation with Language Guidance  [[PDF](https://arxiv.org/abs/2502.11655),[Page](https://play-slot.github.io/TextOCVP/)] ![Code](https://img.shields.io/github/stars/angelvillar96/TextOCVP?style=social&label=Star)

[arxiv 2025.03]  Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled Sampling [[PDF](https://arxiv.org/abs/2503.08605),[Page](https://syncos2025.github.io/)] 


[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## long video generation 
[arxiv 2023.]Gen-L-Video: Long Video Generation via Temporal Co-Denoising [[PDF](https://arxiv.org/abs/2305.18264), [Page](https://g-u-n.github.io/projects/gen-long-video/index.html)]

[arxiv 2023.10]FreeNoise: Tuning-Free Longer Video Diffusion Via Noise Rescheduling [[PDF](https://arxiv.org/abs/2310.15169),[Page](http://haonanqiu.com/projects/FreeNoise.html)]

[arxiv 2023.12]VIDiff: Translating Videos via Multi-Modal Instructions with Diffusion Models[[PDF](https://arxiv.org/abs/2311.18837),[Page](https://chenhsing.github.io/VIDiff)]

[arxiv 2023.12]AVID: Any-Length Video Inpainting with Diffusion Model [[PDF](https://arxiv.org/abs/2312.03816),[Page](https://zhang-zx.github.io/AVID/)]

[arxiv 2023.12]RealCraft: Attention Control as A Solution for Zero-shot Long Video Editing [[PDF](https://arxiv.org/abs/2312.12635)]

[arxiv 2024.03]VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis [[PDF](https://arxiv.org/pdf/2403.13501.pdf),[Page](https://yumengli007.github.io/VSTAR/)]

[arxiv 2024.03]StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text [[PDF](https://arxiv.org/abs/2403.14773)]

[arxiv 2024.04]FlexiFilm: Long Video Generation with Flexible Conditions [[PDF](https://arxiv.org/abs/2404.18620)]

[arxiv 2024.05] FIFO-Diffusion: Generating Infinite Videos from Text without Training [[PDF](https://arxiv.org/abs/2405.11473),[Page](https://jjihwan.github.io/projects/FIFO-Diffusion)]

[arxiv 2024.05]Controllable Long Image Animation with Diffusion Models[[PDF](https://arxiv.org/pdf/2405.17306),[Page](https://wangqiang9.github.io/Controllable.github.io/)]

[arxiv 2024.06]CoNo: Consistency Noise Injection for Tuning-free Long Video Diffusion [[PDF](https://arxiv.org/abs/2406.05082), [Page](https://wxrui182.github.io/CoNo.github.io/)]

[arxiv 2024.06]Video-Infinity: Distributed Long Video Generation [[PDF](https://arxiv.org/abs/2406.16260), [Page](https://video-infinity.tanzhenxiong.com/)]

[arxiv 2024.06] FreeLong : Training-Free Long Video Generation with SpectralBlend Temporal Attention [[PDF](https://arxiv.org/pdf/2407.19918), [Page](https://freelongvideo.github.io/)]

[arxiv 2024.06] ViD-GPT: Introducing GPT-style Autoregressive Generation in Video Diffusion Models [[PDF](https://arxiv.org/pdf/2406.10981),[Page](https://github.com/Dawn-LX/CausalCache-VDM)] ![Code](https://img.shields.io/github/stars/Dawn-LX/CausalCache-VDM?style=social&label=Star)

[arxiv 2024.06] Live2Diff: Live Stream Translation via Uni-directional Attention in Video Diffusion Models [[PDF](https://arxiv.org/abs/2407.08701),[Page](https://live2diff.github.io/)] ![Code](https://img.shields.io/github/stars/open-mmlab/Live2Diff?style=social&label=Star)


[arxiv 2024.07]Multi-sentence Video Grounding for Long Video Generation[[PDF](https://arxiv.org/abs/2407.13219)]

[arxiv 2024.08]Training-free High-quality Video Generation with Chain of Diffusion Model Experts [[PDF](https://arxiv.org/abs/2408.13423), [Page](https://confiner2025.github.io/)]

[arxiv 2024.08] TVG: A Training-free Transition Video Generation Method with Diffusion Models[[PDF](https://arxiv.org/abs/2408.13413), [Page](https://sobeymil.github.io/tvg.com/)]

[arxiv 2024.09] DiVE: DiT-based Video Generation with Enhanced Control [[PDF](https://arxiv.org/abs/2409.01595), [Page](https://liautoad.github.io/DIVE/)]

[arxiv 2024.10] Progressive Autoregressive Video Diffusion Models [[PDF](https://arxiv.org/abs/2410.08151), [Page](https://desaixie.github.io/pa-vdm/)]

[arxiv 2024.10] ARLON: Boosting Diffusion Transformers with Autoregressive Models for Long Video Generation [[PDF](https://arxiv.org/abs/2410.20502), [Page](https://arlont2v.github.io/)]

[arxiv 2024.12] Long Video Diffusion Generation with Segmented Cross-Attention and Content-Rich Video Data Curation [[PDF](https://arxiv.org/abs/2412.01316),[Page](https://presto-video.github.io/)] ![Code](https://img.shields.io/github/stars/rhymes-ai/Allegro?style=social&label=Star)

[arxiv 2024.12] Advancing Auto-Regressive Continuation for Video Frames [[PDF](https://arxiv.org/pdf/2412.03758)]

[arxiv 2024.12] From Slow Bidirectional to Fast Causal Video Generators  [[PDF](https://arxiv.org/abs/2412.07772),[Page](https://causvid.github.io/)]

[arxiv 2024.12] Owl-1: Omni World Model for Consistent Long Video Generation [[PDF](https://arxiv.org/abs/2412.09600),[Page](https://github.com/huang-yh/Owl)] ![Code](https://img.shields.io/github/stars/huang-yh/Owl?style=social&label=Star)

[arxiv 2024.12] DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation [[PDF](https://arxiv.org/abs/2412.18597),[Page](https://onevfall.github.io/project_page/ditctrl/)] ![Code](https://img.shields.io/github/stars/TencentARC/DiTCtrl?style=social&label=Star)

[arxiv 2025.01] Tuning-Free Long Video Generation via Global-Local Collaborative Diffusion [[PDF](https://arxiv.org/pdf/2501.05484)]

[arxiv 2025.01] Ouroboros-Diffusion: Exploring Consistent Content Generation in Tuning-free Long Video Diffusion [[PDF](https://arxiv.org/abs/2501.09019)]

[arxiv 2025.02] MaskFlow: Discrete Flows for Flexible and Efficient Long Video Generation  [[PDF](https://arxiv.org/abs/2502.11234),[Page](https://compvis.github.io/maskflow/)] ![Code](https://img.shields.io/github/stars/CompVis/maskflow?style=social&label=Star)

[arxiv 2025.02] MALT Diffusion: Memory-Augmented Latent Transformers for Any-Length Video Generation  [[PDF](https://arxiv.org/pdf/2502.12632)]

[arxiv 2025.03] Training-free and Adaptive Sparse Attention for Efficient Long Video Generation  [[PDF](https://arxiv.org/abs/2502.21079)]

[arxiv 2025.03]  VideoMerge: Towards Training-free Long Video Generation [[PDF](https://arxiv.org/abs/2503.09926)]

[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)






## sound generation 
[arxiv 2024.07] Read, Watch and Scream! Sound Generation from Text and Video
[[PDF](https://arxiv.org/abs/2407.05551), [Page](https://naver-ai.github.io/rewas)]

[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## Higher Resolution 
[arxiv 2023.10] ScaleCrafter: Tuning-free Higher-Resolution Visual Generation with Diffusion Models [[PDF](https://arxiv.org/abs/2310.07702), [Page](https://yingqinghe.github.io/scalecrafter/)]



## infinity scene /360
[arxiv 2023.12]Going from Anywhere to Everywhere[[PDF](https://arxiv.org/abs/2312.03884),[Page](https://kovenyu.com/wonderjourney/)]

[arxiv 2024.1]360DVD: Controllable Panorama Video Generation with 360-Degree Video Diffusion Model [[PDF](https://arxiv.org/abs/2401.06578)]

## Story /  Concept 
[arxiv 2023.05]TaleCrafter: Interactive Story Visualization with Multiple Characters [[PDF](https://arxiv.org/abs/2305.18247), [Page](https://videocrafter.github.io/TaleCrafter/)]

[arxiv 2023.07]Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation [[PDF](https://arxiv.org/abs/2307.06940), [Page](https://videocrafter.github.io/Animate-A-Story)]

[arxiv 2024.01]VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM [[PDF](https://arxiv.org/abs/2401.01256), [Page](https://videodrafter.github.io/)]

[arxiv 2024.01]Vlogger: Make Your Dream A Vlog [[PDF](https://arxiv.org/abs/2401.09414),[Page](https://github.com/zhuangshaobin/Vlogger)]

[arxiv 2024.03]AesopAgent: Agent-driven Evolutionary System on Story-to-Video Production [[PDF](https://arxiv.org/abs/2403.07952),[Page](https://aesopai.github.io/)]

[arxiv 2024.04]StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation [[PDF](https://arxiv.org/abs/2405.01434),[Page](https://github.com/HVision-NKU/StoryDiffusion)]

[arxiv 2024.05]The Lost Melody: Empirical Observations on Text-to-Video Generation From A Storytelling Perspective [[PDF](https://arxiv.org/abs/2405.08720)]

[arxiv 2024.05]DisenStudio: Customized Multi-subject Text-to-Video Generation with Disentangled Spatial Control [[PDF](https://arxiv.org/abs/2405.12796),[Page](https://forchchch.github.io/disenstudio.github.io/)]

[arxiv 2024.11] DreamRunner: Fine-Grained Storytelling Video Generation with Retrieval-Augmented Motion Adaptation [[PDF](https://arxiv.org/abs/2411.16657),[Page](https://dreamrunner-story2video.github.io/)] ![Code](https://img.shields.io/github/stars/wz0919/DreamRunner?style=social&label=Star)

[arxiv 2025.01] VideoAuteur: Towards Long Narrative Video Generation [[PDF](https://arxiv.org/abs/2501.06173),[Page](https://videoauteur.github.io/)] ![Code](https://img.shields.io/github/stars/lambert-x/VideoAuteur?style=social&label=Star)

[arxiv 2025.03] Text2Story: Advancing Video Storytelling with Text Guidance  [[PDF](https://arxiv.org/pdf/2503.06310)]

[arxiv 2025.03] Long Context Tuning for Video Generation  [[PDF](https://arxiv.org/pdf/2503.10589),[Page](https://guoyww.github.io/projects/long-context-video/)] 

[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)

## Stereo Video Generation 

[arxiv 2024.09]StereoCrafter: Diffusion-based Generation of Long and High-fidelity Stereoscopic 3D from Monocular Videos  [[PDF](https://arxiv.org/abs/2409.07447),[Page](https://stereocrafter.github.io/)]

[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## Controllable Video Generation 

*[arxiv 2023.04]Motion-Conditioned Diffusion Model for Controllable Video Synthesis [[PDF](https://arxiv.org/abs/2304.14404), [Page](https://tsaishien-chen.github.io/MCDiff/)]

[arxiv 2023.06]Video Diffusion Models with Local-Global Context Guidance [[PDF](https://arxiv.org/abs/2306.02562)]

[arxiv 2023.06]VideoComposer: Compositional Video Synthesis with Motion Controllability [[PDF](https://arxiv.org/abs/2306.02018)]

[arxiv 2023.07]Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation [[PDF](https://arxiv.org/abs/2307.06940), [Page](https://videocrafter.github.io/Animate-A-Story)]

[arxiv 2023.10]MotionDirector: Motion Customization of Text-to-Video Diffusion Models [[PDF](https://arxiv.org/abs/2310.08465),[Page](https://showlab.github.io/MotionDirector/)]

[arxiv 2023.11]Space-Time Diffusion Features for Zero-Shot Text-Driven Motion Transfer[[PDF](https://arxiv.org/abs/2311.17009),[Page](https://diffusion-motion-transfer.github.io/)]

[arxiv 2023.11]SparseCtrl: Adding Sparse Controls to Text-to-Video Diffusion Models[[PDF](https://arxiv.org/abs/2311.16933), [Page](https://guoyww.github.io/projects/SparseCtrl)]

[arxiv 2023.12]Fine-grained Controllable Video Generation via Object Appearance and Context [[PDF](https://arxiv.org/abs/2312.02919),[Page](https://hhsinping.github.io/factor)]

[arxiv 2023.12]Drag-A-Video: Non-rigid Video Editing with Point-based Interaction [[PDF](https://arxiv.org/abs/2312.02936),[Page](https://drag-a-video.github.io/)]

[arxiv 2023.12]Peekaboo: Interactive Video Generation via Masked-Diffusion [[PDF](https://arxiv.org/abs/2312.07509),[Page](https://jinga-lala.github.io/projects/Peekaboo/)]

[arxiv 2023.12]InstructVideo: Instructing Video Diffusion Models with Human Feedback [[PDF](https://arxiv.org/abs/2312.12490),[Page](https://instructvideo.github.io/)]

[arxiv 2024.01]Motion-Zero: Zero-Shot Moving Object Control Framework for Diffusion-Based Video Generation[[PDF](https://arxiv.org/abs/2401.10150)]

[arxiv 2024.01]Synthesizing Moving People with 3D Control [[PDF](https://arxiv.org/abs/2401.10889),[PDF](https://boyiliee.github.io/3DHM.github.io/)]

[arxiv 2024.02]Boximator: Generating Rich and Controllable Motions for Video Synthesis [[PDF](https://arxiv.org/abs/2402.01566),[Page](https://boximator.github.io/)]

[arxiv 2024.02]InteractiveVideo: User-Centric Controllable Video Generation with Synergistic Multimodal Instructions [[PDF](https://arxiv.org/abs/2402.03040),[Page](https://github.com/invictus717/InteractiveVideo)]

[arxiv 2024.03]Animate Your Motion: Turning Still Images into Dynamic Videos [[PDF](https://arxiv.org/abs/2403.10179),[Page](https://mingxiao-li.github.io/smcd/)]

[arxiv 2024.04]Motion Inversion for Video Customization [[PDF](https://arxiv.org/abs/2403.20193),[Page](https://wileewang.github.io/MotionInversion/)]

[arxiv 2023.12]Generative Rendering: Controllable 4D-Guided Video Generation with 2D Diffusion Models [[PDF](https://arxiv.org/abs/2312.01409),[Page](https://primecai.github.io/generative_rendering/)]

[arxiv 2024.05]MOFA-Video: Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model [[PDF](https://arxiv.org/abs/2405.20222),[Page](https://myniuuu.github.io/MOFA_Video/)]

[arxiv 2024.06] FreeTraj: Tuning-Free Trajectory Control in Video Diffusion Models [[PDF](https://arxiv.org/abs/2406.16863),[Page](http://haonanqiu.com/projects/FreeTraj.html)]

[arxiv 2024.06] MVOC: a training-free multiple video object composition method with diffusion models [[PDF](https://arxiv.org/abs/2406.15829),[Page](https://sobeymil.github.io/mvoc.com/)]

[arxiv 2024.06] MotionBooth: Motion-Aware Customized Text-to-Video Generation [[PDF](https://arxiv.org/abs/2406.17758),[Page](https://jianzongwu.github.io/projects/motionbooth)]

[CVPR 2025] Tora: Trajectory-oriented Diffusion Transformer for Video Generation  [[PDF](https://arxiv.org/abs/2407.21705),[Page](https://ali-videoai.github.io/tora_video/)] [![Code](https://img.shields.io/github/stars/alibaba/Tora?style=social&label=Star)](https://github.com/alibaba/Tora)

[arxiv 2024.08] Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics[[PDF](https://arxiv.org/abs/2408.04631),[Page](https://vgg-puppetmaster.github.io/)]

[arxiv 2024.08] TrackGo: A Flexible and Efficient Method for Controllable Video Generation [[PDF](https://arxiv.org/abs/2408.11475),[Page](https://zhtjtcz.github.io/TrackGo-Page/)]

[arxiv 2024.10] DragEntity: Trajectory Guided Video Generation using Entity and Positional Relationships [[PDF](https://arxiv.org/abs/2410.10751)]

[arxiv 2024.10] MovieCharacter: A Tuning-Free Framework for Controllable Character Video Synthesis [[PDF](https://arxiv.org/abs/2410.20974),[Page](https://moviecharacter.github.io/)]

[arxiv 2024.11] SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation [[PDF](https://arxiv.org/abs/2411.04989),[Page](https://kmcode1.github.io/Projects/SG-I2V/)]

[arxiv 2024.11] Motion Control for Enhanced Complex Action Video Generation [[PDF](https://arxiv.org/abs/2411.08328),[Page](https://mvideo-v1.github.io/)]

[arxiv 2024.11] OnlyFlow: Optical Flow based Motion Conditioning for Video Diffusion Models [[PDF](https://arxiv.org/abs/2411.10501)]

[arxiv 2024.12] Motion Prompting: Controlling Video Generation with Motion Trajectories [[PDF](https://arxiv.org/abs/2412.02700),[Page](https://motion-prompting.github.io/)] 

[arxiv 2024.12] Video Motion Transfer with Diffusion Transformers [[PDF](https://arxiv.org/abs/2412.07776),[Page](https://ditflow.github.io/)] ![Code](https://img.shields.io/github/stars/ditflow/ditflow?style=social&label=Star)

[arxiv 2024.12] Repurposing Pre-trained Video Diffusion Models for Event-based Video Interpolation [[PDF](https://arxiv.org/abs/2412.07761/),[Page](https://vdm-evfi.github.io/)] 

[arxiv 2024.12] Trajectory Attention for Fine-grained Video Motion Control [[PDF](https://arxiv.org/abs/2411.19324),[Page]()] ![Code](https://img.shields.io/github/stars/xizaoqu/TrajectoryAttntion?style=social&label=Star)

[arxiv 2024.12] 3DTrajMaster: Mastering 3D Trajectory for Multi-Entity Motion in Video Generation [[PDF](https://arxiv.org/abs/2412.07759),[Page](http://fuxiao0719.github.io/projects/3dtrajmaster)] ![Code](https://img.shields.io/github/stars/KwaiVGI/3DTrajMaster?style=social&label=Star)

[arxiv 2024.12] Mojito: Motion Trajectory and Intensity Control for Video Generation [[PDF](https://arxiv.org/abs/2412.08948),[Page](https://sites.google.com/view/mojito-video)] ![Code](https://img.shields.io/github/stars/jkooy/mojito?style=social&label=Star)

[arxiv 2024.12] MotionBridge: Dynamic Video Inbetweening with Flexible Controls [[PDF](https://arxiv.org/pdf/2412.13190)]

[arxiv 2024.12] AniDoc: Animation Creation Made Easier [[PDF](https://arxiv.org/pdf/2412.14173),[Page](https://github.com/yihao-meng/AniDoc)] ![Code](https://img.shields.io/github/stars/yihao-meng/AniDoc?style=social&label=Star)

[arxiv 2024.12] LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis [[PDF](https://arxiv.org/abs/2412.15214),[Page](https://ppetrichor.github.io/levitor.github.io/)] ![Code](https://img.shields.io/github/stars/qiuyu96/LeviTor?style=social&label=Star)

[arxiv 2025.01] Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control [[PDF](https://arxiv.org/abs/2501.03847),[Page](https://igl-hkust.github.io/das/more_results.html#mesh-to-video)] ![Code](https://img.shields.io/github/stars/IGL-HKUST/DiffusionAsShader?style=social&label=Star)

[arxiv 2025.01] Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise [[PDF](https://arxiv.org/abs/2501.08331),[Page](https://vgenai-netflix-eyeline-research.github.io/Go-with-the-Flow/)] ![Code](https://img.shields.io/github/stars/VGenAI-Netflix-Eyeline-Research/Go-with-the-Flow?style=social&label=Star)

[arxiv 2025.01] LayerAnimate:Layer-specific Control for Animation [[PDF](https://arxiv.org/abs/2501.08295),[Page](https://layeranimate.github.io/)] ![Code](https://img.shields.io/github/stars/IamCreateAI/LayerAnimate?style=social&label=Star)

[arxiv 2025.01] Training-Free Motion-Guided Video Generation with Enhanced Temporal Consistency Using Motion Consistency Loss [[PDF](https://arxiv.org/abs/2501.07563v1),[Page](https://zhangxinyu-xyz.github.io/SimulateMotion.github.io/)] 

[arxiv 2025.01] Separate Motion from Appearance: Customizing Motion via Customizing Text-to-Video Diffusion Models [[PDF](https://arxiv.org/abs/2501.16714)]

[arxiv 2025.02] VidSketch: Hand-drawn Sketch-Driven Video Generation with Diffusion Control  [[PDF](https://arxiv.org/abs/2502.01101),[Page](https://csfufu.github.io/vid_sketch/)] ![Code](https://img.shields.io/github/stars/CSfufu/VidSketch?style=social&label=Star)

[arxiv 2025.02] MotionCanvas: Cinematic Shot Design with Controllable Image-to-Video Generation  [[PDF](https://arxiv.org/abs/2502.04299),[Page](https://motion-canvas25.github.io/)] 

[arxiv 2025.02] MotionMatcher: Motion Customization of Text-to-Video Diffusion Models via Motion Feature Matching  [[PDF](https://arxiv.org/abs/2502.13234),[Page](https://www.csie.ntu.edu.tw/~b09902097/motionmatcher/)] ![Code](https://img.shields.io/github/stars/b09902097/motionmatcher?style=social&label=Star)

[arxiv 2025.02]  C-Drag: Chain-of-Thought Driven Motion Controller for Video Generation [[PDF](https://arxiv.org/pdf/2502.19868),[Page](https://github.com/WesLee88524/C-Drag-Official-Repo)] ![Code](https://img.shields.io/github/stars/WesLee88524/C-Drag-Official-Repo?style=social&label=Star)

[arxiv 2025.03]  MagicMotion: Controllable Video Generation with Dense-to-Sparse Trajectory Guidance [[PDF](https://arxiv.org/abs/2503.16421),[Page](https://quanhaol.github.io/magicmotion-site/)] ![Code](https://img.shields.io/github/stars/quanhaol/MagicMotion?style=social&label=Star)

[arxiv 2025.03] PoseTraj: Pose-Aware Trajectory Control in Video Diffusion  [[PDF](https://arxiv.org/abs/2503.16068),[Page](https://robingg1.github.io/Pose-Traj/)] ![Code](https://img.shields.io/github/stars/robingg1/PoseTraj?style=social&label=Star)

[arxiv 2025.03] Enabling Versatile Controls for Video Diffusion Models  [[PDF](https://pp-vctrl.github.io/),[Page](https://pp-vctrl.github.io/)] ![Code](https://img.shields.io/github/stars/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl?style=social&label=Star)

[arxiv 2025.03] Re-HOLD: Video Hand Object Interaction Reenactment via adaptive Layout-instructed Diffusion Model  [[PDF](https://arxiv.org/abs/2503.16942),[Page](https://fyycs.github.io/Re-HOLD/)] 


[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)




## motion transfer | pose
[arxiv 2023.05]LEO: Generative Latent Image Animator for Human Video Synthesis [[PDF](https://arxiv.org/abs/2305.03989),[Page](https://wyhsirius.github.io/LEO-project/)]

*[arxiv 2023.03]Conditional Image-to-Video Generation with Latent Flow Diffusion Models [[PDF](https://arxiv.org/abs/2303.13744)]

[arxiv 2023.07]DisCo: Disentangled Control for Referring Human Dance Generation in Real World
[[PDF](https://arxiv.org/abs/2307.00040), [Page](https://disco-dance.github.io/)]

[arxiv 2023.11]MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model [[PDF](https://arxiv.org/abs/2311.16498), [Page](https://showlab.github.io/magicanimate)]

[arxiv 2023.12]DreaMoving: A Human Dance Video Generation Framework based on Diffusion Models [[PDF](https://arxiv.org/abs/2312.05107), [Page](https://dreamoving.github.io/dreamoving)]

[arxiv 2023.12]MotionEditor: Editing Video Motion via Content-Aware Diffusion [[PDF](https://arxiv.org/abs/2311.18830),[Page](https://francis-rings.github.io/MotionEditor/)]

[arxiv 2023.12]Customizing Motion in Text-to-Video Diffusion Models [[PDF](https://arxiv.org/abs/2312.04966),[Page](https://joaanna.github.io/customizing_motion/)]

[arxiv 2023.12]MotionCrafter: One-Shot Motion Customization of Diffusion Models [[PDF](https://arxiv.org/abs/2312.05288)]

[arxiv 2023.11]MagicDance: Realistic Human Dance Video Generation with Motions & Facial Expressions Transfer [[PDF](https://arxiv.org/abs/2311.12052), [Page](https://boese0601.github.io/magicdance/)]

[arxiv 2023.11]MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model[[PDF](https://arxiv.org/abs/2311.16498),[Page](https://showlab.github.io/magicanimate)]

[arxiv 2023.12] Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation[[PDF](https://arxiv.org/abs/2311.17117),[Page](https://humanaigc.github.io/animate-anyone/)]

[arxiv 2024.01]Motion-Zero: Zero-Shot Moving Object Control Framework for Diffusion-Based Video Generation[[PDF](https://arxiv.org/abs/2401.10150)]

[arxiv 2024.03]Spectral Motion Alignment for Video Motion Transfer using Diffusion Models[[PDF](https://arxiv.org/abs/2403.15249),[Page](https://geonyeong-park.github.io/spectral-motion-alignment/)]

[arxiv 2024.03]Champ: Controllable and Consistent Human Image Animation with 3D Parametric Guidance [[PDF](https://arxiv.org/pdf/2403.14781.pdf),[Page](https://fudan-generative-vision.github.io/champ/#/)]

[arxiv 2024.03]Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework [[PDF](https://arxiv.org/abs/2403.16510), [Page](https://github.com/ICTMCG/Make-Your-Anchor)]

[arxiv 2024.05]ReVideo: Remake a Video with Motion and Content Control [[PDF](https://arxiv.org/abs/2405.13865),[Page](https://mc-e.github.io/project/ReVideo/)]

[arxiv 2024.05]VividPose: Advancing Stable Video Diffusion for Realistic Human Image Animation  [[PDF](https://arxiv.org/abs/2405.18156)]

[arxiv 2024.05]Disentangling Foreground and Background Motion for Enhanced Realism in Human Video Generation [[PDF](https://arxiv.org/abs/2405.16393),[Page](https://liujl09.github.io/humanvideo_movingbackground/)]

[arxiv 2024.05] MusePose: a Pose-Driven Image-to-Video Framework for Virtual Human Generation. [[PDF](),[Page](https://github.com/TMElyralab/MusePose?tab=readme-ov-file)]

[arxiv 2024.05]MotionFollower: Editing Video Motion via Lightweight Score-Guided Diffusion [[PDF](https://arxiv.org/abs/2405.20325),[Page](https://francis-rings.github.io/MotionFollower/)]

[arxiv 2024.06] UniAnimate: Taming Unified Video Diffusion Models for Consistent Human Image Animation[[PDF](https://arxiv.org/abs/2406.01188),[Page](https://unianimate.github.io/)]

[arxiv 2024.06] Searching Priors Makes Text-to-Video Synthesis Better[[PDF](https://arxiv.org/abs/2406.03215),[Page](https://hrcheng98.github.io/Search_T2V/)]

[arxiv 2024.06]MotionClone: Training-Free Motion Cloning for Controllable Video Generation[[PDF](https://arxiv.org/pdf/2406.05338)]

[arxiv 2024.07]IDOL: Unified Dual-Modal Latent Diffusion for Human-Centric Joint Video-Depth Generation  [[PDF](https://arxiv.org/abs/2407.10937),[Page](https://yhzhai.github.io/idol/)]

[arxiv 2024.07]HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation  [[PDF](https://arxiv.org/abs/2407.17438),[Page](https://github.com/zhenzhiwang/HumanVid/)]

[arxiv 2024.10] Replace Anyone in Videos [[PDF](https://arxiv.org/abs/2409.19911)]

[arxiv 2024.10] MIMO: Controllable Character Video Synthesis with Spatial Decomposed Modeling [[PDF](http://arxiv.org/abs/2409.16160),[Page](https://menyifang.github.io/projects/MIMO/index.html)]

[arxiv 2024.11] MikuDance: Animating Character Art with Mixed Motion Dynamics [[PDF](https://arxiv.org/abs/2411.08656),[Page](https://kebii.github.io/MikuDance/)]

[arxiv 2024.11] StableAnimator: High-Quality Identity-Preserving Human Image Animation [[PDF](https://arxiv.org/abs/2411.17697),[Page](https://francis-rings.github.io/StableAnimator/)] ![Code](https://img.shields.io/github/stars/Francis-Rings/StableAnimator?style=social&label=Star)

[arxiv 2024.11] AnimateAnything: Consistent and Controllable Animation for Video Generation [[PDF](https://arxiv.org/abs/2411.10836),[Page](https://yu-shaonian.github.io/Animate_Anything/)]

[arxiv 2024.11] AnchorCrafter: Animate CyberAnchors Saling Your Products via Human-Object Interacting Video Generation  [[PDF](https://arxiv.org/abs/2411.17383),[Page](https://cangcz.github.io/Anchor-Crafter/)] ![Code](https://img.shields.io/github/stars/cangcz/AnchorCrafter?style=social&label=Star)

[arxiv 2024.12]  Fleximo: Towards Flexible Text-to-Human Motion Video Generation [[PDF](https://arxiv.org/abs/2411.19459)] 

[arxiv 2024.12]  MotionFlow: Attention-Driven Motion Transfer in Video Diffusion Models [[PDF](https://arxiv.org/pdf/2412.05275.pdf),[Page](https://motionflow-diffusion.github.io/)] 

[arxiv 2024.12]  MotionShop: Zero-Shot Motion Transfer in Video Diffusion Models with Mixture of Score Guidance [[PDF](https://motionshop-diffusion.github.io/#),[Page](https://motionshop-diffusion.github.io/#)] ![Code](https://img.shields.io/github/stars/gemlab-vt/motionshop?style=social&label=Star)

[arxiv 2024.12] DisPose: Disentangling Pose Guidance for Controllable Human Image Animation  [[PDF](https://arxiv.org/abs/2412.09349)]

[arxiv 2024.12] Consistent Human Image and Video Generation with Spatially Conditioned Diffusion  [[PDF](https://arxiv.org/abs/2412.14531),[Page](https://github.com/ljzycmd/SCD)] ![Code](https://img.shields.io/github/stars/ljzycmd/SCD?style=social&label=Star)

[arxiv 2024.12] VAST 1.0: A Unified Framework for Controllable and Consistent Video Generation  [[PDF](https://arxiv.org/pdf/2412.16677)]

[arxiv 2025.01]  RAIN: Real-time Animation Of Infinite Video Stream [[PDF](https://arxiv.org/abs/2412.19489),[Page](https://pscgylotti.github.io/pages/RAIN/)] ![Code](https://img.shields.io/github/stars/Pscgylotti/RAIN?style=social&label=Star)

[arxiv 2025.01]  X-Dyna: Expressive Dynamic Human Image Animation [[PDF](https://arxiv.org/abs/2501.10021),[Page](https://x-dyna.github.io/xdyna.github.io/)] ![Code](https://img.shields.io/github/stars/bytedance/X-Dyna?style=social&label=Star)

[arxiv 2025.02] HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion Video Generation  [[PDF](https://arxiv.org/abs/2502.04847),[Page](https://agnjason.github.io/HumanDiT-page/)] 

[arxiv 2025.02] AnyCharV: Bootstrap Controllable Character Video Generation with Fine-to-Coarse Guidance  [[PDF](),[Page](https://anycharv.github.io/)] ![Code](https://img.shields.io/github/stars/AnyCharV/AnyCharV?style=social&label=Star)

[arxiv 2025.03] Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control  [[PDF](https://arxiv.org/abs/2503.14492),[Page](https://github.com/nvidia-cosmos/cosmos-transfer1)] ![Code](https://img.shields.io/github/stars/nvidia-cosmos/cosmos-transfer1?style=social&label=Star)

[arxiv 2025.03] Decouple and Track: Benchmarking and Improving Video Diffusion Transformers For Motion Transfer  [[PDF](https://arxiv.org/pdf/2503.17350),[Page](https://shi-qingyu.github.io/DeT.github.io/)] 



[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)




## autoregressive for video 

[arxiv 2024.12]  Autoregressive Video Generation without Vector Quantization [[PDF](https://arxiv.org/abs/2412.14169),[Page](https://github.com/baaivision/NOVA)] ![Code](https://img.shields.io/github/stars/baaivision/NOVA?style=social&label=Star)

[arxiv 2025.03] AR-Diffusion: Asynchronous Video Generation with Auto-Regressive Diffusion  [[PDF](https://arxiv.org/abs/2503.07418),[Page](https://github.com/iva-mzsun/AR-Diffusion)] ![Code](https://img.shields.io/github/stars/iva-mzsun/AR-Diffusion?style=social&label=Star)

[arxiv 2025.03]  Fast Autoregressive Video Generation with Diagonal Decoding[[PDF](https://arxiv.org/abs/2503.14070),[Page](https://www.microsoft.com/en-us/research/project/ar-videos/diagonal-decoding/)] 


[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## text 
[arxiv 2024.06]  Text-Animator: Controllable Visual Text Video Generation[[PDF](https://arxiv.org/abs/2406.17777),[Page](https://laulampaul.github.io/text-animator.html)]

[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)



## Camera 
[arxiv 2023.12]MotionCtrl: A Unified and Flexible Motion Controller for Video Generation [[PDF](https://arxiv.org/abs/2312.03641),[Page](https://wzhouxiff.github.io/projects/MotionCtrl/)]

[arxiv 2024.02]Direct-a-Video: Customized Video Generation with User-Directed Camera Movement and Object Motion [[PDF](https://arxiv.org/pdf/2402.03162.pdf),[Page](https://direct-a-video.github.io/)]

[arxiv 2024.04]CameraCtrl: Enabling Camera Control for Text-to-Video Generation [[PDF](https://arxiv.org/abs/2404.02101),[Page](https://hehao13.github.io/projects-CameraCtrl/)]

[arxiv 2024.04]Customizing Text-to-Image Diffusion with Camera Viewpoint Control [[PDF](https://arxiv.org/abs/2404.12333),[Page](https://customdiffusion360.github.io/)]

[arxiv 2024.04]MotionMaster: Training-free Camera Motion Transfer For Video Generation[[PDF](https://arxiv.org/abs/2404.15789)]

[arxiv 2024.05] Video Diffusion Models are Training-free Motion Interpreter and Controller[[PDF](https://arxiv.org/abs/2405.14864),[Page](https://xizaoqu.github.io/moft/)]

[arxiv 2024.05] VidvidDream Generating 3D Scene with Ambient Dynamics [[PDF](https://arxiv.org/abs/2405.20334),[Page](https://vivid-dream-4d.github.io/)]

[arxiv 2024.06] CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation [[PDF](https://arxiv.org/abs/2406.02509),[Page](https://ir1d.github.io/CamCo/)]

[arxiv 2024.06]Training-free Camera Control for Video Generation[[PDF](https://arxiv.org/abs/2406.10126),[Page](https://lifedecoder.github.io/CamTrol/)]

[arxiv 2024.06] Image Conductor: Precision Control for Interactive Video Synthesis [[PDF](https://liyaowei-stu.github.io/project/ImageConductor/),[Page](https://liyaowei-stu.github.io/project/ImageConductor/)]

[arxiv 2024.07]VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control [[PDF](https://arxiv.org/abs/2407.12781),[Page](https://snap-research.github.io/vd3d/)]

[arxiv 2024.08] DreamCinema: Cinematic Transfer with Free Camera and 3D Character [[PDF](https://arxiv.org/abs/2408.12601),[Page](https://liuff19.github.io/DreamCinema)]

[arxiv 2024.09] CinePreGen: Camera Controllable Video Previsualization via Engine-powered Diffusion[[PDF](https://arxiv.org/abs/2408.17424)]

[arxiv 2024.10] Boosting Camera Motion Control for Video Diffusion Transformers [[PDF](https://arxiv.org/abs/2410.10802),[Page](https://soon-yau.github.io/CameraMotionGuidance/)]

[arxiv 2024.10] Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention [[PDF](https://arxiv.org/abs/2410.10774),[Page](https://ir1d.github.io/Cavia/)]

[arxiv 2024.10] CamI2V: Camera-Controlled Image-to-Video Diffusion Model [[PDF](https://arxiv.org/abs/2410.15957),[Page](https://zgctroy.github.io/CamI2V/)]

[arxiv 2024.11] ReCapture: Generative Video Camera Controls for User-Provided Videos using Masked Video Fine-Tuning [[PDF](https://arxiv.org/abs/2411.05003),[Page](https://generative-video-camera-controls.github.io/)]

[arxiv 2024.11] I2VControl-Camera: Precise Video Camera Control with Adjustable Motion Strength [[PDF](https://arxiv.org/abs/2411.06525)]

[arxiv 2024.11] AnimateAnything: Consistent and Controllable Animation for Video Generation [[PDF](https://arxiv.org/abs/2411.10836),[Page](https://yu-shaonian.github.io/Animate_Anything/)] ![Code](https://img.shields.io/github/stars/yu-shaonian/AnimateAnything?style=social&label=Star)


[arxiv 2024.12] I2VControl: Disentangled and Unified Video Motion Synthesis Control [[PDF](https://arxiv.org/abs/2411.17765),[Page](https://wanquanf.github.io/I2VControl)] 

[arxiv 2024.12] Generative Photography Scene-Consistent Camera Control for Realistic Text-to-Image Synthesis [[PDF](https://arxiv.org/abs/2412.02168),[Page](https://generative-photography.github.io/project/)] 

[arxiv 2024.12] CPA: Camera-pose-awareness Diffusion Transformer for Video Generation [[PDF](https://arxiv.org/abs/2412.01429)]

[arxiv 2024.12] Latent-Reframe: Enabling Camera Control for Video Diffusion Model without Training [[PDF](https://arxiv.org/abs/2412.06029),[Page](https://latent-reframe.github.io/)]

[arxiv 2024.12] SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse Viewpoints [[PDF](https://arxiv.org/abs/2412.07760),[Page](https://jianhongbai.github.io/SynCamMaster/)] ![Code](https://img.shields.io/github/stars/KwaiVGI/SynCamMaster?style=social&label=Star)


[arxiv 2024.12] ObjCtrl-2.5D: Training-free Object Control with Camera Poses [[PDF](https://arxiv.org/pdf/2412.07721),[Page](https://wzhouxiff.github.io/projects/ObjCtrl-2.5D/)] ![Code](https://img.shields.io/github/stars/wzhouxiff/ObjCtrl-2.5D?style=social&label=Star)

[arxiv 2024.12] Learning Camera Movement Control from Real-World Drone Videos [[PDF](https://arxiv.org/abs/2412.09620),[Page](https://dvgformer.github.io/)] ![Code](https://img.shields.io/github/stars/hou-yz/dvgformer?style=social&label=Star)

[arxiv 2024.12] Switch-a-View: Few-Shot View Selection Learned from Edited Videos [[PDF](https://arxiv.org/abs/2412.18386),[Page](https://vision.cs.utexas.edu/projects/switch_a_view/)] 

[arxiv 2025.01] Free-Form Motion Control: A Synthetic Video Generation Dataset with Controllable Camera and Object Motions [[PDF](https://arxiv.org/abs/2501.01425),[Page](https://henghuiding.github.io/SynFMC/)] 

[arxiv 2025.01] OG3R: On Unifying Video Generation and Camera Pose Estimation [[PDF](https://arxiv.org/abs/2501.01409),[Page](https://paulchhuang.github.io/jog3rwebsite/)] 

[arxiv 2025.01] Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise [[PDF](https://arxiv.org/abs/2501.08331),[Page](https://vgenai-netflix-eyeline-research.github.io/Go-with-the-Flow/)] ![Code](https://img.shields.io/github/stars/VGenAI-Netflix-Eyeline-Research/Go-with-the-Flow?style=social&label=Star)

[arxiv 2025.02] CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation  [[PDF](https://arxiv.org/abs/2502.08639),[Page](https://cinemaster-dev.github.io/)] 

[arxiv 2025.02] FloVD: Optical Flow Meets Video Diffusion Model for Enhanced Camera-Controlled Video Synthesis  [[PDF](https://arxiv.org/pdf/2502.08244)]

[arxiv 2025.02] VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation  [[PDF](https://arxiv.org/pdf/2502.07531)]

[arxiv 2025.02]  RealCam-I2V: Real-World Image-to-Video Generation with Interactive Complex Camera Control [[PDF](https://arxiv.org/abs/2502.10059),[Page](https://zgctroy.github.io/RealCam-I2V/)] ![Code](https://img.shields.io/github/stars/ZGCTroy/RealCam-I2V?style=social&label=Star)

[arxiv 2025.03] GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control  [[PDF](https://arxiv.org/pdf/2503.03751),[Page](https://research.nvidia.com/labs/toronto-ai/GEN3C/)] ![Code](https://img.shields.io/github/stars/nv-tlabs/GEN3C?style=social&label=Star)

[arxiv 2025.03] TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models  [[PDF](https://arxiv.org/abs/2503.05638),[Page](https://trajectorycrafter.github.io/)] ![Code](https://img.shields.io/github/stars/TrajectoryCrafter/TrajectoryCrafter?style=social&label=Star)

[arxiv 2025.03] Reangle-A-Video: 4D Video Generation as Video-to-Video Translation  [[PDF](https://arxiv.org/pdf/2503.09151)]

[arxiv 2025.03]  CameraCtrl II: Dynamic Scene Exploration via Camera-controlled Video Diffusion Models  [[PDF](https://arxiv.org/abs/2503.10592),[Page](https://hehao13.github.io/Projects-CameraCtrl-II/)] 

[arxiv 2025.03] I2V3D: Controllable image-to-video generation with 3D guidance  [[PDF](https://arxiv.org/pdf/2503.09733),[Page](https://bestzzhang.github.io/I2V3D/)] 

[arxiv 2025.03]  ReCamMaster: Camera-Controlled Generative Rendering from A Single Video [[PDF](https://arxiv.org/abs/2503.11647),[Page](https://jianhongbai.github.io/ReCamMaster/)] ![Code](https://img.shields.io/github/stars/KwaiVGI/ReCamMaster?style=social&label=Star)


[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## lighting 
[arxiv 2025.02] Light-A-Video: Training-free Video Relighting via Progressive Light Fusion  [[PDF](https://arxiv.org/abs/2502.08590),[Page](https://bujiazi.github.io/light-a-video.github.io/)] ![Code](https://img.shields.io/github/stars/bcmi/Light-A-Video/?style=social&label=Star)

[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## inpainting / outpainting 
[MM 2023.09]Hierarchical Masked 3D Diffusion Model for Video Outpainting [[PDF](https://arxiv.org/abs/2309.02119)]

[arxiv 2023.11]Flow-Guided Diffusion for Video Inpainting [[PDF](https://arxiv.org/abs/2311.15368)]

[arxiv 2024.01]ActAnywhere: Subject-Aware Video Background Generation [[PDF](https://arxiv.org/abs/2401.10822), [Page](https://actanywhere.github.io/)]

[arxiv 2024.03]CoCoCo: Improving Text-Guided Video Inpainting for Better Consistency, Controllability and Compatibility [[PDF](https://arxiv.org/abs/2403.12035),[Page](https://cococozibojia.github.io/)]

[arxiv 2024.03]Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific Adaptation [[PDF](https://arxiv.org/abs/2403.13745),[Page](https://github.com/G-U-N/Be-Your-Outpainter)]

[arxiv 2024.04]AudioScenic: Audio-Driven Video Scene Editing [[PDF](https://arxiv.org/abs/2404.16581)]

[arxiv 2024.05]Semantically Consistent Video Inpainting with Conditional Diffusion Models [[PDF(https://arxiv.org/abs/2405.00251)]

[arxiv 2024.05]ReVideo: Remake a Video with Motion and Content Control [[PDF](https://arxiv.org/abs/2405.13865),[Page](https://mc-e.github.io/project/ReVideo/)]

[arxiv 2024.08]Video Diffusion Models are Strong Video Inpainter  [[PDF](https://arxiv.org/abs/2408.11402)]

[arxiv 2024.09] Follow-Your-Canvas: Higher-Resolution Video Outpainting with Extensive Content Generation [[PDF](https://arxiv.org/abs/2409.01055),[Page](https://github.com/mayuelala/FollowYourCanvas)]

[arxiv 2024.12]  UniPaint: Unified Space-time Video Inpainting via Mixture-of-Experts [[PDF](https://arxiv.org/abs/2412.06340)]

[arxiv 2024.12] OmniDrag: Enabling Motion Control for Omnidirectional Image-to-Video Generation  [[PDF](https://arxiv.org/abs/2412.09623),[Page](https://lwq20020127.github.io/OmniDrag/)] 

[arxiv 2025.01] DiffuEraser: A Diffusion Model for Video Inpainting  [[PDF](https://arxiv.org/abs/2501.10018),[Page](https://github.com/lixiaowen-xw/DiffuEraser)] ![Code](https://img.shields.io/github/stars/lixiaowen-xw/DiffuEraser?style=social&label=Star)


[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## Video Quality 
[arxiv 2024.03]VideoElevator : Elevating Video Generation Quality with Versatile Text-to-Image Diffusion Models[[PDF](https://arxiv.org/abs/2403.05438),[Page](https://videoelevator.github.io/)]




## super-resolution
[arxiv 2023.11]Enhancing Perceptual Quality in Video Super-Resolution through Temporally-Consistent Detail Synthesis using Diffusion Models [[PDF](https://arxiv.org/abs/2311.15908)]

[arxiv 2023.12]Upscale-A-Video: Temporal-Consistent Diffusion Model for Real-World Video Super-Resolution [[PDF](https://arxiv.org/abs/2312.06640),[Page](https://shangchenzhou.com/projects/upscale-a-video/)]

[arxiv 2023.12]Video Dynamics Prior: An Internal Learning Approach for Robust Video Enhancements [[PDF](https://arxiv.org/abs/2312.07835),[Page](http://www.cs.umd.edu/~gauravsh/vdp.html)]

[arxiv 2024.03]Learning Spatial Adaptation and Temporal Coherence in Diffusion Models for Video Super-Resolution [[PDF](https://arxiv.org/abs/2403.17000)]

[arxiv 2024.04]VideoGigaGAN: Towards Detail-rich Video Super-Resolution [[PDF](https://videogigagan.github.io/assets/paper.pdf), [Page](https://videogigagan.github.io/)]

[arxiv 2024.06] EvTexture: Event-driven Texture Enhancement for Video Super-Resolution [[PDF](https://arxiv.org/abs/2406.13457),[Page](https://dachunkai.github.io/evtexture.github.io/)]

[arxiv 2024.06]  DiffIR2VR-Zero:Zero-Shot Video Restoration with Diffusion-based Image Restoration Models [[PDF](https://arxiv.org/abs/2304.06706),[Page](https://jimmycv07.github.io/DiffIR2VR_web/)]

[arxiv 2024.07]  DiffIR2VR-Zero: Zero-Shot Video Restoration with Diffusion-based Image Restoration Models [[PDF](https://arxiv.org/abs/2407.01519),[Page](https://jimmycv07.github.io/DiffIR2VR_web/)]

[arxiv 2024.07] Zero-shot Video Restoration and Enhancement Using Pre-Trained Image Diffusion Model  [[PDF](https://arxiv.org/abs/2407.01960)]

[arxiv 2024.07] VEnhancer: Generative Space-Time Enhancement for Video Generation[[PDF](https://arxiv.org/abs/2407.07667),[Page](https://vchitect.github.io/VEnhancer-project/)]


[arxiv 2024.07] Noise Calibration: Plug-and-play Content-Preserving Video Enhancement using Pre-trained Video Diffusion Models [[PDF](https://arxiv.org/abs/2407.10285),[Page](https://yangqy1110.github.io/NC-SDEdit/)]

[arxiv 2024.07]  RealViformer: Investigating Attention for Real-World Video Super-Resolution [[PDF](),[Page]()]

[arxiv 2024.08]Kalman-Inspired Feature Propagation for Video Face Super-Resolution[[PDF](https://arxiv.org/abs/2408.05205),[Page](https://jnjaby.github.io/projects/KEEP/)]

[arxiv 2024.08] Unrolled Decomposed Unpaired Learning for Controllable Low-Light Video Enhancement  [[PDF](https://arxiv.org/abs/2408.12316),[Page](https://github.com/lingyzhu0101/UDU)]

[arxiv 2024.08] SeeClear: Semantic Distillation Enhances Pixel Condensation for Video Super-Resolution  [[PDF](https://arxiv.org/abs/2410.05799),[Page](https://github.com/Tang1705/SeeClear-NeurIPS24)]

[arxiv 2025.01]  SeedVR: Seeding Infinity in Diffusion Transformer Towards Generic Video Restoration [[PDF](https://arxiv.org/abs/2501.01320),[Page](https://iceclear.github.io/projects/seedvr/)] 

[arxiv 2025.01]  STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution [[PDF](https://arxiv.org/abs/2501.02976),[Page](https://nju-pcalab.github.io/projects/STAR/)] ![Code](https://img.shields.io/github/stars/NJU-PCALab/STAR?style=social&label=Star)

[arxiv 2025.01]  SVFR: A Unified Framework for Generalized Video Face Restoration [[PDF](https://arxiv.org/pdf/2501.01235),[Page](https://wangzhiyaoo.github.io/SVFR/)] ![Code](https://img.shields.io/github/stars/wangzhiyaoo/SVFR?style=social&label=Star)

[arxiv 2025.01]  DiffVSR: Enhancing Real-World Video Super-Resolution with Diffusion Models for Advanced Visual Quality and Temporal Consistency [[PDF](https://arxiv.org/abs/2501.10110),[Page](https://xh9998.github.io/DiffVSR-project/)] ![Code](https://img.shields.io/github/stars/xh9998/DiffVSR?style=social&label=Star)

[arxiv 2025.03]  Temporal-Consistent Video Restoration with Pre-trained Diffusion Models [[PDF](https://arxiv.org/pdf/2503.14863)]


[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## restoration 
[arxiv 2024.08] Towards Real-world Event-guided Low-light Video Enhancement and Deblurring[[PDF](https://arxiv.org/abs/2408.14916)]

[arxiv 2024.08] Cross-Modal Temporal Alignment for Event-guided Video Deblurring[[PDF](https://arxiv.org/abs/2408.14930)]

[arxiv 2025.01]  SVFR: A Unified Framework for Generalized Video Face Restoration [[PDF](https://arxiv.org/pdf/2501.01235),[Page](https://wangzhiyaoo.github.io/SVFR/)] ![Code](https://img.shields.io/github/stars/wangzhiyaoo/SVFR?style=social&label=Star)

[arxiv 2025.02] Human Body Restoration with One-Step Diffusion Model and A New Benchmark  [[PDF](https://arxiv.org/abs/2502.01411),[Page](https://github.com/gobunu/OSDHuman)] ![Code](https://img.shields.io/github/stars/gobunu/OSDHuman?style=social&label=Star)



[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## downstream apps
[arxiv 2023.11]Breathing Life Into Sketches Using Text-to-Video Priors [[PDF](https://arxiv.org/abs/2311.13608),[Page](https://livesketch.github.io/)]

[arxiv 2023.11]Flow-Guided Diffusion for Video Inpainting [[PDF](https://arxiv.org/abs/2311.15368)]

[arxiv 2024.02]Animated Stickers: Bringing Stickers to Life with Video Diffusion [[PDF](https://arxiv.org/abs/2402.06088)]

[arxiv 2024.03]DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation [[PDF](https://arxiv.org/abs/2403.06845),[Page](https://drivedreamer2.github.io/)]

[arxiv 2024.03]Intention-driven Ego-to-Exo Video Generation [[PDF](https://arxiv.org/abs/2403.09194)]

[arxiv 2024.04]PhysDreamer: Physics-Based Interaction with 3D Objects via Video Generation [[PDF](https://arxiv.org/abs/2404.13026),[Page](https://physdreamer.github.io/)]

[arxiv 2024.04]Tunnel Try-on: Excavating Spatial-temporal Tunnels for High-quality Virtual Try-on in Videos [[PDF](https://arxiv.org/abs/2404.17571),[Page](https://mengtingchen.github.io/tunnel-try-on-page/)]

[arxiv 2024.04]Dance Any Beat: Blending Beats with Visuals in Dance Video Generation [[PDF](https://arxiv.org/abs/2405.09266), [Page](https://dabfusion.github.io/)]

[arxiv 2024.05] ViViD: Video Virtual Try-on using Diffusion Models  [[PDF](https://arxiv.org/abs/2405.11794),[Page](https://becauseimbatman0.github.io/ViViD)]

[arxiv 2024.05] VITON-DiT: Learning In-the-Wild Video Try-On from Human Dance Videos via Diffusion Transformers[[PDF](https://arxiv.org/abs/2405.18326),[Page](https://zhengjun-ai.github.io/viton-dit-page/)]

[arxiv 2024.07]WildVidFit: Video Virtual Try-On in the Wild via Image-Based Controlled Diffusion Models [[PDF](https://arxiv.org/abs/2407.10625), [Page](https://wildvidfit-project.github.io/)]

[arxiv 2024.07]Streetscapes: Large-scale Consistent Street View Generation Using Autoregressive Video Diffusion
 [[PDF](https://arxiv.org/abs/2407.13759), [Page](https://boyangdeng.com/streetscapes)]

[arxiv 2024.08] Panacea+: Panoramic and Controllable Video Generation for Autonomous Driving[[PDF](https://arxiv.org/abs/2408.07605), [Page](https://panacea-ad.github.io/)]

[arxiv 2024.08]Diffusion Models Are Real-Time Game Engines [[PDF](https://arxiv.org/abs/2408.14837), [Page](https://gamengen.github.io/)]

[arxiv 2024.09] DriveScape: Towards High-Resolution Controllable Multi-View Driving Video Generation [[PDF](https://arxiv.org/abs/2409.05463), [Page](https://metadrivescape.github.io/papers_project/drivescapev1/index.html)]

[arxiv 2024.09] Pose-Guided Fine-Grained Sign Language Video Generation [[PDF](https://arxiv.org/abs/2409.16709)]

[arxiv 2024.10] VidPanos: Generative Panoramic Videos from Casual Panning Videos [[PDF](https://arxiv.org/abs/2410.13832), [Page](https://vidpanos.github.io/)]

[arxiv 2024.11]  GameGen-X: Interactive Open-world Game Video Generation[[PDF](https://arxiv.org/abs/2411.00769), [Page](https://github.com/GameGen-X/GameGen-X)]

[arxiv 2024.11] Fashion-VDM: Video Diffusion Model for Virtual Try-On [[PDF](https://arxiv.org/abs/2411.00225), [Page]()]

[arxiv 2024.11] EgoVid-5M: A Large-Scale Video-Action Dataset for Egocentric Video Generation [[PDF](https://arxiv.org/abs/2411.08380), [Page](https://egovid.github.io/)]

[arxiv 2024.11] FlipSketch: Flipping Static Drawings to Text-Guided Sketch Animations [[PDF](https://arxiv.org/abs/2411.10818), [Page](https://github.com/hmrishavbandy/FlipSketch)]

[arxiv 2024.11] PhysMotion: Physics-Grounded Dynamics From a Single Image [[PDF](https://arxiv.org/abs/2411.17189),[Page](https://supertan0204.github.io/physmotion_website/)] 

[arxiv 2024.11] InTraGen: Trajectory-controlled Video Generation for Object Interactions [[PDF](https://arxiv.org/abs/2411.16804),[Page](https://github.com/insait-institute/InTraGen)] ![Code](https://img.shields.io/github/stars/insait-institute/InTraGen?style=social&label=Star)

[arxiv 2024.12] MatchDiffusion:Training-free Generation of Match-Cuts [[PDF](https://arxiv.org/abs/2411.18677),[Page](https://matchdiffusion.github.io/)] ![Code](https://img.shields.io/github/stars/PardoAlejo/MatchDiffusion?style=social&label=Star)

[arxiv 2024.12]  Instructional Video Generation [[PDF](https://arxiv.org/pdf/2412.04189),[Page](https://excitedbutter.github.io/Instructional-Video-Generation/)] 

[arxiv 2024.12] InfiniCube: Unbounded and Controllable Dynamic 3D Driving Scene Generation with World-Guided Video Models [[PDF](https://arxiv.org/abs/2412.03934),[Page](https://research.nvidia.com/labs/toronto-ai/infinicube/)] 

[arxiv 2024.12] Video Creation by Demonstration [[PDF](https://arxiv.org/abs/2412.09551),[Page](https://delta-diffusion.github.io/)] 

[arxiv 2024.12] InterDyn: Controllable Interactive Dynamics with Video Diffusion Models [[PDF](https://arxiv.org/abs/2412.11785),[Page](https://interdyn.is.tue.mpg.de/)] 

[arxiv 2025.01] TransPixar: Advancing Text-to-Video Generation with Transparency [[PDF](https://arxiv.org/abs/2501.03006),[Page](https://wileewang.github.io/TransPixar/)] ![Code](https://img.shields.io/github/stars/wileewang/TransPixar?style=social&label=Star)

[arxiv 2025.01] Cosmos World Foundation Model Platform for Physical AI [[PDF](https://arxiv.org/abs/2501.03575),[Page](https://www.nvidia.com/en-us/ai/cosmos/)] ![Code](https://img.shields.io/github/stars/NVIDIA/Cosmos?style=social&label=Star)

[arxiv 2025.01] SynthLight: Portrait Relighting with Diffusion Model by Learning to Re-render Synthetic Faces [[PDF](https://arxiv.org/abs/2501.09756),[Page](https://vrroom.github.io/synthlight/)] 

[arxiv 2025.01] VanGogh: A Unified Multimodal Diffusion-based Framework for Video Colorization [[PDF](https://arxiv.org/abs/2501.09499),[Page](https://becauseimbatman0.github.io/VanGogh)] ![Code](https://img.shields.io/github/stars/BecauseImBatman0/VanGogh?style=social&label=Star)

[arxiv 2025.01] RelightVid: Temporal-Consistent Diffusion Model for Video Relighting [[PDF](https://arxiv.org/abs/2501.16330)]

[arxiv 2025.02] Mobius: Text to Seamless Looping Video Generation via Latent Shift  [[PDF](https://arxiv.org/abs/2502.20307),[Page](https://mobius-diffusion.github.io/)] ![Code](https://img.shields.io/github/stars/YisuiTT/Mobius?style=social&label=Star)

[arxiv 2025.03]  TASTE-Rob: Advancing Video Generation of Task-Oriented Hand-Object Interaction for Generalizable Robotic Manipulation [[PDF](https://arxiv.org/pdf/2503.11423),[Page](https://taste-rob.github.io/)]

[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## Concept 
[arxiv 2023.07]Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation [[PDF](https://arxiv.org/abs/2307.06940), [Page](https://videocrafter.github.io/Animate-A-Story)]

[arxiv 2023.11]VideoDreamer: Customized Multi-Subject Text-to-Video Generation with Disen-Mix Finetuning[[PDF](https://arxiv.org/pdf/%3CARXIV%20PAPER%20ID%3E.pdf),[Page](https://videodreamer23.github.io/)]

[arxiv 2023.12]VideoAssembler: Identity-Consistent Video Generation with Reference Entities using Diffusion Model [[PDF](https://arxiv.org/abs/2311.17338),[Page](https://gulucaptain.github.io/videoassembler/)]

[arxiv 2023.12]VideoBooth: Diffusion-based Video Generation with Image Prompts [[PDF](https://arxiv.org/abs/2312.00777),[Page](https://vchitect.github.io/VideoBooth-project/)]

[arxiv 2023.12]DreamVideo: Composing Your Dream Videos with Customized Subject and Motion [[PDF](https://arxiv.org/abs/2312.04433),[Page](https://dreamvideo-t2v.github.io/)]

[arxiv 2023.12]PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models [[PDF](https://pi-animator.github.io/)]

[arxiv 2024.01]CustomVideo: Customizing Text-to-Video Generation with Multiple Subjects [[PDF](https://arxiv.org/abs/2401.09962)]

[arxiv 2024.02]Magic-Me: Identity-Specific Video Customized Diffusion [[PDf](https://arxiv.org/abs/2402.09368),[Page](https://magic-me-webpage.github.io/)]

[arxiv 2024.03]EVA: Zero-shot Accurate Attributes and Multi-Object Video Editing [[PDF](https://arxiv.org/abs/2403.16111),[Page](https://knightyxp.github.io/EVA/)]

[arxiv 2024.04]AniClipart: Clipart Animation with Text-to-Video Priors [[PDF](https://arxiv.org/abs/2404.12347),[Page](https://aniclipart.github.io/)]

[arxiv 2024.04]ID-Animator: Zero-Shot Identity-Preserving Human Video Generation [[PDF](),[Page](https://id-animator.github.io/)]

[arxiv 2024.07]Still-Moving: Customized Video Generation without Customized Video Data [[PDF](https://arxiv.org/abs/2407.08674),[Page](https://still-moving.github.io/)]

[arxiv 2024.08] CustomCrafter: Customized Video Generation with Preserving Motion and Concept Composition Abilities[[PDF](https://arxiv.org/abs/2408.13239),[Page](https://customcrafter.github.io/)]

[arxiv 2024.10] TweedieMix: Improving Multi-Concept Fusion for Diffusion-based Image/Video Generation [[PDF](https://arxiv.org/abs/2410.05591),[Page](https://github.com/KwonGihyun/TweedieMix)]

[arxiv 2024.10] PersonalVideo: High ID-Fidelity Video Customization With Static Images [[PDF](https://openreview.net/pdf?id=ndtFyx7UWs),[Page](https://personalvideo.github.io/)]

[arxiv 2024.10] DreamVideo-2: Zero-Shot Subject-Driven Video Customization with Precise Motion Control [[PDF](https://arxiv.org/abs/2410.13830),[Page](https://dreamvideo2.github.io/)]

[arxiv 2024.12]  MotionCharacter: Identity-Preserving and Motion Controllable Human Video Generation [[PDF](https://arxiv.org/abs/2411.18281),[Page](https://motioncharacter.github.io/)]

[arxiv 2024.12] Multi-Shot Character Consistency for Text-to-Video Generation  [[PDF](https://arxiv.org/abs/2412.07750),[Page](https://research.nvidia.com/labs/par/video_storyboarding)] 

[arxiv 2024.12] LoRACLR: Contrastive Adaptation for Customization of Diffusion Models  [[PDF](https://arxiv.org/abs/2412.09622),[Page](https://loraclr.github.io/)]

[arxiv 2024.12] CustomTTT: Motion and Appearance Customized Video Generation via Test-Time Training  [[PDF](https://arxiv.org/abs/2412.15646),[Page](https://github.com/RongPiKing/CustomTTT)] ![Code](https://img.shields.io/github/stars/RongPiKing/CustomTTT?style=social&label=Star)

[arxiv 2025.01] VideoMaker: Zero-shot Customized Video Generation with the Inherent Force of Video Diffusion Models  [[PDF](https://arxiv.org/abs/2412.19645),[Page](https://wutao-cs.github.io/VideoMaker/)] 

[arxiv 2025.01] Magic Mirror: ID-Preserved Video Generation in Video Diffusion Transformers  [[PDF](https://arxiv.org/abs/2501.03931),[Page](https://julianjuaner.github.io/projects/MagicMirror/)] ![Code](https://img.shields.io/github/stars/dvlab-research/MagicMirror/?style=social&label=Star)

[arxiv 2025.01] ConceptMaster: Multi-Concept Video Customization on Diffusion Transformer Models Without Test-Time Tuning  [[PDF](https://arxiv.org/abs/2501.04698),[Page](https://yuzhou914.github.io/ConceptMaster/)] 

[arxiv 2025.01] Multi-subject Open-set Personalization in Video Generation  [[PDF](https://arxiv.org/abs/2501.06187),[Page](https://snap-research.github.io/open-set-video-personalization/)] 

[arxiv 2025.01] EchoVideo: Identity-Preserving Human Video Generation by Multimodal Feature Fusion  [[PDF](https://arxiv.org/abs/2501.13452)]

[arxiv 2025.02] Movie Weaver: Tuning-Free Multi-Concept Video Personalization with Anchored Prompts  [[PDF](https://jeff-liangf.github.io/projects/movieweaver/Movie_Weaver.pdf),[Page](https://jeff-liangf.github.io/projects/movieweaver/)] 

[arxiv 2025.02] Phantom: Subject-consistent video generation via cross-modal alignment  [[PDF](https://phantom-video.github.io/Phantom/),[Page](https://github.com/Phantom-video/Phantom)] ![Code](https://img.shields.io/github/stars/Phantom-video/Phantom?style=social&label=Star)

[arxiv 2025.02] FantasyID: Face Knowledge Enhanced ID-Preserving Video Generation  [[PDF](https://arxiv.org/abs/2502.13995),[Page](https://fantasy-amap.github.io/fantasy-id/)] ![Code](https://img.shields.io/github/stars/Fantasy-AMAP/fantasy-id?style=social&label=Star)

[arxiv 2025.03] CINEMA: Coherent Multi-Subject Video Generation via MLLM-Based Guidance  [[PDF](https://arxiv.org/pdf/2503.10391),[Page](https://ml-gsai.github.io/Concat-ID-demo/)] ![Code](https://img.shields.io/github/stars/ML-GSAI/Concat-ID?style=social&label=Star)

[arxiv 2025.03] Concat-ID: Towards Universal Identity-Preserving Video Synthesis  [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)

## relation 
[arxiv 2025.03]  DreamRelation: Relation-Centric Video Customization [[PDF](https://arxiv.org/abs/2503.07602),[Page](https://dreamrelation.github.io/)] 


[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)



## Talking Face 
[arxiv 2024.02]EMO Emote Portrait Alive: Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions [[PDF](https://arxiv.org/abs/2402.17485),[Page](https://humanaigc.github.io/emote-portrait-alive/)]

[arxiv 2024.04] VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time [[PDF](https://arxiv.org/abs/2404.10667),[Page](https://www.microsoft.com/en-us/research/project/vasa-1/)]

[arxiv 2024.04]MuseTalk: Real-Time High Quality Lip Synchronization with Latent Space Inpainting[[PDF](),[Page](https://github.com/TMElyralab/MuseTalk)]

[arxiv 2024.06]V-Express: Conditional Dropout for Progressive Training of Portrait Video Generation[[PDF](https://arxiv.org/abs/2406.01900),[Page](https://github.com/tencent-ailab/V-Express)]

[arxiv 2024.06]Follow-Your-Emoji: Fine-Controllable and Expressive Freestyle Portrait Animation[[PDF](),[Page](https://follow-your-emoji.github.io/)]

[arxiv 2024.06] X-Portrait: Expressive Portrait Animation with Hierarchical Motion Attention [[PDF](https://arxiv.org/abs/2403.15931),[Page](https://github.com/bytedance/X-Portrait)]


[arxiv 2024.09] CyberHost: Taming Audio-driven Avatar Diffusion Model with Region Codebook Attention[[PDF](https://arxiv.org/pdf/2409.01876),[Page](https://cyberhost.github.io/)]

[arxiv 2024.09] SVP: Style-Enhanced Vivid Portrait Talking Head Diffusion Model [[PDF](https://arxiv.org/abs/2409.03270)]

[arxiv 2024.09] Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency [[PDF](https://arxiv.org/pdf/2409.02634),[Page](https://loopyavatar.github.io/)]

[arxiv 2024.09] DiffTED: One-shot Audio-driven TED Talk Video Generation with Diffusion-based Co-speech Gestures [[PDF](https://arxiv.org/abs/2409.07649)]

[arxiv 2024.09] Stable Video Portraits [[PDF](https://arxiv.org/abs/2409.18083),[Page](https://svp.is.tue.mpg.de/)]

[arxiv 2024.09] Portrait Video Editing Empowered by Multimodal Generative Priors [[PDF](https://arxiv.org/abs/2409.13591),[Page](https://ustc3dv.github.io/PortraitGen/)]


[arxiv 2024.10] Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation [[PDF](https://arxiv.org/abs/2410.07718),[Page]()]

[arxiv 2024.10] MuseTalk: Real-Time High Quality Lip Synchronization with Latent Space Inpainting [[PDF](https://arxiv.org/abs/2410.10122),[Page](https://github.com/TMElyralab/MuseTalk)]

[arxiv 2024.10] DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework for Talking Head Video Generation [[PDF](https://hanbo-cheng.github.io/DAWN/),[Page](https://hanbo-cheng.github.io/DAWN/)]

[arxiv 2024.12] HelloMeme: Integrating Spatial Knitting Attentions to Embed High-Level andFidelity-Rich Conditions in Diffusion Models  [[PDF](),[Page](https://songkey.github.io/hellomeme/)] ![Code](https://img.shields.io/github/stars/HelloVision/HelloMeme?style=social&label=Star)


[arxiv 2024.10] Takin-ADA: Emotion Controllable Audio-Driven Animation with Canonical and Landmark Loss Optimization [[PDF](https://arxiv.org/pdf/2410.14283)]

[arxiv 2024.11] X-Portrait 2: Highly Expressive Portrait Animation [[PDF](),[Page](https://byteaigc.github.io/X-Portrait2/)]

[arxiv 2024.11] EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation[[PDF](https://arxiv.org/abs/2411.10061),[Page](https://antgroup.github.io/ai/echomimic_v2/)]

[arxiv 2024.11]  ConsistentAvatar: Learning to Diffuse Fully Consistent Talking Head Avatar with Temporal Guidance [[PDF](https://arxiv.org/abs/2411.15436)] 

[arxiv 2024.11] LetsTalk: Latent Diffusion Transformer for Talking Video Synthesis  [[PDF](https://arxiv.org/abs/2411.16748),[Page](https://zhang-haojie.github.io/project-pages/letstalk.html)] ![Code](https://img.shields.io/github/stars/zhang-haojie/letstalk?style=social&label=Star)

[arxiv 2024.11] EmotiveTalk: Expressive Talking Head Generation through Audio Information Decoupling and Emotional Video Diffusion  [[PDF](https://arxiv.org/abs/2411.16726)] 

[arxiv 2024.11] Sonic: Shifting Focus to Global Audio Perception in Portrait Animation  [[PDF](https://arxiv.org/pdf/2411.16331),[Page](https://jixiaozhong.github.io/Sonic/)] ![Code](https://img.shields.io/github/stars/jixiaozhong/Sonic?style=social&label=Star)

[arxiv 2024.12] EmojiDiff: Advanced Facial Expression Control with High Identity Preservation in Portrait Generation  [[PDF](https://arxiv.org/abs/2412.01254),[Page](https://emojidiff.github.io/)] 

[arxiv 2024.12]  FLOAT Generative Motion Latent Flow Matching for Audio-driven Talking Portrait [[PDF](https://arxiv.org/abs/2412.01064),[Page](https://deepbrainai-research.github.io/float/)] 

[arxiv 2024.12]  Hallo3: Highly Dynamic and Realistic Portrait Image Animation with Diffusion Transformer Networks [[PDF](https://arxiv.org/pdf/2412.00733),[Page](https://github.com/fudan-generative-vision/hallo3)] ![Code](https://img.shields.io/github/stars/fudan-generative-vision/hallo3?style=social&label=Star)

[arxiv 2024.12] MEMO: Memory-Guided Diffusion for Expressive Talking Video Generation  [[PDF](https://arxiv.org/abs/2412.04448),[Page](https://memoavatar.github.io/)] ![Code](https://img.shields.io/github/stars/memoavatar/memo?style=social&label=Star)

[arxiv 2024.12] PortraitTalk: Towards Customizable One-Shot Audio-to-Talking Face Generation  [[PDF](https://arxiv.org/abs/2412.07754)]

[arxiv 2024.12]  LatentSync: Audio Conditioned Latent Diffusion Models for Lip Sync [[PDF](https://arxiv.org/abs/2412.09262),[Page](https://github.com/bytedance/LatentSync)] ![Code](https://img.shields.io/github/stars/bytedance/LatentSync?style=social&label=Star)

[arxiv 2024.12] CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models  [[PDF](https://arxiv.org/abs/2412.12093),[Page](https://felixtaubner.github.io/cap4d/)] ![Code](https://img.shields.io/github/stars/felixtaubner/cap4d/?style=social&label=Star)

[arxiv 2024.12]  VividFace: A Diffusion-Based Hybrid Framework for High-Fidelity Video Face Swapping [[PDF](https://arxiv.org/abs/2403.16999),[Page](https://hao-shao.com/projects/vividface.html)] ![Code](https://img.shields.io/github/stars/deepcs233/VividFace?style=social&label=Star)

[arxiv 2024.12] OSA-LCM: Real-time One-Step Diffusion-based Expressive Portrait Videos Generation  [[PDF](http://arxiv.org/abs/2412.13479),[Page](https://guohanzhong.github.io/osalcm/)] ![Code](https://img.shields.io/github/stars/Guohanzhong/OSA-LCM?style=social&label=Star)

[arxiv 2024.12] INFP: Audio-Driven Interactive Head Generation in Dyadic Conversations  [[PDF](https://www.arxiv.org/pdf/2412.04037),[Page](https://grisoon.github.io/INFP/)]

[arxiv 2025.01]  JoyGen: Audio-Driven 3D Depth-Aware Talking-Face Video Editing [[PDF](https://arxiv.org/abs/2501.01798),[Page](https://joy-mm.github.io/JoyGen/)] ![Code](https://img.shields.io/github/stars/JOY-MM/JoyGen?style=social&label=Star)

[arxiv 2025.02]  Long-Term TalkingFace Generation via Motion-Prior Conditional Diffusion Model [[PDF](https://arxiv.org/pdf/2502.09533)]

[arxiv 2025.02] SayAnything: Audio-Driven Lip Synchronization with Conditional Video Diffusion  [[PDF](https://arxiv.org/pdf/2502.11515)]

[arxiv 2025.02] SkyReels-A1: Expressive Portrait Animation in Video Diffusion Transformer  [[PDF](https://arxiv.org/abs/2502.10841),[Page](https://skyworkai.github.io/skyreels-a1.github.io/)] ![Code](https://img.shields.io/github/stars/SkyworkAI/SkyReels-A1?style=social&label=Star)

[arxiv 2025.02]  AV-Flow: Transforming Text to Audio-Visual Human-like Interactions [[PDF](https://arxiv.org/abs/2502.13133),[Page](https://aggelinacha.github.io/AV-Flow/)] 

[arxiv 2025.02] InsTaG: Learning Personalized 3D Talking Head from Few-Second Video  [[PDF](https://arxiv.org/abs/2502.20387),[Page](https://fictionarry.github.io/InsTaG/)] ![Code](https://img.shields.io/github/stars/Fictionarry/InsTaG?style=social&label=Star)

[arxiv 2025.02] ARTalk: Speech-Driven 3D Head Animation via Autoregressive Model  [[PDF](https://arxiv.org/abs/2502.20323),[Page](https://xg-chu.site/project_artalk/)] 

[arxiv 2025.02]  High-Fidelity Relightable Monocular Portrait Animation with Lighting-Controllable Video Diffusion Model [[PDF](https://arxiv.org/pdf/2502.19894)]

[arxiv 2025.03] KeyFace: Expressive Audio-Driven Facial Animation for Long Sequences via KeyFrame Interpolation  [[PDF](https://arxiv.org/abs/2503.01715)]

[arxiv 2025.03]  RASA: Replace Anyone, Say Anything – A Training-Free Framework for Audio-Driven and Universal Portrait Video Editing [[PDF](https://arxiv.org/abs/2503.11571),[Page](https://alice01010101.github.io/RASA/)]

[arxiv 2025.03]  PC-Talk: Precise Facial Animation Control for Audio-Driven Talking Face Generation [[PDF](https://arxiv.org/abs/2503.14295),[Page](https://bq-wang0511.github.io/PC-Talk/)] 

[arxiv 2025.03] Cafe-Talk: Generating 3D Talking Face Animation with Multimodal Coarse- and Fine-grained Control  [[PDF](https://arxiv.org/abs/2503.14517),[Page](https://harryxd2018.github.io/cafe-talk/)] 

[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## Talking Body
[arxiv 2024.09] CyberHost: Taming Audio-driven Avatar Diffusion Model with Region Codebook Attention [[PDF](https://arxiv.org/pdf/2409.01876),[Page](https://cyberhost.github.io/)]

[arxiv 2025.01] EMO2: End-Effector Guided Audio-Driven Avatar Video Generation  [[PDF](https://arxiv.org/abs/2501.10687),[Page](https://humanaigc.github.io/emote-portrait-alive-2/)] 

[arxiv 2025.02]  OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models [[PDF](http://arxiv.org/abs/2502.01061),[Page](https://omnihuman-lab.github.io/)] 

[arxiv 2025.03] Versatile Multimodal Controls for Whole-Body Talking Human Animation  [[PDF](https://arxiv.org/pdf/2503.08714)]



[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## Face swapping 



[arxiv 2024.12] HiFiVFS: High Fidelity Video Face Swapping  [[PDF](https://arxiv.org/abs/2411.18293),[Page](https://cxcx1996.github.io/HiFiVFS/)] 

[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)



## Image-to-video Generation 
[arxiv 2023.09]VideoGen: A Reference-Guided Latent Diffusion Approach for High Definition Text-to-Video Generation [[PDF](https://arxiv.org/abs/2309.00398)]

[arxiv 2023.09]Generative Image Dynamics [[PDF](https://arxiv.org/abs/2309.07906),[Page](http://generative-dynamics.github.io/)]

[arxiv 2023.10]DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors [[PDF](https://arxiv.org/abs/2310.12190), [Page](https://github.com/AILab-CVC/VideoCrafter)]

[arxiv 2023.11]SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction [[PDF](https://arxiv.org/abs/2310.20700),[Page](https://vchitect.github.io/SEINE-project/)]

[arxiv 2023.11]I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models
[[PDF](https://arxiv.org/abs/2311.04145),[Page](https://i2vgen-xl.github.io/page04.html)]

[arxiv 2023.11]Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning [[PDF](https://arxiv.org/abs/2311.10709),[Page](https://emu-video.metademolab.com/)]

[arxiv 2023.11]MoVideo: Motion-Aware Video Generation with Diffusion Models[[PDF](https://github.com/JingyunLiang/MoVideo/releases/download/v0.0/MoVideo.pdf),[Page](https://jingyunliang.github.io/MoVideo/)]

[arxiv 2023.11]Make Pixels Dance: High-Dynamic Video Generation[[PDF](),[Page](https://makepixelsdance.github.io/)]

[arxiv 2023.11]Decouple Content and Motion for Conditional Image-to-Video Generation [[PDF](https://arxiv.org/abs/2311.14294)]

[arxiv 2023.12]ART•V: Auto-Regressive Text-to-Video Generation with Diffusion Models [[PDF](https://arxiv.org/abs/2311.18834), [Page](https://warranweng.github.io/art.v)]

[arxiv 2023.12]MicroCinema: A Divide-and-Conquer Approach for Text-to-Video Generation [[PDF](https://arxiv.org/abs/2311.18829), [Page](https://wangyanhui666.github.io/MicroCinema.github.io/)]

[arxiv 2023.12]DreamVideo: High-Fidelity Image-to-Video Generation with Image Retention and Text Guidance [[PDF](https://arxiv.org/abs/2312.03018),[Page](https://anonymous0769.github.io/DreamVideo/)]

[arxiv 2023.12]LivePhoto: Real Image Animation with Text-guided Motion Control [[PDF](https://arxiv.org/abs/2312.02928), [Page](https://xavierchen34.github.io/LivePhoto-Page/)]

[arxiv 2023.12]I2V-Adapter: A General Image-to-Video Adapter for Video Diffusion Models [[PDF](https://arxiv.org/abs/2312.16693)]

[arxiv 2023.11] Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning [[PDF](https://arxiv.org/abs/2311.10709),[Page](https://emu-video.metademolab.com/)]

[arxiv 2024.01]UniVG: Towards UNIfied-modal Video Generation [[PDF](https://arxiv.org/abs/2401.09084),[Page](https://univg-baidu.github.io/)]

[arxiv 2024.03]Tuning-Free Noise Rectification for High Fidelity Image-to-Video Generation [[PDF](https://arxiv.org/abs/2403.02827),[Page](https://noise-rectification.github.io/)]

[arxiv 2024.03]AtomoVideo: High Fidelity Image-to-Video Generation [[PDF](https://arxiv.org/abs/2403.01800),[Page](https://atomo-video.github.io/)]

[arxiv 2024.03]Pix2Gif: Motion-Guided Diffusion for GIF Generation[[PDF](https://arxiv.org/abs/2403.04634),[Page](https://hiteshk03.github.io/Pix2Gif/)]

[arxiv 2024.03]Follow-Your-Click: Open-domain Regional Image Animation via Short Prompts [[PDF](https://arxiv.org/abs/2403.08268),[Page](https://github.com/mayuelala/FollowYourClick)]

[arxiv 2024.03]TimeRewind: Rewinding Time with Image-and-Events Video Diffusion [[PDF](https://arxiv.org/abs/2403.13800),[Page](https://timerewind.github.io/)]

[arxiv 2024.03]TRIP: Temporal Residual Learning with Image Noise Prior for Image-to-Video Diffusion Models [[PDF](https://arxiv.org/abs/2403.17005),[Page](https://trip-i2v.github.io/TRIP/)]

[arxiv 2024.04]LASER: Tuning-Free LLM-Driven Attention Control for Efficient Text-conditioned Image-to-Animation [[PDF](https://arxiv.org/abs/2404.13558)]

[arxiv 2024.04]TI2V-Zero: Zero-Shot Image Conditioning for Text-to-Video Diffusion Models [[PDF](https://arxiv.org/abs/2404.16306),[Page](https://merl.com/research/highlights/TI2V-Zero)]

[arxiv 2024.06] I4VGen: Image as Stepping Stone for Text-to-Video Generation[[PDF](https://arxiv.org/abs/2406.02230),[Page](https://xiefan-guo.github.io/i4vgen/)]

[arxiv 2024.06] AID: Adapting Image2Video Diffusion Models for Instruction-based Video Prediction[[PDF](https://arxiv.org/abs/2406.06465),[Page](https://chenhsing.github.io/AID/)]

[arxiv 2024.06] Identifying and Solving Conditional Image Leakage in Image-to-Video Generation[[PDF](https://arxiv.org/pdf/2406.15735),[Page](https://cond-image-leak.github.io/)]

[arxiv 2024.07]Cinemo: Consistent and Controllable Image Animation with Motion Diffusion Models [[PDF](https://arxiv.org/abs/2407.15642),[Page](https://maxin-cn.github.io/cinemo_project/)]

[arxiv 2024.09] PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation [[PDF](https://arxiv.org/abs/2409.18964),[Page](https://stevenlsw.github.io/physgen/)]

[arxiv 2024.10] FrameBridge: Improving Image-to-Video Generation with Bridge Models [[PDF](https://arxiv.org/abs/2410.15371),[Page](https://framebridge-demo.github.io/)]

[arxiv 2025.01] Through-The-Mask: Mask-based Motion Trajectories for Image-to-Video Generation [[PDF](https://arxiv.org/abs/1234.56789),[Page](https://guyyariv.github.io/TTM/)] 

[arxiv 2025.02] MotionAgent: Fine-grained Controllable Video Generation via Motion Field Agent  [[PDF](https://arxiv.org/pdf/2502.03207)]



[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## 4D generation 
[arxiv 2023.11]Animate124: Animating One Image to 4D Dynamic Scene [[PDF](https://arxiv.org/abs/2311.14603),[Page](https://animate124.github.io/)]

[arxiv 2023.12]4D-fy: Text-to-4D Generation Using Hybrid Score Distillation Sampling[[PDF](https://arxiv.org/abs/2311.17984), [Page](https://sherwinbahmani.github.io/4dfy)]

[arxiv 2023.12]4DGen: Grounded 4D Content Generation with Spatial-temporal Consistency [[PDF](https://arxiv.org/abs/2312.17225),[Page](https://vita-group.github.io/4DGen/)]

[arxiv 2023.12]DreamGaussian4D: Generative 4D Gaussian Splatting [[PDF](https://arxiv.org/abs/2312.17142), [Page](https://jiawei-ren.github.io/projects/dreamgaussian4d)]

[arxiv 2024.10] AvatarGO: Zero-shot 4D Human-Object Interaction Generation and Animation [[PDF](https://yukangcao.github.io/AvatarGO/),[Page](https://yukangcao.github.io/AvatarGO/)]

[arxiv 2024.10] Trans4D: Realistic Geometry-Aware Transition for Compositional Text-to-4D Synthesis [[PDF](https://arxiv.org/abs/2410.07155),[Page](https://github.com/YangLing0818/Trans4D)]

[arxiv 2024.11] DimensionX: Create Any 3D and 4D Scenes from a Single Image with Controllable Video Diffusion [[PDF](https://arxiv.org/abs/2411.04928),[Page](https://chenshuo20.github.io/DimensionX/)]

[arxiv 2024.12] Diffusion Self-Distillation for Zero-Shot Customized Image Generation  [[PDF](https://arxiv.org/abs/2411.18613),[Page](https://cat-4d.github.io/)] 

[arxiv 2024.12] PaintScene4D: Consistent 4D Scene Generation from Text Prompts  [[PDF](https://arxiv.org/abs/2412.04471),[Page](https://paintscene4d.github.io/)] ![Code](https://img.shields.io/github/stars/paintscene4d/paintscene4d.github.io?style=social&label=Star)

[arxiv 2024.12]  4Real-Video: Learning Generalizable Photo-Realistic 4D Video Diffusion [[PDF](https://arxiv.org/abs/2412.04462),[Page](https://snap-research.github.io/4Real-Video/)] 

[arxiv 2024.12]  Birth and Death of a Rose [[PDF](https://arxiv.org/abs/2412.05278),[Page](https://chen-geng.com/rose4d)] 

[arxiv 2024.12]  DNF: Unconditional 4D Generation with Dictionary-based Neural Fields [[PDF](https://arxiv.org/abs/2412.05161),[Page](https://xzhang-t.github.io/project/DNF/)] 

[arxiv 2025.01] AR4D: Autoregressive 4D Generation from Monocular Videos [[PDF](https://arxiv.org/abs/2501.01722),[Page](https://hanxinzhu-lab.github.io/AR4D/)] 

[arxiv 2025.02] MVTokenFlow: High-quality 4D Content Generation using Multiview Token Flow  [[PDF](https://arxiv.org/abs/2502.11697),[Page](https://soolab.github.io/MVTokenFlow)] ![Code](https://img.shields.io/github/stars/SooLab/MVTokenFlow?style=social&label=Star)

[arxiv 2025.03]  SV4D 2.0: Enhancing Spatio-Temporal Consistency in Multi-View Video Diffusion for High-Quality 4D Generation [[PDF](https://arxiv.org/pdf/2503.16396)]


[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## Audio-to-video Generation
[arxiv 2023.09]Diverse and Aligned Audio-to-Video Generation via Text-to-Video Model Adaptation [[PDF](https://arxiv.org/abs/2309.16429)]

[arxiv 2024.02]Seeing and Hearing Open-domain Visual-Audio Generation with Diffusion Latent Aligners [[PDF](https://arxiv.org/abs/2402.17723),[Page](https://yzxing87.github.io/Seeing-and-Hearing/)]

[arxiv 2024.04]TAVGBench: Benchmarking Text to Audible-Video Generation [[PDF](https://arxiv.org/abs/2404.14381),[Page](https://github.com/OpenNLPLab/TAVGBench)]

[arxiv 2024.09] Draw an Audio: Leveraging Multi-Instruction for Video-to-Audio Synthesis [[PDF](https://arxiv.org/abs/2409.06135),[Page](https://yannqi.github.io/Draw-an-Audio/)]

[arxiv 2024.11] Tell What You Hear From What You See -- Video to Audio Generation Through Text [[PDF](https://arxiv.org/abs/2411.05679),[Page](https://github.com/Christina200/Online-LoRA-official)]

[arxiv 2024.12]  AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal Audio-Video Generation [[PDF](https://arxiv.org/abs/2412.15191),[Page](https://snap-research.github.io/AVLink/)] ![Code](https://img.shields.io/github/stars/snap-research/AVLink?style=social&label=Star)

[arxiv 2024.12] Every Image Listens, Every Image Dances: Music-Driven Image Animation  [[PDF](https://arxiv.org/html/2501.18801v1)]

[arxiv 2025.02]  AGAV-Rater: Enhancing LMM for AI-Generated Audio-Visual Quality Assessment [[PDF](https://arxiv.org/abs/2501.18314),[Page](https://agav-rater.github.io/)] 

[arxiv 2025.02] UniForm: A Unified Diffusion Transformer for Audio-Video Generation  [[PDF](https://arxiv.org/abs/2502.03897),[Page](https://uniform-t2av.github.io/)] 

[arxiv 2025.03]  MusicInfuser: Making Video Diffusion Listen and Dance [[PDF](https://arxiv.org/abs/2503.14505),[Page](https://susunghong.github.io/MusicInfuser/)] ![Code](https://img.shields.io/github/stars/SusungHong/MusicInfuser?style=social&label=Star)


[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)

## audio generation 

[arxiv 2025.03] AudioX: Diffusion Transformer for Anything-to-Audio Generation  [[PDF](https://arxiv.org/abs/2503.10522/),[Page](https://zeyuet.github.io/AudioX/)] ![Code](https://img.shields.io/github/stars/ZeyueT/AudioX?style=social&label=Star)



[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## unified editing and generation
[arxiv 2025.03] VACE: All-in-One Video Creation and Editing  [[PDF](https://arxiv.org/pdf/2503.07598),[Page](https://ali-vilab.github.io/VACE-Page/)] 

[arxiv 2025.03] VEGGIE: Instructional Editing and Reasoning Video Concepts with Grounded Generation  [[PDF](VEGGIE: Instructional Editing and Reasoning of Video Concepts with Grounded Generation),[Page](https://veggie-gen.github.io/)] ![Code](https://img.shields.io/github/stars/Yui010206/VEGGIE-VidEdit/?style=social&label=Star)


[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## editing with video models 
[arxiv 2023.12]VIDiff: Translating Videos via Multi-Modal Instructions with Diffusion Models[[PDF](https://arxiv.org/abs/2311.18837),[Page](https://chenhsing.github.io/VIDiff)]

[arxiv 2023.12]Neutral Editing Framework for Diffusion-based Video Editing [[PDF](https://arxiv.org/abs/2312.06708),[Page](https://neuedit.github.io/)]

[arxiv 2024.01]FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis[[PDF](https://arxiv.org/abs/2312.17681),[Page](https://jeff-liangf.github.io/projects/flowvid/)]

[arxiv 2024.02]UniEdit: A Unified Tuning-Free Framework for Video Motion and Appearance Editing [[PDF](https://arxiv.org/abs/2402.13185),[Page](https://jianhongbai.github.io/UniEdit/)]

[arxiv 2024.02]Customize-A-Video: One-Shot Motion Customization of Text-to-Video Diffusion Models [[PDF](https://arxiv.org/abs/2402.14780),[Page](https://anonymous-314.github.io/)]

[arxiv 2024.03]FastVideoEdit: Leveraging Consistency Models for Efficient Text-to-Video Editing[[PDF](https://arxiv.org/abs/2403.06269)]

[arxiv 2024.03]DreamMotion: Space-Time Self-Similarity Score Distillation for Zero-Shot Video Editing [[PDF](https://arxiv.org/abs/2403.12002),[Page](https://hyeonho99.github.io/dreammotion/)]

[arxiv 2024.03]EffiVED:Efficient Video Editing via Text-instruction Diffusion Models [[PDF](https://arxiv.org/abs/2403.11568)]

[arxiv 2024.03]Videoshop: Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion [[PDF](https://arxiv.org/abs/2403.14617),[Page](https://videoshop-editing.github.io/)]

[arxiv 2024.03]AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks  [[PDF](https://arxiv.org/pdf/2403.14468.pdf),[Page](https://tiger-ai-lab.github.io/AnyV2V/)]

[arxiv 2024.04]Investigating the Effectiveness of Cross-Attention to Unlock Zero-Shot Editing of Text-to-Video Diffusion Models [[PDF](https://arxiv.org/abs/2404.05519)]

[arxiv 2024.05]I2VEdit: First-Frame-Guided Video Editing via Image-to-Video Diffusion Models[[PDF](https://arxiv.org/abs/2405.16537),[Page](https://i2vedit.github.io/)]

[arxiv 2024.05] Streaming Video Diffusion: Online Video Editing with Diffusion Models[[PDF](https://arxiv.org/abs/2405.1972),[Page](https://github.com/Chenfeng1271/SVDiff)]

[arxiv 2024.06]Zero-Shot Video Editing through Adaptive Sliding Score Distillation[[PDF](https://arxiv.org/abs/2406.04888),[Page](https://nips24videoedit.github.io/zeroshot_videoedit/)]

[arxiv 2024.06]FRAG: Frequency Adapting Group for Diffusion Video Editing[[PDF](https://arxiv.org/abs/2406.06044)]

[arxiv 2024.07] Fine-gained Zero-shot Video Sampling[[PDF](https://arxiv.org/pdf/2407.21475),[Page](https://densechen.github.io/zss/)]

[arxiv 2024.09] DNI: Dilutional Noise Initialization for Diffusion Video Editing [[PDF](https://arxiv.org/abs/2409.13037)]

[arxiv 2024.10]FreeMask: Rethinking the Importance of Attention Masks for Zero-Shot Video Editing[[PDF](https://arxiv.org/abs/2409.20500),[Page](https://freemask-edit.github.io/)]

[arxiv 2024.11] StableV2V: Stablizing Shape Consistency in Video-to-Video Editing [[PDF](https://arxiv.org/abs/2411.11045),[Page](https://alonzoleeeooo.github.io/StableV2V)]

[arxiv 2024.11] VIRES: Video Instance Repainting with Sketch and Text Guidance  [[PDF](https://arxiv.org/abs/2411.16199)]

[arxiv 2024.11] VideoDirector: Precise Video Editing via Text-to-Video Models  [[PDF](https://arxiv.org/abs/2411.17592),[Page](https://anonymous.4open.science/w/c4KzqAbCaz89o0FeWkdya/)] 

[arxiv 2024.12] MoViE: Mobile Diffusion for Video Editing  [[PDF](https://arxiv.org/abs/2412.06578)]

[arxiv 2024.12] Re-Attentional Controllable Video Diffusion Editing  [[PDF](https://arxiv.org/abs/2412.11710),[Page](https://github.com/mdswyz/ReAtCo)] ![Code](https://img.shields.io/github/stars/mdswyz/ReAtCo?style=social&label=Star)

[arxiv 2024.12] MIVE: New Design and Benchmark for Multi-Instance Video Editing  [[PDF](https://arxiv.org/abs/2412.12877),[Page](https://kaist-viclab.github.io/mive-site/)]

[arxiv 2024.12] AniGS: Animatable Gaussian Avatar from a Single Image with Inconsistent Gaussian Reconstruction  [[PDF](https://arxiv.org/pdf/2412.02684),[Page](https://lingtengqiu.github.io/2024/AniGS/)] ![Code](https://img.shields.io/github/stars/aigc3d/AniGS?style=social&label=Star)

[arxiv 2025.01] Generative Video Propagation  [[PDF](https://arxiv.org/abs/2412.19761),[Page](https://genprop.github.io//)]

[arxiv 2025.02]  DynVFX: Augmenting Real Videos with Dynamic Content [[PDF](https://arxiv.org/abs/2502.03621),[Page](https://dynvfx.github.io/)] 

[arxiv 2025.02]  Animate Anyone 2: High-Fidelity Character Image Animation with Environment Affordance [[PDF](https://arxiv.org/abs/2502.06145),[Page](https://humanaigc.github.io/animate-anyone-2/)] 

[arxiv 2025.02] VFX Creator: Animated Visual Effect Generation with Controllable Diffusion Transformer  [[PDF](https://arxiv.org/abs/2502.05979),[Page](https://vfx-creator0.github.io/)] 

[arxiv 2025.02]  AdaFlow: Efficient Long Video Editing via Adaptive Attention Slimming And Keyframe Selection [[PDF](https://arxiv.org/abs/2502.05433),[Page](https://github.com/jidantang55/AdaFlow)] ![Code](https://img.shields.io/github/stars/jidantang55/AdaFlow?style=social&label=Star)

[arxiv 2025.02]  VideoGrain: Modulating Space-Time Attention for Multi-Grained Video Editing [[PDF](https://arxiv.org/abs/2502.17258),[Page](https://knightyxp.github.io/VideoGrain_project_page/)] ![Code](https://img.shields.io/github/stars/knightyxp/VideoGrain?style=social&label=Star)

[arxiv 2025.03] SwapAnyone: Consistent and Realistic Video Synthesis for Swapping Any Person into Any Video  [[PDF](https://arxiv.org/abs/2503.09154),[Page](https://github.com/PKU-YuanGroup/SwapAnyone)] ![Code](https://img.shields.io/github/stars/PKU-YuanGroup/SwapAnyone?style=social&label=Star)


[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## Editing with image model 
*[arxiv 2022.12]Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation [[PDF](https://arxiv.org/abs/2212.11565), [Page](https://tuneavideo.github.io/)]

[arxiv 2023.03]Video-P2P: Video Editing with Cross-attention Control [[PDF](https://arxiv.org/abs/2303.04761), [Page](https://video-p2p.github.io/)]

[arxiv 2023.03]Edit-A-Video: Single Video Editing with Object-Aware Consistency [[PDF](https://arxiv.org/abs/2303.07945), [Page](https://edit-a-video.github.io/)]

[arxiv 2023.03]FateZero: Fusing Attentions for Zero-shot Text-based Video Editing [[PDF](https://arxiv.org/abs/2303.09535), [Page](https://github.com/ChenyangQiQi/FateZero)]

[arxiv 2023.03]Pix2Video: Video Editing using Image Diffusion [[PDF](https://arxiv.org/abs/2303.12688)]

->[arxiv 2023.03]Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators [[PDF](https://arxiv.org/abs/2303.13439), [code](https://github.com/Picsart-AI-Research/Text2Video-Zero)]

[arxiv 2023.03]Zero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models[[PDF](https://arxiv.org/abs/2303.17599),[code](https://github.com/baaivision/vid2vid-zero)]

[arxiv 2023.04]Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free Videos[[PDF](https://arxiv.org/abs/2304.01186)]

[arxiv 2023.05]ControlVideo: Training-free Controllable Text-to-Video Generation [[PDF](https://arxiv.org/abs/2305.13077), [Page](https://github.com/YBYBZhang/ControlVideo)]

[arxiv 2023.05]Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models[[PDF](https://arxiv.org/abs/2305.13840), [Page](https://controlavideo.github.io/)]

[arxiv-2023.05]Large Language Models are Frame-level Directors for Zero-shot Text-to-Video Generation [[PDF](https://arxiv.org/abs/2305.14330), [Page](https://github.com/KU-CVLAB/DirecT2V)]

[arxiv 2023.05]Video ControlNet: Towards Temporally Consistent Synthetic-to-Real Video Translation Using Conditional Image Diffusion Models [[PDF](https://arxiv.org/abs/2305.19193)]

[arxiv 2023.05]SAVE: Spectral-Shift-Aware Adaptation of Image Diffusion Models for Text-guided Video Editing [[PDF](https://arxiv.org/abs/2305.18670)]

[arxiv 2023.05]InstructVid2Vid: Controllable Video Editing with Natural Language Instructions [[PDF](https://arxiv.org/abs/2305.12328)]

[arxiv 2023.05] ControlVideo: Adding Conditional Control for One Shot Text-to-Video Editing [[PDF](https://arxiv.org/pdf/2305.17098.pdf), [Page](https://ml.cs.tsinghua.edu.cn/controlvideo/)]

[arxiv 2023.05]Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising [[PDF](https://arxiv.org/abs/2305.18264),[Page](https://g-u-n.github.io/projects/gen-long-video/index.html)]

[arxiv 2023.06]Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance [[PDF](https://arxiv.org/abs/2306.00943), [Page](https://doubiiu.github.io/projects/Make-Your-Video/)]

[arxiv 2023.06]VidEdit: Zero-Shot and Spatially Aware Text-Driven Video Editing [[PDF](https://arxiv.org/abs/2306.08707),[Page](https://videdit.github.io/)]

*[arxiv 2023.06]Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation [[PDF](https://arxiv.org/abs/2306.07954), [Page](https://anonymous-31415926.github.io/)]

*[arxiv 2023.07]AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning [[PDF](https://arxiv.org/abs/2307.04725),  [Page](https://animatediff.github.io/)]

*[arxiv 2023.07]TokenFlow: Consistent Diffusion Features for Consistent Video Editing [[PDF](https://arxiv.org/pdf/2307.10373.pdf),[Page](https://diffusion-tokenflow.github.io/)]

[arxiv 2023.07]VideoControlNet: A Motion-Guided Video-to-Video Translation Framework by Using Diffusion Model with ControlNet [[PDF](https://arxiv.org/pdf/2307.14073.pdf), [Page](https://vcg-aigc.github.io/)]

[arxiv 2023.08]CoDeF: Content Deformation Fields for Temporally Consistent Video Processing [[PDF](https://arxiv.org/pdf/2308.07926.pdf), [Page](https://qiuyu96.github.io/CoDeF/)]

[arxiv 2023.08]DragNUWA: Fine-grained Control in Video Generation by Integrating Text, Image, and Trajectory [[PDF](https://arxiv.org/abs/2308.08089), [Page](https://www.microsoft.com/en-us/research/project/dragnuwa/)]

[arxiv 2023.08]StableVideo: Text-driven Consistency-aware Diffusion Video Editing [[PDF](https://arxiv.org/abs/2308.09592), [Page](https://github.com/rese1f/StableVideo)]

[arxiv 2023.08]Edit Temporal-Consistent Videos with Image Diffusion Model [[PDF](https://arxiv.org/abs/2308.09091)]

[arxiv 2023.08]EVE: Efficient zero-shot text-based Video Editing with Depth Map Guidance and Temporal Consistency Constraints [[PDF](https://arxiv.org/pdf/2308.10648.pdf)]

[arxiv 2023.08]MagicEdit: High-Fidelity and Temporally Coherent Video Editing [[PDF](https://arxiv.org/pdf/2308.14749), [Page](https://magic-edit.github.io/)]

[arxiv 2023.09]MagicProp: Diffusion-based Video Editing via Motionaware Appearance Propagation[[PDF](https://arxiv.org/pdf/2309.00908.pdf)]

[arxiv 2023.09]Free-Bloom: Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator[[PDF](https://arxiv.org/abs/2309.14494), [Page](https://github.com/SooLab/Free-Bloom)]

[arxiv 2023.09]CCEdit: Creative and Controllable Video Editing via Diffusion Models [[PDF](https://arxiv.org/abs/2309.16496)]

[arxiv 2023.10]Ground-A-Video: Zero-shot Grounded Video Editing using Text-to-image Diffusion Models [[PDF](https://arxiv.org/abs/2310.01107),[Page](https://ground-a-video.github.io/)]

[arxiv 2023.10]FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video editing [[PDF](https://arxiv.org/abs/2310.05922),[Page](https://flatten-video-editing.github.io/)]

[arxiv 2023.10]ConditionVideo: Training-Free Condition-Guided Text-to-Video Generation [[PDF](https://arxiv.org/abs/2310.07697),[Page](https://pengbo807.github.io/conditionvideo-website/)]

[arxiv 2023.10, nerf] DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and View-Change Human-Centric Video Editing [[PDF](https://arxiv.org/abs/2310.10624), [Page](https://showlab.github.io/DynVideo-E/)]

[arxiv 2023.10]LAMP: Learn A Motion Pattern for Few-Shot-Based Video Generation [[PDF](https://arxiv.org/abs/2310.10769),[Page](https://rq-wu.github.io/projects/LAMP/index.html)]

[arxiv 2023.11]LATENTWARP: CONSISTENT DIFFUSION LATENTS FOR ZERO-SHOT VIDEO-TO-VIDEO TRANSLATION [[PDF](https://arxiv.org/pdf/2311.00353.pdf)]

[arxiv 2023.11]Cut-and-Paste: Subject-Driven Video Editing with Attention Control[[PDF](https://arxiv.org/abs/2311.11697)]

[arxiv 2023.11]MotionZero:Exploiting Motion Priors for Zero-shot Text-to-Video Generation [[PDF](https://arxiv.org/abs/2311.16635)]

[arxiv 2023.12]Motion-Conditioned Image Animation for Video Editing [[PDF](https://arxiv.org/pdf/2311.18827.pdf), [Page](https://facebookresearch.github.io/MoCA/)]

[arxiv 2023.12]RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models [[PDF](https://arxiv.org/abs/2312.04524),[Page](https://rave-video.github.io/)]

[arxiv 2023.12]DiffusionAtlas: High-Fidelity Consistent Diffusion Video Editing [[PDF](https://arxiv.org/abs/2312.03772)]

[arxiv 2023.12]MagicStick: Controllable Video Editing via Control Handle Transformations [[PDF](https://arxiv.org/abs/2312.03047),[Page](https://github.com/mayuelala/MagicStick)]

[arxiv 2023.12]SAVE: Protagonist Diversification with Structure Agnostic Video Editing [[PDF](https://arxiv.org/abs/2312.02503),[Page](https://ldynx.github.io/SAVE/)]

[arxiv 2023.12]VidToMe: Video Token Merging for Zero-Shot Video Editing [[PDF](https://arxiv.org/abs/2312.10656),[Page](https://vidtome-diffusion.github.io/)]

[arxiv 2023.12]Fairy: Fast Parallelized Instruction-Guided Video-to-Video Synthesis [[PDF](https://arxiv.org/abs/2312.13834),[Page](https://fairy-video2video.github.io/)]

[arxiv 2024.1]Object-Centric Diffusion for Efficient Video Editing [[PDF](https://arxiv.org/abs/2401.05735)]

[arxiv 2024.1]VASE: Object-Centric Shape and Appearance Manipulation of Real Videos [[PDF](https://arxiv.org/abs/2401.02473),[Page](https://helia95.github.io/vase-website/)]

[arxiv 2024.03]FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation [[PDF](https://arxiv.org/abs/2403.12962),[Page](https://www.mmlab-ntu.com/project/fresco/)]

[arxiv 2024.04]GenVideo: One-shot Target-image and Shape Aware Video Editing using T2I Diffusion Models [[PDF](https://arxiv.org/abs/2404.12541)]

[arxiv 2024.05]Edit-Your-Motion: Space-Time Diffusion Decoupling Learning for Video Motion Editing [[PDF](https://arxiv.org/abs/2405.04496),[Page](https://github.com/yiiizuo/Edit-Your-Motion)]

[arxiv 2024.05] Slicedit: Zero-Shot Video Editing With Text-to-Image Diffusion Models Using Spatio-Temporal Slices  [[PDF](https://arxiv.org/abs/2405.12211),[Page](https://matankleiner.github.io/slicedit/)]

[arxiv 2024.05] Looking Backward: Streaming Video-to-Video Translation with Feature Banks [[PDF](https://arxiv.org/abs/2405.15757),[Page](https://jeff-liangf.github.io/projects/streamv2v)]

[arxiv 2024.06]Follow-Your-Pose v2: Multiple-Condition Guided Character Image Animation for Stable Pose Control [[PDF](https://arxiv.org/abs/2406.03035)]

[arxiv 2024.06]NaRCan: Natural Refined Canonical Image with Integration of Diffusion Prior for Video Editing[[PDF](https://arxiv.org/abs/2406.06523),[Page](https://koi953215.github.io/NaRCan_page/)]

[arxiv 2024.06]VIA: A Spatiotemporal Video Adaptation Framework for Global and Local Video Editing [[PDF](https://arxiv.org/abs/2406.12831),[Page](https://via-video.github.io/)]

[arxiv 2024.10] L-C4: Language-Based Video Colorization for Creative and Consistent Color [[PDF](https://arxiv.org/abs/2410.04972)] 

[arxiv 2024.10] HARIVO: Harnessing Text-to-Image Models for Video Generation [[PDF](https://kwonminki.github.io/HARIVO/),[Page](https://kwonminki.github.io/HARIVO/)] 

[arxiv 2024.12] DIVE: Taming DINO for Subject-Driven Video Editing  [[PDF](https://arxiv.org/abs/2412.03347),[Page](https://dino-video-editing.github.io/)] 

[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)




## Completion (animation, interpolation, prediction)
[arxiv 2022; Meta] Tell Me What Happened: Unifying Text-guided Video Completion via Multimodal Masked Video Generation \[[PDF](https://arxiv.org/pdf/2211.12824.pdf), code]

[arxiv 2023.03]LDMVFI: Video Frame Interpolation with Latent Diffusion Models[[PDF](https://arxiv.org/abs/2303.09508)]

*[arxiv 2023.03]Seer: Language Instructed Video Prediction with Latent Diffusion Models [[PDF](https://arxiv.org/abs/2303.14897)]


[arxiv 2024.12]  Extracting Motion and Appearance via Inter-Frame Attention for Efficient Video Frame Interpolation
 [[PDF](https://arxiv.org/abs/2303.00440),[Page](https://github.com/MCG-NJU/EMA-VFI?tab=readme-ov-file)] ![Code](https://img.shields.io/github/stars/MCG-NJU/EMA-VFI?style=social&label=Star)



[arxiv 2023.10]DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors [[PDF](https://arxiv.org/abs/2310.12190), [Page](https://github.com/AILab-CVC/VideoCrafter)]


[arxiv 2024.03]Explorative Inbetweening of Time and Space [[PDF](https://time-reversal.github.io/),[Page](https://time-reversal.github.io/)]

[arxiv 2024.04]Video Interpolation With Diffusion Models [[PDF](https://arxiv.org/abs/2404.01203),[Page](https://vidim-interpolation.github.io/)]

[arxiv 2024.04]Sparse Global Matching for Video Frame Interpolation with Large Motion [[PDF](https://arxiv.org/abs/2404.06913),[Page](https://sgm-vfi.github.io/)]

[arxiv 2024.04]LADDER: An Efficient Framework for Video Frame Interpolation [[PDF](https://arxiv.org/abs/2404.11108)]

[arxiv 2024.04]Motion-aware Latent Diffusion Models for Video Frame Interpolation [[PDF](https://arxiv.org/abs/2404.13534)]

[arxiv 2024.04]Event-based Video Frame Interpolation with Edge Guided Motion Refinement [[PDF](https://arxiv.org/abs/2404.18156)]

[arxiv 2024.04]StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation [[PDF](https://arxiv.org/abs/2405.01434),[Page](https://github.com/HVision-NKU/StoryDiffusion)]

[arxiv 2024.04]Frame Interpolation with Consecutive Brownian Bridge Diffusion[[PDF](https://arxiv.org/abs/2405.05953),[Page](https://zonglinl.github.io/videointerp/)]

[arxiv 2024.05]ToonCrafter: Generative Cartoon Interpolation [[PDF](https://arxiv.org/abs/2405.17933),[Page](https://doubiiu.github.io/projects/ToonCrafter/)]

[arxiv 2024.06]Disentangled Motion Modeling for Video Frame Interpolation [[PDF](https://arxiv.org/abs/2406.17256),[Page](https://github.com/JHLew/MoMo)]

[arxiv 2024.07] VFIMamba: Video Frame Interpolation with State Space Models [[PDF](https://arxiv.org/abs/2407.02315),[Page](https://github.com/MCG-NJU/VFIMamba)]

[arxiv 2024.08] Generative Inbetweening: Adapting Image-to-Video Models for Keyframe Interpolation[[PDF](https://arxiv.org/abs/2408.15239),[Page](https://svd-keyframe-interpolation.github.io/)]

[arxiv 2024.10] High-Resolution Frame Interpolation with Patch-based Cascaded Diffusion [[PDF](https://arxiv.org/abs/2410.11838),[Page](https://hifi-diffusion.github.io/)]

[arxiv 2024.10] Framer: Interactive Frame Interpolation [[PDF](https://arxiv.org/abs/2410.18978),[Page](https://aim-uofa.github.io/Framer/)]

[arxiv 2024.12] Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion  [[PDF](https://arxiv.org/pdf/2412.00857)]

[arxiv 2024.12] Elevating Flow-Guided Video Inpainting with Reference Generation  [[PDF](https://arxiv.org/abs/2412.08975),[Page](https://github.com/suhwan-cho/RGVI)] ![Code](https://img.shields.io/github/stars/suhwan-cho/RGVI?style=social&label=Star)

[arxiv 2024.12] Generative Inbetweening through Frame-wise Conditions-Driven Video Generation  [[PDF](https://fcvg-inbetween.github.io/),[Page](https://fcvg-inbetween.github.io/)] ![Code](https://img.shields.io/github/stars/Tian-one/FCVG?style=social&label=Star)

[arxiv 2025.01]  MoG: Motion-Aware Generative Frame Interpolation [[PDF](https://arxiv.org/abs/2501.03699),[Page](https://mcg-nju.github.io/MoG_Web/)] 

[arxiv 2025.02]  Seeing World Dynamics in a Nutshell [[PDF](https://arxiv.org/pdf/2502.03465),[Page](https://github.com/Nut-World/NutWorld)] ![Code](https://img.shields.io/github/stars/Nut-World/NutWorld?style=social&label=Star)


[arxiv 2025.02] Event-based Video Frame Interpolation with Cross-Modal Asymmetric Bidirectional Motion Fields  [[PDF](https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Event-Based_Video_Frame_Interpolation_With_Cross-Modal_Asymmetric_Bidirectional_Motion_Fields_CVPR_2023_paper.pdf),[Page](https://github.com/intelpro/CBMNet)] ![Code](https://img.shields.io/github/stars/intelpro/CBMNet?style=social&label=Star)

[arxiv 2025.03] VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control  [[PDF](https://arxiv.org/abs/2503.05639),[Page](https://yxbian23.github.io/project/video-painter/)] ![Code](https://img.shields.io/github/stars/TencentARC/VideoPainter?style=social&label=Star)

[arxiv 2025.03] MTV-Inpaint: Multi-Task Long Video Inpainting  [[PDF](https://arxiv.org/pdf/2503.11412),[Page](https://mtv-inpaint.github.io/)] ![Code](https://img.shields.io/github/stars/ysy31415/MTV-Inpaint?style=social&label=Star)

[arxiv 2025.03] EDEN: Enhanced Diffusion for High-quality Large-motion Video Frame Interpolation  [[PDF](https://arxiv.org/abs/2503.15831),[Page](https://github.com/bbldCVer/EDEN)] ![Code](https://img.shields.io/github/stars/bbldCVer/EDEN?style=social&label=Star)


[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)







## style transfer 
[arxiv 2023.06]Probabilistic Adaptation of Text-to-Video Models [[PDF](https://arxiv.org/abs/2306.01872)]

[arxiv 2023.11]Highly Detailed and Temporal Consistent Video Stylization via Synchronized Multi-Frame Diffusion[[PDF](https://arxiv.org/abs/2311.14343)]

[arxiv 2023.12]StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter[[PDF](https://arxiv.org/abs/2312.00330),[Page](https://gongyeliu.github.io/StyleCrafter.github.io/)]

[arxiv 2023.12]DragVideo: Interactive Drag-style Video Editing [[PDF](https://arxiv.org/abs/2312.02216)]

[arxiv 2024.03]FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation [[PDF](https://arxiv.org/abs/2403.12962),[Page](https://github.com/williamyang1991/fresco)]

[arxiv 2024.10] UniVST: A Unified Framework for Training-free Localized Video Style Transfer [[PDF](https://arxiv.org/abs/2410.20084)]

[arxiv 2024.12]  StyleMaster: Stylize Your Video with Artistic Generation and Translation [[PDF](https://arxiv.org/abs/2412.07744),[Page](https://zixuan-ye.github.io/stylemaster)] ![Code](https://img.shields.io/github/stars/KwaiVGI/StyleMaster?style=social&label=Star)

[arxiv 2025.03]  SOYO: A Tuning-Free Approach for Video Style Morphing via Style-Adaptive Interpolation in Diffusion Models [[PDF](https://arxiv.org/pdf/2503.06998)]


[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## architecture/distribution
[arxiv 2024.12] Efficient Continuous Video Flow Model for Video Prediction  [[PDF](https://arxiv.org/abs/2412.05633)]

[arxiv 2025.02] Efficient-vDiT: Efficient Video Diffusion Transformers With Attention Tile  [[PDF](https://arxiv.org/abs/2502.06155)]

[arxiv 2025.02] Next Block Prediction: Video Generation via Semi-Autoregressive Modeling  [[PDF](https://arxiv.org/abs/2502.07737),[Page](https://renshuhuai-andy.github.io/NBP-project/)] ![Code](https://img.shields.io/github/stars/RenShuhuai-Andy/NBP?style=social&label=Star)



[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)





## Evaluation 
[arxiv 2023.10]EvalCrafter: Benchmarking and Evaluating Large Video Generation Models [[PDF](https://arxiv.org/abs/2310.11440),[Page](https://evalcrafter.github.io/)]

[arxiv 2023.11]FETV: A Benchmark for Fine-Grained Evaluation of Open-Domain Text-to-Video Generation [[PDF](https://arxiv.org/abs/2311.01813)]

[arxiv 2023.11]Online Video Quality Enhancement with Spatial-Temporal Look-up Tables [[PDF](https://arxiv.org/abs/2311.13616)]


[ICCV 2023]Exploring Video Quality Assessment on User Generated Contents from Aesthetic and Technical Perspectives [[PDF](https://arxiv.org/abs/2211.04894),[Page](https://github.com/VQAssessment/DOVER)]

[arxiv 2023.10]EvalCrafter: Benchmarking and Evaluating Large Video Generation Models[[PDF](https://arxiv.org/abs/2310.11440), [Page](https://arxiv.org/abs/2310.11440)]

[arxiv 2023.11]HIDRO-VQA: High Dynamic Range Oracle for Video Quality Assessment [[PDF](https://arxiv.org/abs/2311.11059)]

[arxiv 2023.12]VBench: Comprehensive Benchmark Suite for Video Generative Models [[PDF](https://arxiv.org/abs/2311.17982), [Page](https://vchitect.github.io/VBench-project/)]

[arxiv 2024.02]Perceptual Video Quality Assessment: A Survey [[PDF](https://arxiv.org/abs/2402.03413)]

[arxiv 2024.02]KVQ: Kaleidoscope Video Quality Assessment for Short-form Videos [[PDf](https://arxiv.org/abs/2402.07220)]

[arxiv 2024.03]STREAM: Spatio-TempoRal Evaluation and Analysis Metric for Video Generative Models [[PDF](https://arxiv.org/abs/2403.09669)]

[arxiv 2024.03]Modular Blind Video Quality Assessment [[PDF](https://arxiv.org/abs/2402.19276)]

[arxiv 2024.03]Subjective-Aligned Dateset and Metric for Text-to-Video Quality Assessment [[PDF](https://arxiv.org/abs/2403.11956)]

[arxiv 2024.06] GenAI Arena: An Open Evaluation Platform for Generative Models[[PDF](https://arxiv.org/abs/2406.04485),[Page](https://huggingface.co/spaces/TIGER-Lab/GenAI-Arena)]

[arxiv 2024.06]VideoPhy: Evaluating Physical Commonsense for Video Generation [[PDF](http://arxiv.org/abs/2406.03520),[Page](https://videophy.github.io/)]

[arxiv 2024.07]T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video Generation [[PDF](https://arxiv.org/abs/2407.14505),[Page](https://t2v-compbench.github.io/)]

[arxiv 2024.07]Fr\'echet Video Motion Distance: A Metric for Evaluating Motion Consistency in Videos [[PDF](https://arxiv.org/abs/2407.16124)]

[arxiv 2024.10] The Dawn of Video Generation: Preliminary Explorations with SORA-like Models [[PDF](https://arxiv.org/abs/2410.05227),[Page](https://ailab-cvc.github.io/VideoGen-Eval/)]

[arxiv 2024.10] Beyond FVD: Enhanced Evaluation Metrics for Video Generation Quality[[PDF](https://arxiv.org/abs/2410.05203),[Page](https://oooolga.github.io/JEDi.github.io/)]

[arxiv 2024.11] ViBe: A Text-to-Video Benchmark for Evaluating Hallucination in Large Multimodal Models [[PDF](https://arxiv.org/abs/2411.10867),[Page](https://vibe-t2v-bench.github.io/)]

[arxiv 2024.11] VBench++: Comprehensive and Versatile Benchmark Suite for Video Generative Models [[PDF](https://arxiv.org/abs/2411.13503),[Page](https://github.com/Vchitect/VBench)]

[arxiv 2024.12]  Is Your World Simulator a Good Story Presenter? A Consecutive Events-Based Benchmark for Future Long Video Generation [[PDF](https://arxiv.org/abs/),[Page](https://ypwang61.github.io/project/StoryEval/)] ![Code](https://img.shields.io/github/stars/ypwang61/StoryEval?style=social&label=Star)

[arxiv 2025.01]  MEt3R: Measuring Multi-View Consistency in Generated Images [[PDF](https://arxiv.org/abs/2501.06336),[Page](https://geometric-rl.mpi-inf.mpg.de/met3r/)] ![Code](https://img.shields.io/github/stars/mohammadasim98/MEt3R?style=social&label=Star)

[arxiv 2025.02]  MJ-VIDEO: Fine-Grained Benchmarking and Rewarding Video Preferences in Video Generation [[PDF](https://arxiv.org/pdf/2502.01719)]

[arxiv 2025.03]  What Are You Doing? A Closer Look at Controllable Human Video Generation [[PDF](https://arxiv.org/pdf/2503.04666),[Page](https://github.com/google-deepmind/wyd-benchmark)] ![Code](https://img.shields.io/github/stars/google-deepmind/wyd-benchmark?style=social&label=Star)


[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)



## Survey
[arxiv 2023.03]A Survey on Video Diffusion Models [[PDF](https://arxiv.org/abs/2310.10647)]

[arxiv 2024.05]Video Diffusion Models: A Survey [[PDF](https://arxiv.org/abs/2405.03150)]

[arxiv 2024.07]Diffusion Model-Based Video Editing: A Survey [[PDF](https://arxiv.org/abs/2407.07111),[Page](https://github.com/wenhao728/awesome-diffusion-v2v)]

[ResearchGate 2024.07]Conditional Video Generation Guided by Multimodal Inputs: A Comprehensive Survey [[PDF](https://www.researchgate.net/publication/382443305_Conditional_Video_Generation_Guided_by_Multimodal_Inputs_A_Comprehensive_Survey)]





## Speed 
[arxiv 2023.12]F3-Pruning: A Training-Free and Generalized Pruning Strategy towards Faster and Finer Text-to-Video Synthesis [[PDF](https://arxiv.org/abs/2312.03459)]

[arxiv 2023.12]VideoLCM: Video Latent Consistency Model [[PDF](https://arxiv.org/abs/2312.09109)]

[arxiv 2024.01]FlashVideo: A Framework for Swift Inference in Text-to-Video Generation [[PDF](https://arxiv.org/abs/2401.00869)]

[arxiv 2024.01]AnimateLCM: Accelerating the Animation of Personalized Diffusion Models and Adapters with Decoupled Consistency Learning [[PDF](https://arxiv.org/abs/2402.00769),[Page](https://animatelcm.github.io/)]

[arxiv 2024.03]AnimateDiff-Lightning: Cross-Model Diffusion Distillation [[PDF](https://arxiv.org/abs/2403.12706)]

[arxiv 2024.05] T2V-Turbo: Breaking the Quality Bottleneck of Video Consistency Model with Mixed Reward Feedback[[PDF](https://arxiv.org/abs/2405.18750),[Page](https://t2v-turbo.github.io/)]

[arxiv 2024.05] PCM : Phased Consistency Model[[PDF](https://arxiv.org/abs/2405.18407),[Page](https://g-u-n.github.io/projects/pcm/)]

[arxiv 2024.06]SF-V: Single Forward Video Generation Model [[PDF](https://arxiv.org/abs/2406.04324),[Page](https://snap-research.github.io/SF-V/)]

[arxiv 2024.06] Motion Consistency Model: Accelerating Video Diffusion with Disentangled Motion-Appearance Distillation [[PDF](https://arxiv.org/abs/2406.06890), [Page](https://yhzhai.github.io/mcm/)]

[arxiv 2024.07]QVD: Post-training Quantization for Video Diffusion Models [[PDF](https://arxiv.org/abs/2407.11585),[Page]()]

[arxiv 2024.08]Real-Time Video Generation with Pyramid Attention Broadcast [[PDF](https://arxiv.org/abs/2408.12588),[Page](https://github.com/NUS-HPC-AI-Lab/VideoSys)]

[arxiv 2024.11] Adaptive Caching for Faster Video Generation with Diffusion Transformers [[PDF](https://arxiv.org/abs/2411.02397),[Page](https://adacache-dit.github.io/)]

[arxiv 2024.11] Fast and Memory-Efficient Video Diffusion Using Streamlined Inference [[PDF](https://arxiv.org/abs/2411.01171)]

[arxiv 2024.11] Accelerating Vision Diffusion Transformers with Skip Branches  [[PDF](https://arxiv.org/abs/2411.17616),[Page](https://github.com/OpenSparseLLMs/Skip-DiT)] ![Code](https://img.shields.io/github/stars/OpenSparseLLMs/Skip-DiT?style=social&label=Star)

[arxiv 2024.12] Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model  [[PDF](https://arxiv.org/abs/2411.19108),[Page](https://liewfeng.github.io/TeaCache/)] ![Code](https://img.shields.io/github/stars/LiewFeng/TeaCache?style=social&label=Star)

[arxiv 2024.12] Individual Content and Motion Dynamics Preserved Pruning for Video Diffusion Models  [[PDF](https://arxiv.org/abs/2411.18350),] 

[arxiv 2024.12] Accelerating Video Diffusion Models via Distribution Matching  [[PDF](https://arxiv.org/abs/2412.05899)]

[arxiv 2024.12] From Slow Bidirectional to Fast Causal Video Generators  [[PDF](https://arxiv.org/abs/2412.07772),[Page](https://causvid.github.io/)] 

[arxiv 2024.12]  Mobile Video Diffusion [[PDF](https://arxiv.org/abs/2412.07583)]

[arxiv 2024.12]  AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric Reduction and Restoration [[PDF](https://arxiv.org/abs/2412.11706)]

[arxiv 2024.12]  SnapGen-V: Generating a Five-Second Video within Five Seconds on a Mobile Device [[PDF](https://arxiv.org/abs/2412.10494),[Page](https://snap-research.github.io/snapgen-v/)] 

[arxiv 2025.01] Diffusion Adversarial Post-Training for One-Step Video Generation  [[PDF](https://arxiv.org/abs/2501.08316)]

[arxiv 2025.02]  Fast Video Generation with SLIDING TILE ATTENTION [[PDF](https://arxiv.org/pdf/2502.04507)]

[arxiv 2025.02]  Magic 1-For-1: Generating One Minute Video Clips within One Minute [[PDF](https://arxiv.org/abs/2502.07701),[Page](https://magic-141.github.io/Magic-141/)] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


[arxiv 2025.02]  Hardware-Friendly Static Quantization Method for Video Diffusion Transformers [[PDF](https://arxiv.org/pdf/2502.15077)]

[arxiv 2025.03] W2SVD: Weak-to-Strong Video Distillation for Large-Scale Portrait Few-Step Synthesis  [[PDF](https://arxiv.org/abs/2503.13319),[Page](https://w2svd.github.io/W2SVD/)]


[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## Dataset optimization 
[arxiv 2025.01] A Large-Scale Study on Video Action Dataset Condensation  [[PDF](https://arxiv.org/abs/2412.21197),[Page](https://github.com/MCG-NJU/Video-DC)] ![Code](https://img.shields.io/github/stars/MCG-NJU/Video-DC?style=social&label=Star)



## Others 
[arxiv 2023.05]AADiff: Audio-Aligned Video Synthesis with Text-to-Image Diffusion [[PDF](https://arxiv.org/abs/2305.04001)]

[arxiv 2023.05]Multi-object Video Generation from Single Frame Layouts [[PDF](https://arxiv.org/abs/2305.03983)]

[arxiv 2023.06]Learn the Force We Can: Multi-Object Video Generation from Pixel-Level Interactions [[PDF](https://arxiv.org/abs/2306.03988)]

[arxiv 2023.08]DiffSynth: Latent In-Iteration Deflickering for Realistic Video Synthesis [[PDF](https://arxiv.org/abs/2308.03463)]


## Visual effect

[arxiv 2024.11] AutoVFX: Physically Realistic Video Editing from Natural Language Instructions [[PDF](https://arxiv.org/abs/2411.02394),[Page](https://haoyuhsu.github.io/autovfx-website/)]

[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)

## CG2real
[arxiv 2024.09] AMG: Avatar Motion Guided Video Generation [[PDF](https://arxiv.org/abs/2409.01502),[Page](https://github.com/zshyang/amg)]

[arxiv 2024.09] Compositional 3D-aware Video Generation with LLM Director [[PDF](https://arxiv.org/abs/2409.00558),[Page](https://www.microsoft.com/en-us/research/project/compositional-3d-aware-video-generation/)]

[arxiv 2024.10] SceneCraft: Layout-Guided 3D Scene Generation [[PDF](https://arxiv.org/abs/2410.09049),[Page](https://orangesodahub.github.io/SceneCraft)]

[arxiv 2024.10] Tex4D: Zero-shot 4D Scene Texturing with Video Diffusion Models [[PDF](https://arxiv.org/abs/2410.10821),[Page](https://tex4d.github.io/)]

[arxiv 2024.10] Diffusion Curriculum: Synthetic-to-Real Generative Curriculum Learning via Image-Guided Diffusion [[PDF](https://arxiv.org/abs/2410.13674),[Page](https://github.com/tianyi-lab/DisCL)]

[arxiv 2024.10] FashionR2R: Texture-preserving Rendered-to-Real Image Translation with Diffusion Models [[PDF](https://arxiv.org/pdf/2410.14429),[Page](https://rickhh.github.io/FashionR2R/)]

[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## world model & interactive generation
[arxiv 2024.06] AVID: Adapting Video Diffusion Models to World Models [[PDF](),[Page](https://sites.google.com/view/avid-world-model-adapters/home)]

[arxiv 2024.08]Diffusion Models Are Real-Time Game Engines [[PDF](https://arxiv.org/abs/2408.14837),[Page](https://gamengen.github.io/)]

[arxiv 2024.08] Body of Her: A Preliminary Study on End-to-End Humanoid Agent  [[PDF](https://arxiv.org/pdf/2408.02879)] 

[arxiv 2024.09] Video Game Generation: A Practical Study using Mario [[PDF](https://virtual-protocol.github.io/mario-videogamegen/static/pdfs/VideoGameGen.pdf),[Page](https://virtual-protocol.github.io/mario-videogamegen/)]


[arxiv 2024.10] WorldSimBench: Towards Video Generation Models as World Simulators [[PDF](https://arxiv.org/abs/2410.18072),[Page](https://iranqin.github.io/WorldSimBench.github.io/)]

[arxiv 2024.10] Agent-to-Sim: Learning Interactive Behavior Models from Casual Longitudinal Videos [[PDF](https://arxiv.org/abs/2410.16259),[Page](https://gengshan-y.github.io/agent2sim-www/)]

[arxiv 2024.10] SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video Generation [[PDF](https://arxiv.org/abs/2410.23277),[Page](https://slowfast-vgen.github.io/)]

[arxiv 2024.10] ADAM: An Embodied Causal Agent in Open-World Environments [[PDF](https://arxiv.org/abs/2410.22194),[Page](https://opencausalab.github.io/ADAM/)]

[arxiv 2024.11] How Far is Video Generation from World Model: A Physical Law Perspective [[PDF](https://arxiv.org/abs/2411.02385),[Page](https://phyworld.github.io/)]

[arxiv 2024.11] Oasis: an interactive, explorable world model [[PDF](https://oasis-model.github.io/),[Page](https://www.etched.com/blog-posts/oasis)]

[arxiv 2024.11] GameGen-X: Interactive Open-world Game Video Generation [[PDF](https://arxiv.org/abs/2411.00769),[Page](https://gamegen-x.github.io/)]

[arxiv 2024.11] Generative World Explorer [[PDF](https://arxiv.org/abs/2411.11844),[Page](http://generative-world-explorer.github.io/)]

[arxiv 2024.11] The Matrix： Infinite-Horizon World Generation with Real-Time Interaction [[PDF](https://thematrix1999.github.io/article/the_matrix.pdf),[Page](https://thematrix1999.github.io/)]

[arxiv 2024.12] Navigation World Models  [[PDF](https://arxiv.org/abs/2412.03572),[Page](https://www.amirbar.net/nwm/)]

[arxiv 2024.12] GenEx: Generating an Explorable World  [[PDF](https://arxiv.org/abs/2412.09624),[Page](http://genex.world/)] 

[arxiv 2025.01] GameFactory: Creating New Games with Generative Interactive Videos  [[PDF](KwaiVGI/GameFactory),[Page](https://vvictoryuki.github.io/gamefactory/)] ![Code](https://img.shields.io/github/stars/KwaiVGI/GameFactory?style=social&label=Star)

[arxiv 2025.02] Pre-Trained Video Generative Models as World Simulators  [[PDF](https://arxiv.org/pdf/2502.07825)]

[arxiv 2025.03] Position: Interactive Generative Video as Next-Generation Game Engine  [[PDF](https://arxiv.org/abs/2503.17359)]


[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## driving
[arxiv 2024.10] FreeVS: Generative View Synthesis on Free Driving Trajectory [[PDF](https://arxiv.org/abs/2410.18079),[Page](https://freevs24.github.io/)]

[arxiv 2024.11] DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving  [[PDF](https://arxiv.org/abs/2411.15139),[Page](https://github.com/hustvl/DiffusionDrive)] ![Code](https://img.shields.io/github/stars/hustvl/DiffusionDrive?style=social&label=Star)

[arxiv 2024.12]  InfinityDrive: Breaking Time Limits in Driving World Models [[PDF](https://arxiv.org/abs/2412.01522),[Page](https://metadrivescape.github.io/papers_project/InfinityDrive/page.html)] 

[arxiv 2024.12] Stag-1: Towards Realistic 4D Driving Simulation with Video Generation Model  [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/wzzheng/Stag?style=social&label=Star)

[arxiv 2024.12]  UniMLVG: Unified Framework for Multi-view Long Video Generation with Comprehensive Control Capabilities for Autonomous Driving [[PDF](https://arxiv.org/abs/2412.04842),[Page](https://sensetime-fvg.github.io/UniMLVG/)]

[arxiv 2024.12] UniScene: Unified Occupancy-centric Driving Scene Generation  [[PDF](https://arxiv.org/abs/2412.05435),[Page](https://arlo0o.github.io/uniscene/)]

[arxiv 2024.12] ACT-BENCH: Towards Action Controllable World Models for Autonomous Driving  [[PDF](https://arxiv.org/abs/2412.05337)]

[arxiv 2024.12] Physical-Informed Driving World Model  [[PDF](https://arxiv.org/abs/2412.08410)]

[arxiv 2024.12]  Doe-1: Closed-Loop Autonomous Driving with Large World Model [[PDF](https://arxiv.org/pdf/2412.08643),[Page](https://wzzheng.net/Doe)] ![Code](https://img.shields.io/github/stars/wzzheng/Doe?style=social&label=Star)

[arxiv 2024.12]  GaussianWorld: Gaussian World Model for Streaming 3D Occupancy Prediction [[PDF](https://arxiv.org/abs/2412.10373),[Page](https://github.com/zuosc19/GaussianWorld)] ![Code](https://img.shields.io/github/stars/zuosc19/GaussianWorld?style=social&label=Star)

[arxiv 2024.12] StreetCrafter: Street View Synthesis with Controllable Video Diffusion Models  [[PDF](https://arxiv.org/pdf/2412.13188)]

[arxiv 2025.01]  DrivingWorld: Constructing World Model for Autonomous Driving via Video GPT [[PDF](https://arxiv.org/abs/2412.19505),[Page](https://huxiaotaostasy.github.io/DrivingWorld/index.html)] ![Code](https://img.shields.io/github/stars/YvanYin/DrivingWorld?style=social&label=Star)

[arxiv 2025.01] HERMES: A Unified Self-Driving World Model for Simultaneous 3D Scene Understanding and Generation  [[PDF](https://arxiv.org/abs/2501.14729),[Page](https://github.com/LMD0311/HERMES)] ![Code](https://img.shields.io/github/stars/LMD0311/HERMES?style=social&label=Star)

[arxiv 2025.02] VaViM and VaVAM: Autonomous Driving through Video Generative Modeling  [[PDF](https://arxiv.org/abs/2502.15672),[Page](https://valeoai.github.io/vavim-vavam/)] ![Code](https://img.shields.io/github/stars/valeoai/VideoActionModel?style=social&label=Star)

[arxiv 2025.03] MiLA: Multi-view Intensive-fidelity Long-term Video Generation World Model for Autonomous Driving  [[PDF](https://xiaomi-mlab.github.io/mila.github.io/),[Page](https://github.com/xiaomi-mlab/mila.github.io)] ![Code](https://img.shields.io/github/stars/xiaomi-mlab/mila.github.io?style=social&label=Star)


[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)



## Feedback

[arxiv 2024.12]  Improving Dynamic Object Interactions in Text-to-Video Generation with AI Feedback [[PDF](https://arxiv.org/abs/2412.02617),[Page](https://sites.google.com/view/aif-dynamic-t2v/)] 


[arxiv 2024.12]  LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment [[PDF](https://arxiv.org/pdf/2412.04814),[Page](https://codegoat24.github.io/LiFT/)] ![Code](https://img.shields.io/github/stars/CodeGoat24/LiFT?style=social&label=Star)

[arxiv 2024.12] OnlineVPO: Align Video Diffusion Model with Online Video-Centric Preference Optimization  [[PDF](https://arxiv.org/abs/2412.15159),[Page](https://onlinevpo.github.io/)] 

[arxiv 2024.12] Prompt-A-Video: Prompt Your Video Diffusion Model via Preference-Aligned LLM  [[PDF](https://arxiv.org/abs/2412.15156),[Page](https://arxiv.org/abs/2412.15156)] 

[arxiv 2025.01] Personalized Preference Fine-tuning of Diffusion Models  [[PDF](https://arxiv.org/pdf/2501.06655)]

[arxiv 2025.01] Improving Video Generation with Human Feedback  [[PDF](https://arxiv.org/abs/2501.13918),[Page](https://gongyeliu.github.io/videoalign/)] 

[arxiv 2025.02]  HuViDPO: Enhancing Video Generation through Direct Preference Optimization for Human-Centric Alignment [[PDF](https://arxiv.org/abs/2502.01690)]

[arxiv 2025.02] CHATS: Combining Human-Aligned Optimization and Test-Time Sampling for Text-to-Image Generation  [[PDF](https://arxiv.org/pdf/2502.12579)]


[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## CV Related 
[arxiv 2022.12; ByteDace]PV3D: A 3D GENERATIVE MODEL FOR PORTRAIT VIDEO GENERATION [[PDF](https://arxiv.org/pdf/2212.06384.pdf)]

[arxiv 2022.12]MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation[[PDF](https://arxiv.org/pdf/2212.09478.pdf)]

[arxiv 2022.12]Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation [[PDF](https://arxiv.org/pdf/2212.11565.pdf), [Page](https://tuneavideo.github.io/)]

[arxiv 2023.01]Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation [[PDF](https://arxiv.org/pdf/2301.03396.pdf), [Page](https://mstypulkowski.github.io/diffusedheads)]

[arxiv 2023.01]DiffTalk: Crafting Diffusion Models for Generalized Talking Head Synthesis [[PDF](https://arxiv.org/pdf/2301.03786.pdf), [Page](https://sstzal.github.io/DiffTalk/)]

[arxiv 2023.02 Google]Scaling Vision Transformers to 22 Billion Parameters [[PDF](https://arxiv.org/abs/2302.05442)]

[arxiv 2023.05]VDT: An Empirical Study on Video Diffusion with Transformers [[PDF](https://arxiv.org/abs/2305.13311), [code](https://github.com/RERV/VDT)]

[arxiv 2024] MAGVIT-V2 : Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation [[PDF](https://arxiv.org/abs/2310.05737)]

[arxiv 2024.08]Sapiens: Foundation for Human Vision Models [[PDF](https://arxiv.org/abs/2408.12569),[Page](https://about.meta.com/realitylabs/codecavatars/sapiens)]


[arxiv 2024.10] ReferEverything: Towards Segmenting Everything We Can Speak of in Videos [[PDF](https://arxiv.org/abs/2410.23287),[Page](https://miccooper9.github.io/projects/ReferEverything/)]

[arxiv 2024.10]VideoSAM: A Large Vision Foundation Model for High-Speed Video Segmentation  [[PDF](https://arxiv.org/abs/2410.21304),[Page](https://github.com/chikap421/videosam)]


[arxiv 2024.11]  Generative Omnimatte: Learning to Decompose Video into Layers [[PDF](https://arxiv.org/abs/2411.16683),[Page](https://gen-omnimatte.github.io/)]

[arxiv 2025.01] Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos  [[PDF](https://arxiv.org/abs/2501.04001),[Page](https://lxtgh.github.io/project/sa2va/)] ![Code](https://img.shields.io/github/stars/magic-research/Sa2VA?style=social&label=Star)


[arxiv 2025.03]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## NLP related
[arxiv 2022.10]DIFFUSEQ: SEQUENCE TO SEQUENCE TEXT GENERATION WITH DIFFUSION MODELS [[PDF](https://arxiv.org/pdf/2210.08933.pdf)]

[arxiv 2023.02]The Flan Collection: Designing Data and Methods for Effective Instruction Tuning [[PDF](https://arxiv.org/pdf/2301.13688.pdf)]





## Speech 
[arxiv 2023.01]Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers[[PDF](https://arxiv.org/abs/2301.02111), [Page](https://valle-demo.github.io/)]

[arxiv 2024.09]EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions[[PDF](https://arxiv.org/abs/2409.18042), [Page](https://emova-ollm.github.io/)]
