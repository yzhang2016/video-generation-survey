
## Gaussian Face 
[arxiv 2025.01] PERSE: Personalized 3D Generative Avatars from A Single Portrait  [[PDF](https://arxiv.org/abs/2412.21206),[Page](https://hyunsoocha.github.io/perse/)] ![Code](https://img.shields.io/github/stars/snuvclab/perse?style=social&label=Star)

[arxiv 2025.04]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)



## Body 

[arxiv 2024.10] MotionAura: Generating High-Quality and Motion Consistent Videos using Discrete Diffusion  [[PDF](https://arxiv.org/abs/2410.07659)]

[arxiv 2024.10]  ControlMM: Controllable Masked Motion Generation [[PDF](http://arxiv.org/abs/2312.03596),[Page](https://exitudio.github.io/ControlMM-page/)]

[arxiv 2024.10] MotionBank: A Large-scale Video Motion Benchmark with Disentangled Rule-based Annotations  [[PDF](),[Page]()]

[arxiv 2024.10] Multi-modal Pose Diffuser: A Multimodal Generative Conditional Pose Prior  [[PDF](https://arxiv.org/abs/2410.14540)]

[arxiv 2024.10] LEAD: Latent Realignment for Human Motion Diffusion  [[PDF](https://arxiv.org/pdf/2410.14508)]

[arxiv 2024.10]  MotionCLR: Motion Generation and Training-free Editing via Understanding Attention Mechanisms [[PDF](https://arxiv.org/abs/2410.18977),[Page](https://lhchen.top/MotionCLR/)]

[arxiv 2024.11]  KMM: Key Frame Mask Mamba for Extended Motion Generation [[PDF](https://arxiv.org/abs/2411.06481),[Page](https://steve-zeyu-zhang.github.io/KMM/)]

[arxiv 2024.11] Rethinking Diffusion for Text-Driven Human Motion Generation  [[PDF](https://arxiv.org/abs/2411.16575)]

[arxiv 2024.11] SMGDiff: Soccer Motion Generation using diffusion probabilistic models  [[PDF](https://arxiv.org/abs/2411.16216)]

[arxiv 2024.11] DRiVE: Diffusion-based Rigging Empowers Generation of Versatile and Expressive Characters  [[PDF](https://arxiv.org/abs/2411.17423),[Page](https://driveavatar.github.io/)] ![Code](https://img.shields.io/github/stars/DRiVEAvatar/DRiVEAvatar.github.io?style=social&label=Star)

[arxiv 2024.11]  UniPose: A Unified Multimodal Framework for Human Pose Comprehension, Generation and Editing [[PDF](https://arxiv.org/abs/2411.16781)] 

[arxiv 2024.12] AToM: Aligning Text-to-Motion Model at Event-Level with GPT-4Vision Reward  [[PDF](https://arxiv.org/abs/2411.18654),[Page](https://atom-motion.github.io/)] ![Code](https://img.shields.io/github/stars/VincentHancoder/AToM?style=social&label=Star)

[arxiv 2024.12]  AdaVLN: Towards Visual Language Navigation in Continuous Indoor Environments with Moving Humans
 [[PDF](https://arxiv.org/abs/2411.18539),[Page](https://github.com/dillonloh/AdaVLN)] ![Code](https://img.shields.io/github/stars/dillonloh/AdaVLN?style=social&label=Star)

[arxiv 2024.12]  InfiniDreamer: Arbitrarily Long Human Motion Generation via Segment Score Distillation [[PDF](https://arxiv.org/abs/2411.18303)] 

[arxiv 2024.12] One Shot, One Talk: Whole-body Talking Avatar from a Single Image  [[PDF](https://arxiv.org/abs/2412.01106),[Page](https://ustc3dv.github.io/OneShotOneTalk/)] 

[arxiv 2024.12]  SoPo: Text-to-Motion Generation Using Semi-Online Preference Optimization [[PDF](https://sopo-motion.github.io/),[Page](https://sopo-motion.github.io/)] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)

[arxiv 2024.12]  Retrieving Semantics from the Deep: an RAG Solution for Gesture Synthesis [[PDF](https://arxiv.org/abs/2403.17936),[Page](https://vcai.mpi-inf.mpg.de/projects/RAG-Gesture/)] 

[arxiv 2024.12]  CoMA: Compositional Human Motion Generation with Multi-modal Agents [[PDF](https://arxiv.org/abs/2412.07320),[Page](https://gabrie-l.github.io/coma-page/)] ![Code](https://img.shields.io/github/stars/Siwensun/CoMA?style=social&label=Star)

[arxiv 2024.12]  Move-in-2D: 2D-Conditioned Human Motion Generation [[PDF](https://arxiv.org/abs/2412.13185),[Page](https://hhsinping.github.io/Move-in-2D/)] 

[arxiv 2024.12] Motion-2-to-3: Leveraging 2D Motion Data to Boost 3D Motion Generation  [[PDF](https://arxiv.org/abs/2412.13111),[Page](https://zju3dv.github.io/Motion-2-to-3/)] ![Code](https://img.shields.io/github/stars/zju3dv/Motion-2-to-3?style=social&label=Star)

[arxiv 2024.12] ScaMo: Exploring the Scaling Law in Autoregressive Motion Generation Model  [[PDF](https://arxiv.org/abs/2412.14559),[Page](https://shunlinlu.github.io/ScaMo/)] ![Code](https://img.shields.io/github/stars/shunlinlu/ScaMo_code?style=social&label=Star)

[arxiv 2025.01]  Make-A-Character 2: Animatable 3D Character Generation From a Single Image [[PDF](https://arxiv.org/pdf/2501.07870)]

[arxiv 2025.01]  FlexMotion: Lightweight, Physics-Aware, and Controllable Human Motion Generation [[PDF](https://arxiv.org/abs/2501.16778)]

[arxiv 2025.02]  MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm [[PDF](https://arxiv.org/abs/2502.02358),[Page](https://diouo.github.io/motionlab.github.io/)] ![Code](https://img.shields.io/github/stars/Diouo/MotionLab?style=social&label=Star)

[arxiv 2025.02] CASIM: Composite Aware Semantic Injection for Text to Motion Generation  [[PDF](https://arxiv.org/abs/2502.02063),[Page](https://cjerry1243.github.io/casim_t2m/)] ![Code](https://img.shields.io/github/stars/cjerry1243/casim_t2m?style=social&label=Star)

[arxiv 2025.02] Free-T2M: Frequency Enhanced Text-to-Motion Diffusion Model With Consistency Loss  [[PDF](https://arxiv.org/abs/2501.18232),[Page](https://github.com/Hxxxz0/Free-T2m)] ![Code](https://img.shields.io/github/stars/Hxxxz0/Free-T2m?style=social&label=Star)

[arxiv 2025.02] Fg-T2M++: LLMs-Augmented Fine-Grained Text Driven Human Motion Generation  [[PDF](https://arxiv.org/abs/2502.05534)]

[arxiv 2025.03] StickMotion: Generating 3D Human Motions by Drawing a Stickman  [[PDF](https://arxiv.org/abs/2503.04829)]

[arxiv 2025.03]  HumanMM: Global Human Motion Recovery from Multi-shot Videos [[PDF](https://arxiv.org/abs/2503.07597),[Page](https://zhangyuhong01.github.io/HumanMM/)] ![Code](https://img.shields.io/github/stars/zhangyuhong01/HumanMM-code?style=social&label=Star)

[arxiv 2025.03]  PersonaBooth: Personalized Text-to-Motion Generation [[PDF](https://arxiv.org/abs/2503.07390),[Page](https://boeun-kim.github.io/page-PersonaBooth/)] ![Code](https://img.shields.io/github/stars/Boeun-Kim/MoST?style=social&label=Star)

[arxiv 2025.03] Motion Anything: Any to Motion Generation  [[PDF](https://arxiv.org/abs/2503.06955),[Page](https://steve-zeyu-zhang.github.io/MotionAnything/)] ![Code](https://img.shields.io/github/stars/steve-zeyu-zhang/MotionAnything?style=social&label=Star)

[arxiv 2025.03] HERO: Human Reaction Generation from Videos  [[PDF](https://arxiv.org/pdf/2503.08270),[Page](https://jackyu6.github.io/HERO/)] 

[arxiv 2025.03]  NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models [[PDF](https://arxiv.org/pdf/2503.10626)]

[arxiv 2025.03] ACMo: Attribute Controllable Motion Generation  [[PDF](https://arxiv.org/abs/2503.11038),[Page](https://mjwei3d.github.io/ACMo/)] ![Code](https://img.shields.io/github/stars/MingjieWe/ACMo?style=social&label=Star)

[arxiv 2025.03] SALAD: Skeleton-aware Latent Diffusion for Text-driven Motion Generation and Editing
  [[PDF](https://arxiv.org/abs/2503.13836),[Page](https://seokhyeonhong.github.io/projects/salad/)] ![Code](https://img.shields.io/github/stars/seokhyeonhong/salad?style=social&label=Star)

[arxiv 2025.03] MAG: Multi-Modal Aligned Autoregressive Co-Speech Gesture Generation without Vector Quantization  [[PDF](https://arxiv.org/pdf/2503.14040)]

[arxiv 2025.03] MotionStreamer: Streaming Motion Generation via Diffusion-based Autoregressive Model in Causal Latent Space  [[PDF](https://arxiv.org/abs/2503.15451),[Page](https://zju3dv.github.io/MotionStreamer/)] ![Code](https://img.shields.io/github/stars/Li-xingXiao/272-dim-Motion-Representation?style=social&label=Star)

[arxiv 2025.04]  Shape My Moves: Text-Driven Shape-Aware Synthesis of Human Motions [[PDF](https://arxiv.org/abs/2504.03639),[Page](https://shape-move.github.io/)] 


[arxiv 2025.04]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)



## audio-to-gesture 
[arxiv 2024.10] Emphasizing Semantic Consistency of Salient Posture for Speech-Driven Gesture Generation  [[PDF](https://arxiv.org/abs/2410.13786)]

[arxiv 2025.03]  ExGes: Expressive Human Motion Retrieval and Modulation for Audio-Driven Gesture Synthesis [[PDF](https://arxiv.org/pdf/2503.06499)]


[arxiv 2025.04]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)




## Hands 
[arxiv 2024.10]Learning Interaction-aware 3D Gaussian Splatting for One-shot Hand Avatars [[PDF](https://arxiv.org/abs/2410.08840),[Page](https://github.com/XuanHuang0/GuassianHand)]

[arxiv 2024.11] UniHands: Unifying Various Wild-Collected Keypoints for Personalized Hand Reconstruction  [[PDF](https://arxiv.org/abs/2411.11845),[Page]()]

[arxiv 2024.12]  FoundHand: Large-Scale Domain-Specific Learning for Controllable Hand Image Generation [[PDF](https://arxiv.org/abs/2412.02690)]

[arxiv 2024.12]  HandOS: 3D Hand Reconstruction in One Stage [[PDF](https://arxiv.org/abs/2412.01537),[Page](https://idea-research.github.io/HandOSweb/)] 

[arxiv 2024.12] GigaHands: A Massive Annotated Dataset of Bimanual Hand Activities  [[PDF](https://arxiv.org/abs/2412.04244),[Page](https://ivl.cs.brown.edu/research/gigahands.html)] 

[arxiv 2024.12]  Dyn-HaMR: Recovering 4D Interacting Hand Motion from a Dynamic Camera [[PDF](https://arxiv.org/abs/),[Page](https://dyn-hamr.github.io/)] ![Code](https://img.shields.io/github/stars/ZhengdiYu/Dyn-HaMR?style=social&label=Star)

[arxiv 2025.01]  Predicting 4D Hand Trajectory from Monocular Videos [[PDF](https://arxiv.org/abs/2501.08329),[Page](https://judyye.github.io/4dhands)]

[arxiv 2025.04]  Direction-Aware Hybrid Representation Learning for 3D Hand Pose and Shape Estimation [[PDF](https://arxiv.org/abs/2504.01298)]


[arxiv 2025.04]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


# ego
[arxiv 2025.04]  The Invisible EgoHand: 3D Hand Forecasting through EgoBody Pose Estimation [[PDF](https://arxiv.org/abs/2504.08654),[Page](https://masashi-hatano.github.io/EgoH4/)] ![Code](https://img.shields.io/github/stars/masashi-hatano/EgoH4?style=social&label=Star)

[arxiv 2025.04]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)




## Human Interaction 
[arxiv 2024.05]  Scaling Up Dynamic Human-Scene Interaction Modeling [[PDF](https://arxiv.org/abs/2403.08629),[Page](https://jnnan.github.io/trumans/)] ![Code](https://img.shields.io/github/stars/jnnan/trumans_utils?style=social&label=Star)

[arxiv 2024.06]  Introducing HOT3D: An Egocentric Dataset for 3D Hand and Object Tracking [[PDF](https://arxiv.org/pdf/2406.09598),[Page](https://facebookresearch.github.io/hot3d/)] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)

[arxiv 2024.10]  Sitcom-Crafter: A Plot-Driven Human Motion Generation System in 3D Scenes [[PDF](https://arxiv.org/abs/2410.10790),[Page](https://windvchen.github.io/Sitcom-Crafter/)]

[arxiv 2024.09] DreamHOI: Subject-Driven Generation of 3D Human-Object Interactions with Diffusion Priors[[PDF](https://arxiv.org/abs/2409.08278),[Page](https://dreamhoi.github.io/)]

[arxiv 2024.10] GraspDiffusion: Synthesizing Realistic Whole-body Hand-Object Interaction [[PDF](https://arxiv.org/abs/2410.13911),[Page]()]

[arxiv 2024.11] AnchorCrafter: Animate CyberAnchors Saling Your Products via Human-Object Interacting Video Generation  [[PDF](https://arxiv.org/abs/2411.17383),[Page](https://cangcz.github.io/Anchor-Crafter/)] ![Code](https://img.shields.io/github/stars/cangcz/AnchorCrafter?style=social&label=Star)

[arxiv 2024.12] HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos  [[PDF](https://arxiv.org/abs/2411.19167),[Page](https://facebookresearch.github.io/hot3d/)] 

[arxiv 2024.12]  OOD-HOI: Text-Driven 3D Whole-Body Human-Object Interactions Generation Beyond Training Domains [[PDF](https://nickk0212.github.io/ood-hoi/#),[Page](https://nickk0212.github.io/ood-hoi/)] 

[arxiv 2024.12]  FIction: 4D Future Interaction Prediction from Video [[PDF](https://arxiv.org/abs/2408.00672),[Page](https://vision.cs.utexas.edu/projects/FIction/)] ![Code](https://img.shields.io/github/stars/thechargedneutron/FIction?style=social&label=Star)

[arxiv 2024.12]  TriDi: Trilateral Diffusion of 3D Humans, Objects and Interactions [[PDF](https://arxiv.org/abs/),[Page](https://virtualhumans.mpi-inf.mpg.de/tridi/)] ![Code](https://img.shields.io/github/stars/ptrvilya/tridi?style=social&label=Star)

[arxiv 2024.12] ContextHOI: Spatial Context Learning for Human-Object Interaction Detection  [[PDF](https://arxiv.org/abs/2412.09050)]

[arxiv 2025.01] DiffGrasp: Whole-Body Grasping Synthesis Guided by Object Motion Using a Diffusion Model  [[PDF](https://www.arxiv.org/abs/2412.20657),[Page](https://iscas3dv.github.io/DiffGrasp/)] ![Code](https://img.shields.io/github/stars/iscas3dv/DiffGrasp?style=social&label=Star)

[arxiv 2025.01]  DAViD: Modeling Dynamic Affordance of 3D Objects using Pre-trained Video Diffusion Models [[PDF](https://arxiv.org/abs/2501.08333),[Page](https://snuvclab.github.io/david/)] ![Code](https://img.shields.io/github/stars/snuvclab/david?style=social&label=Star)

[arxiv 2025.02]  InterMimic: Towards Universal Whole-Body Control for Physics-Based Human-Object Interactions [[PDF](https://arxiv.org/pdf/2502.20390),[Page](https://sirui-xu.github.io/InterMimic/)] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)

[arxiv 2025.02]  Ready-to-React: Online Reaction Policy for Two-Character Interaction Generation [[PDF](https://arxiv.org/abs/2502.20370),[Page](https://zju3dv.github.io/ready_to_react/)] ![Code](https://img.shields.io/github/stars/zju3dv/ready_to_react?style=social&label=Star)

[arxiv 2025.03]  Towards Semantic 3D Hand-Object Interaction Generation via Functional Text Guidance [[PDF](https://arxiv.org/pdf/2502.20805)]

[arxiv 2025.03] 3D Human Interaction Generation: A Survey [[PDF](https://arxiv.org/abs/2503.13120)]

[arxiv 2025.03] A Survey on Human Interaction Motion Generation  [[PDF](https://arxiv.org/abs/2503.12763)]

[arxiv 2025.03] Auto-Regressive Diffusion for Generating 3D Human-Object Interactions  [[PDF](https://arxiv.org/pdf/2503.16801),[Page](https://github.com/gengzichen/ARDHOI)] ![Code](https://img.shields.io/github/stars/gengzichen/ARDHOI?style=social&label=Star)

[arxiv 2025.03]  Guiding Human-Object Interactions with Rich Geometry and Relations [[PDF](https://arxiv.org/abs/2503.20172),[Page](https://lalalfhdh.github.io/rog_page/)]

[arxiv 2025.04]  SIGHT: Single-Image Conditioned Generation of Hand Trajectories for Hand-Object Interaction [[PDF](https://arxiv.org/abs/2503.22869)]

[arxiv 2025.04]  MixerMDM: Learnable Composition of Human Motion Diffusion Models [[PDF](https://arxiv.org/abs/2504.01019),[Page](https://pabloruizponce.com/papers/MixerMDM)] ![Code](https://img.shields.io/github/stars/pabloruizponce/MixerMDM?style=social&label=Star)

[arxiv 2025.04]  InteractVLM: 3D Interaction Reasoning from 2D Foundational Models [[PDF](https://arxiv.org/abs/2504.05303),[Page](https://interactvlm.is.tue.mpg.de/)] 

[arxiv 2025.04]  How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday Interactions [[PDF](https://arxiv.org/abs/2504.12284),[Page](https://ap229997.github.io/projects/latentact/)] ![Code](https://img.shields.io/github/stars/ap229997/latentact?style=social&label=Star)

[arxiv 2025.04]  InterAnimate: Taming Region-aware Diffusion Model for Realistic Human Interaction Animation [[PDF](https://arxiv.org/pdf/2504.10905)]

[arxiv 2025.04] HUMOTO: A 4D Dataset of Mocap Human Object Interactions  [[PDF](https://arxiv.org/abs/2504.10414),[Page](https://jiaxin-lu.github.io/humoto/)] 


[arxiv 2025.04]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## Interaction Detection 

[arxiv 2024.12]  Orchestrating the Symphony of Prompt Distribution Learning for Human-Object Interaction Detection [[PDF](https://arxiv.org/abs/2412.08506)]

[arxiv 2024.12]  HandsOnVLM: Vision-Language Models for Hand-Object Interaction Prediction [[PDF](https://arxiv.org/abs/2412.13187)]

[arxiv 2025.01]  Interacted Object Grounding in Spatio-Temporal Human-Object Interactions [[PDF](https://arxiv.org/abs/2412.19542),[Page](https://github.com/DirtyHarryLYL/HAKE-AVA)] ![Code](https://img.shields.io/github/stars/DirtyHarryLYL/HAKE-AVA?style=social&label=Star)

[arxiv 2025.03]  UniHOPE: A Unified Approach for Hand-Only and Hand-Object Pose Estimation [[PDF](https://arxiv.org/pdf/2503.13303),[Page](https://github.com/JoyboyWang/UniHOPE_Pytorch)] ![Code](https://img.shields.io/github/stars/JoyboyWang/UniHOPE_Pytorch?style=social&label=Star)

[arxiv 2025.03] Reconstructing In-the-Wild Open-Vocabulary Human-Object Interactions [[PDF](https://arxiv.org/abs/2503.15898),[Page](https://wenboran2002.github.io/3dhoi/)] ![Code](https://img.shields.io/github/stars/wenboran2002/open-3dhoi?style=social&label=Star)


[arxiv 2025.04]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## environment 
[arxiv 2024.10] DepthSplatï¼šConnecting Gaussian Splatting and Depth  [[PDF](https://arxiv.org/abs/2410.13862),[Page](https://haofeixu.github.io/depthsplat/)]

[arxiv 2024.10] Long-LRM: Long-sequence Large Reconstruction Model for Wide-coverage Gaussian Splats  [[PDF](https://arxiv.org/abs/2410.12781),[Page](https://arthurhero.github.io/projects/llrm/)]

[arxiv 2024.12] SCENIC: Scene-aware Semantic Navigation with Instruction-guided Control  [[PDF](https://arxiv.org/abs/2412.15664),[Page](https://virtualhumans.mpi-inf.mpg.de/scenic/)] 

[arxiv 2024.12]  ZeroHSI: Zero-Shot 4D Human-Scene Interaction [[PDF](https://arxiv.org/abs/2412.18600),[Page](https://awfuact.github.io/zerohsi/)] 

[arxiv 2025.03] SceneMI: Motion In-betweening for Modeling Human-Scene Interactions  [[PDF](https://arxiv.org/pdf/2503.16289),[Page](https://inwoohwang.me/SceneMI/)] 


[arxiv 2025.04]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)



## Capture 

[arxiv 2024.11] FreeCap: Hybrid Calibration-Free Motion Capture in Open Environments  [[PDF](https://arxiv.org/abs/2411.04469)]

[arxiv 2025.04]  EMO-X: Efficient Multi-Person Pose and Shape Estimation in One-Stage [[PDF](https://arxiv.org/abs/2504.08718)]

[arxiv 2025.04] CoMotion: Concurrent Multi-person 3D Motion  [[PDF](https://arxiv.org/abs/2504.12186),[Page](https://github.com/apple/ml-comotion)] ![Code](https://img.shields.io/github/stars/apple/ml-comotion?style=social&label=Star)


[arxiv 2025.04]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)
