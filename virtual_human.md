## Dataset
[arxiv 2025.07]  Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset [[PDF](https://arxiv.org/abs/2506.22554),[Page](https://github.com/facebookresearch/seamless_interaction)] ![Code](https://img.shields.io/github/stars/facebookresearch/seamless_interaction?style=social&label=Star)

[arxiv 2025.07] Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data  [[PDF](https://arxiv.org/abs/2507.07095),[Page](https://github.com/VankouF/MotionMillion-Codes)] ![Code](https://img.shields.io/github/stars/VankouF/MotionMillion-Codes?style=social&label=Star)


[arxiv 2025.10]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)



## Gaussian Face 
[arxiv 2025.01] PERSE: Personalized 3D Generative Avatars from A Single Portrait  [[PDF](https://arxiv.org/abs/2412.21206),[Page](https://hyunsoocha.github.io/perse/)] ![Code](https://img.shields.io/github/stars/snuvclab/perse?style=social&label=Star)

[arxiv 2025.10]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)



## Body 

[arxiv 2024.10] MotionAura: Generating High-Quality and Motion Consistent Videos using Discrete Diffusion  [[PDF](https://arxiv.org/abs/2410.07659)]

[arxiv 2024.10]  ControlMM: Controllable Masked Motion Generation [[PDF](http://arxiv.org/abs/2312.03596),[Page](https://exitudio.github.io/ControlMM-page/)]

[arxiv 2024.10] MotionBank: A Large-scale Video Motion Benchmark with Disentangled Rule-based Annotations  [[PDF](),[Page]()]

[arxiv 2024.10] Multi-modal Pose Diffuser: A Multimodal Generative Conditional Pose Prior  [[PDF](https://arxiv.org/abs/2410.14540)]

[arxiv 2024.10] LEAD: Latent Realignment for Human Motion Diffusion  [[PDF](https://arxiv.org/pdf/2410.14508)]

[arxiv 2024.10]  MotionCLR: Motion Generation and Training-free Editing via Understanding Attention Mechanisms [[PDF](https://arxiv.org/abs/2410.18977),[Page](https://lhchen.top/MotionCLR/)]

[arxiv 2024.11]  KMM: Key Frame Mask Mamba for Extended Motion Generation [[PDF](https://arxiv.org/abs/2411.06481),[Page](https://steve-zeyu-zhang.github.io/KMM/)]

[arxiv 2024.11] Rethinking Diffusion for Text-Driven Human Motion Generation  [[PDF](https://arxiv.org/abs/2411.16575)]

[arxiv 2024.11] SMGDiff: Soccer Motion Generation using diffusion probabilistic models  [[PDF](https://arxiv.org/abs/2411.16216)]

[arxiv 2024.11] DRiVE: Diffusion-based Rigging Empowers Generation of Versatile and Expressive Characters  [[PDF](https://arxiv.org/abs/2411.17423),[Page](https://driveavatar.github.io/)] ![Code](https://img.shields.io/github/stars/DRiVEAvatar/DRiVEAvatar.github.io?style=social&label=Star)

[arxiv 2024.11]  UniPose: A Unified Multimodal Framework for Human Pose Comprehension, Generation and Editing [[PDF](https://arxiv.org/abs/2411.16781)] 

[arxiv 2024.12] AToM: Aligning Text-to-Motion Model at Event-Level with GPT-4Vision Reward  [[PDF](https://arxiv.org/abs/2411.18654),[Page](https://atom-motion.github.io/)] ![Code](https://img.shields.io/github/stars/VincentHancoder/AToM?style=social&label=Star)

[arxiv 2024.12]  AdaVLN: Towards Visual Language Navigation in Continuous Indoor Environments with Moving Humans
 [[PDF](https://arxiv.org/abs/2411.18539),[Page](https://github.com/dillonloh/AdaVLN)] ![Code](https://img.shields.io/github/stars/dillonloh/AdaVLN?style=social&label=Star)

[arxiv 2024.12]  InfiniDreamer: Arbitrarily Long Human Motion Generation via Segment Score Distillation [[PDF](https://arxiv.org/abs/2411.18303)] 

[arxiv 2024.12] One Shot, One Talk: Whole-body Talking Avatar from a Single Image  [[PDF](https://arxiv.org/abs/2412.01106),[Page](https://ustc3dv.github.io/OneShotOneTalk/)] 

[arxiv 2024.12]  SoPo: Text-to-Motion Generation Using Semi-Online Preference Optimization [[PDF](https://sopo-motion.github.io/),[Page](https://sopo-motion.github.io/)] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)

[arxiv 2024.12]  Retrieving Semantics from the Deep: an RAG Solution for Gesture Synthesis [[PDF](https://arxiv.org/abs/2403.17936),[Page](https://vcai.mpi-inf.mpg.de/projects/RAG-Gesture/)] 

[arxiv 2024.12]  CoMA: Compositional Human Motion Generation with Multi-modal Agents [[PDF](https://arxiv.org/abs/2412.07320),[Page](https://gabrie-l.github.io/coma-page/)] ![Code](https://img.shields.io/github/stars/Siwensun/CoMA?style=social&label=Star)

[arxiv 2024.12]  Move-in-2D: 2D-Conditioned Human Motion Generation [[PDF](https://arxiv.org/abs/2412.13185),[Page](https://hhsinping.github.io/Move-in-2D/)] 

[arxiv 2024.12] Motion-2-to-3: Leveraging 2D Motion Data to Boost 3D Motion Generation  [[PDF](https://arxiv.org/abs/2412.13111),[Page](https://zju3dv.github.io/Motion-2-to-3/)] ![Code](https://img.shields.io/github/stars/zju3dv/Motion-2-to-3?style=social&label=Star)

[arxiv 2024.12] ScaMo: Exploring the Scaling Law in Autoregressive Motion Generation Model  [[PDF](https://arxiv.org/abs/2412.14559),[Page](https://shunlinlu.github.io/ScaMo/)] ![Code](https://img.shields.io/github/stars/shunlinlu/ScaMo_code?style=social&label=Star)

[arxiv 2025.01]  Make-A-Character 2: Animatable 3D Character Generation From a Single Image [[PDF](https://arxiv.org/pdf/2501.07870)]

[arxiv 2025.01]  FlexMotion: Lightweight, Physics-Aware, and Controllable Human Motion Generation [[PDF](https://arxiv.org/abs/2501.16778)]

[arxiv 2025.02]  MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm [[PDF](https://arxiv.org/abs/2502.02358),[Page](https://diouo.github.io/motionlab.github.io/)] ![Code](https://img.shields.io/github/stars/Diouo/MotionLab?style=social&label=Star)

[arxiv 2025.02] CASIM: Composite Aware Semantic Injection for Text to Motion Generation  [[PDF](https://arxiv.org/abs/2502.02063),[Page](https://cjerry1243.github.io/casim_t2m/)] ![Code](https://img.shields.io/github/stars/cjerry1243/casim_t2m?style=social&label=Star)

[arxiv 2025.02] Free-T2M: Frequency Enhanced Text-to-Motion Diffusion Model With Consistency Loss  [[PDF](https://arxiv.org/abs/2501.18232),[Page](https://github.com/Hxxxz0/Free-T2m)] ![Code](https://img.shields.io/github/stars/Hxxxz0/Free-T2m?style=social&label=Star)

[arxiv 2025.02] Fg-T2M++: LLMs-Augmented Fine-Grained Text Driven Human Motion Generation  [[PDF](https://arxiv.org/abs/2502.05534)]

[arxiv 2025.03] StickMotion: Generating 3D Human Motions by Drawing a Stickman  [[PDF](https://arxiv.org/abs/2503.04829)]

[arxiv 2025.03]  HumanMM: Global Human Motion Recovery from Multi-shot Videos [[PDF](https://arxiv.org/abs/2503.07597),[Page](https://zhangyuhong01.github.io/HumanMM/)] ![Code](https://img.shields.io/github/stars/zhangyuhong01/HumanMM-code?style=social&label=Star)

[arxiv 2025.03]  PersonaBooth: Personalized Text-to-Motion Generation [[PDF](https://arxiv.org/abs/2503.07390),[Page](https://boeun-kim.github.io/page-PersonaBooth/)] ![Code](https://img.shields.io/github/stars/Boeun-Kim/MoST?style=social&label=Star)

[arxiv 2025.03] Motion Anything: Any to Motion Generation  [[PDF](https://arxiv.org/abs/2503.06955),[Page](https://steve-zeyu-zhang.github.io/MotionAnything/)] ![Code](https://img.shields.io/github/stars/steve-zeyu-zhang/MotionAnything?style=social&label=Star)

[arxiv 2025.03] HERO: Human Reaction Generation from Videos  [[PDF](https://arxiv.org/pdf/2503.08270),[Page](https://jackyu6.github.io/HERO/)] 

[arxiv 2025.03]  NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models [[PDF](https://arxiv.org/pdf/2503.10626)]

[arxiv 2025.03] ACMo: Attribute Controllable Motion Generation  [[PDF](https://arxiv.org/abs/2503.11038),[Page](https://mjwei3d.github.io/ACMo/)] ![Code](https://img.shields.io/github/stars/MingjieWe/ACMo?style=social&label=Star)

[arxiv 2025.03] SALAD: Skeleton-aware Latent Diffusion for Text-driven Motion Generation and Editing
  [[PDF](https://arxiv.org/abs/2503.13836),[Page](https://seokhyeonhong.github.io/projects/salad/)] ![Code](https://img.shields.io/github/stars/seokhyeonhong/salad?style=social&label=Star)

[arxiv 2025.03] MAG: Multi-Modal Aligned Autoregressive Co-Speech Gesture Generation without Vector Quantization  [[PDF](https://arxiv.org/pdf/2503.14040)]

[arxiv 2025.03] MotionStreamer: Streaming Motion Generation via Diffusion-based Autoregressive Model in Causal Latent Space  [[PDF](https://arxiv.org/abs/2503.15451),[Page](https://zju3dv.github.io/MotionStreamer/)] ![Code](https://img.shields.io/github/stars/Li-xingXiao/272-dim-Motion-Representation?style=social&label=Star)

[arxiv 2025.04]  Shape My Moves: Text-Driven Shape-Aware Synthesis of Human Motions [[PDF](https://arxiv.org/abs/2504.03639),[Page](https://shape-move.github.io/)] 

[arxiv 2025.05] Deterministic-to-Stochastic Diverse Latent Feature Mapping for Human Motion Synthesis  [[PDF](https://arxiv.org/abs/2505.00998)]

[arxiv 2025.05]  GENMO: A GENeralist Model for Human MOtion [[PDF](https://arxiv.org/abs/2505.01425),[Page](https://research.nvidia.com/labs/dair/genmo/)] 

[arxiv 2025.05]  ReAlign: Bilingual Text-to-Motion Generation via Step-Aware Reward-Guided Alignment [[PDF](https://arxiv.org/abs/2505.04974),[Page](https://github.com/wengwanjiang/ReAlign)] ![Code](https://img.shields.io/github/stars/wengwanjiang/ReAlign?style=social&label=Star)

[arxiv 2025.05] MoCLIP: Motion-Aware Fine-Tuning and Distillation of CLIP for Human Motion Generation  [[PDF](https://arxiv.org/abs/2505.10810)]

[arxiv 2025.06]  Motion-R1: Chain-of-Thought Reasoning and Reinforcement Learning for Human Motion Generation [[PDF](https://arxiv.org/abs/2506.10353),[Page](https://motion-r1.github.io/)] ![Code](https://img.shields.io/github/stars/GigaAI-Research/Motion-R1?style=social&label=Star)

[arxiv 2025.06]  PlanMoGPT: Flow-Enhanced Progressive Planning for Text to Motion Synthesis [[PDF](http://arxiv.org/abs/2506.17912),[Page](https://planmogpt.github.io/)] ![Code](https://img.shields.io/github/stars/PlanMoGPT/PlanMoGPT.github.io?style=social&label=Star)

[arxiv 2025.07]  MotionGPT3: Human Motion as a Second Modality [[PDF](https://arxiv.org/abs/2506.24086),[Page](https://motiongpt3.github.io/)] ![Code](https://img.shields.io/github/stars/OpenMotionLab/MotionGPT3?style=social&label=Star)

[arxiv 2025.07] Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data  [[PDF](https://arxiv.org/abs/2507.07095),[Page](https://github.com/VankouF/MotionMillion-Codes)] ![Code](https://img.shields.io/github/stars/VankouF/MotionMillion-Codes?style=social&label=Star)

[arxiv 2025.07] SnapMoGen: Human Motion Generation from Expressive Texts  [[PDF](https://www.arxiv.org/abs/2507.09122),[Page](https://snap-research.github.io/SnapMoGen/)] ![Code](https://img.shields.io/github/stars/snap-research/SnapMoGen?style=social&label=Star)

[arxiv 2025.07] MOSPA: Human Motion Generation Driven by Spatial Audio  [[PDF](https://arxiv.org/pdf/2507.11949)]

[arxiv 2025.07]  ReMoMask: Retrieval-Augmented Masked Motion Generation [[PDF](https://arxiv.org/abs/2508.02605),[Page](https://aigeeksgroup.github.io/ReMoMask)] ![Code](https://img.shields.io/github/stars/AIGeeksGroup/ReMoMask?style=social&label=Star)

[arxiv 2025.08]  Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model [[PDF](https://arxiv.org/abs/2508.07863),[Page](https://beingbeyond.github.io/Being-M0.5/)] ![Code](https://img.shields.io/github/stars/BeingBeyond/Being-M0.5?style=social&label=Star)

[arxiv 2025.08] InterSyn: Interleaved Learning for Dynamic Motion Synthesis in the Wild  [[PDF](https://arxiv.org/abs/2508.10297),[Page](https://myy888.github.io/InterSyn/)] 

[arxiv 2025.08]  Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence [[PDF](https://arxiv.org/abs/2508.13139)]

[arxiv 2025.08]  DanceEditor: Towards Iterative Editable Music-driven Dance Generation with Open-Vocabulary Descriptions [[PDF](),[Page](https://lzvsdy.github.io/DanceEditor/)] ![Code](https://img.shields.io/github/stars/LZVSDY/DanceEditor?style=social&label=Star)

[arxiv 2025.10] OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression  [[PDF](https://arxiv.org/abs/2510.14954)]

[arxiv 2025.10] OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation  [[PDF](https://arxiv.org/abs/2510.19789),[Page](https://github.com/GuoweiXu368/OmniMotion-X)] ![Code](https://img.shields.io/github/stars/GuoweiXu368/OmniMotion-X?style=social&label=Star)


[arxiv 2025.10]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## audio-to-gesture 
[arxiv 2024.10] Emphasizing Semantic Consistency of Salient Posture for Speech-Driven Gesture Generation  [[PDF](https://arxiv.org/abs/2410.13786)]

[arxiv 2025.03]  ExGes: Expressive Human Motion Retrieval and Modulation for Audio-Driven Gesture Synthesis [[PDF](https://arxiv.org/pdf/2503.06499)]


[arxiv 2025.10]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)




## Hands 
[arxiv 2024.10]Learning Interaction-aware 3D Gaussian Splatting for One-shot Hand Avatars [[PDF](https://arxiv.org/abs/2410.08840),[Page](https://github.com/XuanHuang0/GuassianHand)]

[arxiv 2024.11] UniHands: Unifying Various Wild-Collected Keypoints for Personalized Hand Reconstruction  [[PDF](https://arxiv.org/abs/2411.11845),[Page]()]

[arxiv 2024.12]  FoundHand: Large-Scale Domain-Specific Learning for Controllable Hand Image Generation [[PDF](https://arxiv.org/abs/2412.02690)]

[arxiv 2024.12]  HandOS: 3D Hand Reconstruction in One Stage [[PDF](https://arxiv.org/abs/2412.01537),[Page](https://idea-research.github.io/HandOSweb/)] 

[arxiv 2024.12] GigaHands: A Massive Annotated Dataset of Bimanual Hand Activities  [[PDF](https://arxiv.org/abs/2412.04244),[Page](https://ivl.cs.brown.edu/research/gigahands.html)] 

[arxiv 2024.12]  Dyn-HaMR: Recovering 4D Interacting Hand Motion from a Dynamic Camera [[PDF](https://arxiv.org/abs/),[Page](https://dyn-hamr.github.io/)] ![Code](https://img.shields.io/github/stars/ZhengdiYu/Dyn-HaMR?style=social&label=Star)

[arxiv 2025.01]  Predicting 4D Hand Trajectory from Monocular Videos [[PDF](https://arxiv.org/abs/2501.08329),[Page](https://judyye.github.io/4dhands)]

[arxiv 2025.04]  Direction-Aware Hybrid Representation Learning for 3D Hand Pose and Shape Estimation [[PDF](https://arxiv.org/abs/2504.01298)]

[arxiv 2025.10] TOUCH: Text-gUided Controllable Generation of Free-Form Hand-Object Interactions  [[PDF](https://arxiv.org/abs/2510.14874),[Page](https://guangyid.github.io/hoi123touch/)] 


[arxiv 2025.10]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


# ego
[arxiv 2025.04]  The Invisible EgoHand: 3D Hand Forecasting through EgoBody Pose Estimation [[PDF](https://arxiv.org/abs/2504.08654),[Page](https://masashi-hatano.github.io/EgoH4/)] ![Code](https://img.shields.io/github/stars/masashi-hatano/EgoH4?style=social&label=Star)

[arxiv 2025.10]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)




## Interaction 
[arxiv 2024.05]  Scaling Up Dynamic Human-Scene Interaction Modeling [[PDF](https://arxiv.org/abs/2403.08629),[Page](https://jnnan.github.io/trumans/)] ![Code](https://img.shields.io/github/stars/jnnan/trumans_utils?style=social&label=Star)

[arxiv 2024.06]  Introducing HOT3D: An Egocentric Dataset for 3D Hand and Object Tracking [[PDF](https://arxiv.org/pdf/2406.09598),[Page](https://facebookresearch.github.io/hot3d/)] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)

[arxiv 2024.10]  Sitcom-Crafter: A Plot-Driven Human Motion Generation System in 3D Scenes [[PDF](https://arxiv.org/abs/2410.10790),[Page](https://windvchen.github.io/Sitcom-Crafter/)]

[arxiv 2024.09] DreamHOI: Subject-Driven Generation of 3D Human-Object Interactions with Diffusion Priors[[PDF](https://arxiv.org/abs/2409.08278),[Page](https://dreamhoi.github.io/)]

[arxiv 2024.10] GraspDiffusion: Synthesizing Realistic Whole-body Hand-Object Interaction [[PDF](https://arxiv.org/abs/2410.13911),[Page]()]

[arxiv 2024.11] AnchorCrafter: Animate CyberAnchors Saling Your Products via Human-Object Interacting Video Generation  [[PDF](https://arxiv.org/abs/2411.17383),[Page](https://cangcz.github.io/Anchor-Crafter/)] ![Code](https://img.shields.io/github/stars/cangcz/AnchorCrafter?style=social&label=Star)

[arxiv 2024.12] HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos  [[PDF](https://arxiv.org/abs/2411.19167),[Page](https://facebookresearch.github.io/hot3d/)] 

[arxiv 2024.12]  OOD-HOI: Text-Driven 3D Whole-Body Human-Object Interactions Generation Beyond Training Domains [[PDF](https://nickk0212.github.io/ood-hoi/#),[Page](https://nickk0212.github.io/ood-hoi/)] 

[arxiv 2024.12]  FIction: 4D Future Interaction Prediction from Video [[PDF](https://arxiv.org/abs/2408.00672),[Page](https://vision.cs.utexas.edu/projects/FIction/)] ![Code](https://img.shields.io/github/stars/thechargedneutron/FIction?style=social&label=Star)

[arxiv 2024.12]  TriDi: Trilateral Diffusion of 3D Humans, Objects and Interactions [[PDF](https://arxiv.org/abs/),[Page](https://virtualhumans.mpi-inf.mpg.de/tridi/)] ![Code](https://img.shields.io/github/stars/ptrvilya/tridi?style=social&label=Star)

[arxiv 2024.12] ContextHOI: Spatial Context Learning for Human-Object Interaction Detection  [[PDF](https://arxiv.org/abs/2412.09050)]

[arxiv 2025.01] DiffGrasp: Whole-Body Grasping Synthesis Guided by Object Motion Using a Diffusion Model  [[PDF](https://www.arxiv.org/abs/2412.20657),[Page](https://iscas3dv.github.io/DiffGrasp/)] ![Code](https://img.shields.io/github/stars/iscas3dv/DiffGrasp?style=social&label=Star)

[arxiv 2025.01]  DAViD: Modeling Dynamic Affordance of 3D Objects using Pre-trained Video Diffusion Models [[PDF](https://arxiv.org/abs/2501.08333),[Page](https://snuvclab.github.io/david/)] ![Code](https://img.shields.io/github/stars/snuvclab/david?style=social&label=Star)

[arxiv 2025.02]  InterMimic: Towards Universal Whole-Body Control for Physics-Based Human-Object Interactions [[PDF](https://arxiv.org/pdf/2502.20390),[Page](https://sirui-xu.github.io/InterMimic/)] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)

[arxiv 2025.02]  Ready-to-React: Online Reaction Policy for Two-Character Interaction Generation [[PDF](https://arxiv.org/abs/2502.20370),[Page](https://zju3dv.github.io/ready_to_react/)] ![Code](https://img.shields.io/github/stars/zju3dv/ready_to_react?style=social&label=Star)

[arxiv 2025.03]  Towards Semantic 3D Hand-Object Interaction Generation via Functional Text Guidance [[PDF](https://arxiv.org/pdf/2502.20805)]

[arxiv 2025.03] 3D Human Interaction Generation: A Survey [[PDF](https://arxiv.org/abs/2503.13120)]

[arxiv 2025.03] A Survey on Human Interaction Motion Generation  [[PDF](https://arxiv.org/abs/2503.12763)]

[arxiv 2025.03] Auto-Regressive Diffusion for Generating 3D Human-Object Interactions  [[PDF](https://arxiv.org/pdf/2503.16801),[Page](https://github.com/gengzichen/ARDHOI)] ![Code](https://img.shields.io/github/stars/gengzichen/ARDHOI?style=social&label=Star)

[arxiv 2025.03]  Guiding Human-Object Interactions with Rich Geometry and Relations [[PDF](https://arxiv.org/abs/2503.20172),[Page](https://lalalfhdh.github.io/rog_page/)]

[arxiv 2025.04]  SIGHT: Single-Image Conditioned Generation of Hand Trajectories for Hand-Object Interaction [[PDF](https://arxiv.org/abs/2503.22869)]

[arxiv 2025.04]  MixerMDM: Learnable Composition of Human Motion Diffusion Models [[PDF](https://arxiv.org/abs/2504.01019),[Page](https://pabloruizponce.com/papers/MixerMDM)] ![Code](https://img.shields.io/github/stars/pabloruizponce/MixerMDM?style=social&label=Star)

[arxiv 2025.04]  InteractVLM: 3D Interaction Reasoning from 2D Foundational Models [[PDF](https://arxiv.org/abs/2504.05303),[Page](https://interactvlm.is.tue.mpg.de/)] 

[arxiv 2025.04]  How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday Interactions [[PDF](https://arxiv.org/abs/2504.12284),[Page](https://ap229997.github.io/projects/latentact/)] ![Code](https://img.shields.io/github/stars/ap229997/latentact?style=social&label=Star)

[arxiv 2025.04]  InterAnimate: Taming Region-aware Diffusion Model for Realistic Human Interaction Animation [[PDF](https://arxiv.org/pdf/2504.10905)]

[arxiv 2025.04] HUMOTO: A 4D Dataset of Mocap Human Object Interactions  [[PDF](https://arxiv.org/abs/2504.10414),[Page](https://jiaxin-lu.github.io/humoto/)] 

[arxiv 2025.05]  HOIGaze: Gaze Estimation During Hand-Object Interactions in Extended Reality Exploiting Eye-Hand-Head Coordination [[PDF](https://arxiv.org/abs/2504.19828),[Page](https://zhiminghu.net/hu25_hoigaze.html)] ![Code](https://img.shields.io/github/stars/CraneHzm/HOIGaze?style=social&label=Star)

[arxiv 2025.05]  MEgoHand: Multimodal Egocentric Hand-Object Interaction Motion Generation [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)

[arxiv 2025.05]  Large-Scale Multi-Character Interaction Synthesis [[PDF](https://arxiv.org/abs/2505.14087)]

[arxiv 2025.05]  Multi-Person Interaction Generation from Two-Person Motion Priors [[PDF](https://arxiv.org/pdf/2505.17860)]

[arxiv 2025.06] HOIDiNi: Human-Object Interaction through Diffusion Noise Optimization  [[PDF](https://arxiv.org/pdf/2506.15625),[Page](https://hoidini.github.io/)] ![Code](https://img.shields.io/github/stars/hoidini/HOIDiNi?style=social&label=Star)

[arxiv 2025.06]  GenHOI: Generalizing Text-driven 4D Human-Object Interaction Synthesis for Unseen Objects [[PDF](https://arxiv.org/abs/2506.15483),[Page](https://etach-qs.github.io/GenHOI_project/)] ![Code](https://img.shields.io/github/stars/etach-qs/GenHOI?style=social&label=Star)

[arxiv 2025.06] DuetGen: Music Driven Two-Person Dance Generation via Hierarchical Masked Modeling  [[PDF](https://arxiv.org/abs/2506.18680),[Page](https://github.com/anindita127/DuetGen)] ![Code](https://img.shields.io/github/stars/anindita127/DuetGen?style=social&label=Star)

[arxiv 2025.07]  HOI-Dyn: Learning Interaction Dynamics for Human-Object Motion Diffusion [[PDF](https://arxiv.org/pdf/2507.01737)]

[arxiv 2025.08]  Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions [[PDF](https://arxiv.org/pdf/2508.04681),[Page](https://liangxuy.github.io/InterVLA/)] ![Code](https://img.shields.io/github/stars/liangxuy/InterVLA?style=social&label=Star)

[arxiv 2025.08]  CoopDiff: Anticipating 3D Human-object Interactions via Contact-consistent Decoupled Diffusion [[PDF](https://arxiv.org/abs/2508.07162)]

[arxiv 2025.08]  GaussianArt: Unified Modeling of Geometry and Motion for Articulated Objects [[PDF](https://arxiv.org/abs/2508.14891),[Page](https://sainingzhang.github.io/project/gaussianart/)] 

[arxiv 2025.08]  ECHO: Ego-Centric modeling of Human-Object interactions [[PDF](https://arxiv.org/pdf/2508.21556)]

[arxiv 2025.09] InterAct: A Large-Scale Dataset of Dynamic, Expressive and Interactive Activities between Two People in Daily Scenarios  [[PDF](https://arxiv.org/abs/2509.05747),[Page](https://hku-cg.github.io/interact/)] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)

[arxiv 2025.09] ScoreHOI: Physically Plausible Reconstruction of Human-Object Interaction via Score-Guided Diffusion  [[PDF](https://arxiv.org/abs/2509.07920),[Page](https://github.com/RammusLeo/ScoreHOI)] ![Code](https://img.shields.io/github/stars/RammusLeo/ScoreHOI?style=social&label=Star)

[arxiv 2025.09]  InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction Generation [[PDF](https://arxiv.org/pdf/2509.09555),[Page](https://sirui-xu.github.io/InterAct/)] ![Code](https://img.shields.io/github/stars/wzyabcas/InterAct?style=social&label=Star)

[arxiv 2025.09]  OnlineHOI: Towards Online Human-Object Interaction Generation and Perception [[PDF](https://arxiv.org/abs/2509.12250)]

[arxiv 2025.10] Text2Interact: High-Fidelity and Diverse Text-to-Two-Person Interaction Generation  [[PDF](https://arxiv.org/abs/2510.06504)]

[arxiv 2025.10]  Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation [[PDF](https://arxiv.org/abs/2510.14976),[Page](https://stevenlsw.github.io/ponimator/)] ![Code](https://img.shields.io/github/stars/stevenlsw/ponimator?style=social&label=Star)

[arxiv 2025.10]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## Interaction Detection 

[arxiv 2024.12]  Orchestrating the Symphony of Prompt Distribution Learning for Human-Object Interaction Detection [[PDF](https://arxiv.org/abs/2412.08506)]

[arxiv 2024.12]  HandsOnVLM: Vision-Language Models for Hand-Object Interaction Prediction [[PDF](https://arxiv.org/abs/2412.13187)]

[arxiv 2025.01]  Interacted Object Grounding in Spatio-Temporal Human-Object Interactions [[PDF](https://arxiv.org/abs/2412.19542),[Page](https://github.com/DirtyHarryLYL/HAKE-AVA)] ![Code](https://img.shields.io/github/stars/DirtyHarryLYL/HAKE-AVA?style=social&label=Star)

[arxiv 2025.03]  UniHOPE: A Unified Approach for Hand-Only and Hand-Object Pose Estimation [[PDF](https://arxiv.org/pdf/2503.13303),[Page](https://github.com/JoyboyWang/UniHOPE_Pytorch)] ![Code](https://img.shields.io/github/stars/JoyboyWang/UniHOPE_Pytorch?style=social&label=Star)

[arxiv 2025.03] Reconstructing In-the-Wild Open-Vocabulary Human-Object Interactions [[PDF](https://arxiv.org/abs/2503.15898),[Page](https://wenboran2002.github.io/3dhoi/)] ![Code](https://img.shields.io/github/stars/wenboran2002/open-3dhoi?style=social&label=Star)

[arxiv 2025.08] HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model  [[PDF](https://arxiv.org/abs/2508.11350)]

[arxiv 2025.10]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


[arxiv 2025.10]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)


## environment 
[arxiv 2024.10] DepthSplat：Connecting Gaussian Splatting and Depth  [[PDF](https://arxiv.org/abs/2410.13862),[Page](https://haofeixu.github.io/depthsplat/)]

[arxiv 2024.10] Long-LRM: Long-sequence Large Reconstruction Model for Wide-coverage Gaussian Splats  [[PDF](https://arxiv.org/abs/2410.12781),[Page](https://arthurhero.github.io/projects/llrm/)]

[arxiv 2024.12] SCENIC: Scene-aware Semantic Navigation with Instruction-guided Control  [[PDF](https://arxiv.org/abs/2412.15664),[Page](https://virtualhumans.mpi-inf.mpg.de/scenic/)] 

[arxiv 2024.12]  ZeroHSI: Zero-Shot 4D Human-Scene Interaction [[PDF](https://arxiv.org/abs/2412.18600),[Page](https://awfuact.github.io/zerohsi/)] 

[arxiv 2025.03] SceneMI: Motion In-betweening for Modeling Human-Scene Interactions  [[PDF](https://arxiv.org/pdf/2503.16289),[Page](https://inwoohwang.me/SceneMI/)] 

[arxiv 2025.06]  GenHSI: Controllable Generation of Human-Scene Interaction Videos [[PDF](https://arxiv.org/pdf/2506.19840),[Page](https://kunkun0w0.github.io/project/GenHSI/)] ![Code](https://img.shields.io/github/stars/kunkun0w0/GenHSI?style=social&label=Star)

[arxiv 2025.09]  FantasyHSI: Video-Generation-Centric 4D Human Synthesis In Any Scene through A Graph-based Multi-Agent Framework [[PDF](https://arxiv.org/abs/2509.01232),[Page](https://fantasy-amap.github.io/fantasy-hsi/)] ![Code](https://img.shields.io/github/stars/Fantasy-AMAP/fantasy-hsi?style=social&label=Star)

[arxiv 2025.10]  Human3R: Everyone Everywhere All at Once [[PDF](https://arxiv.org/abs/2510.06219),[Page](https://fanegg.github.io/Human3R)] ![Code](https://img.shields.io/github/stars/fanegg/Human3R?style=social&label=Star)

[arxiv 2025.10]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)



## Capture 

[arxiv 2024.11] FreeCap: Hybrid Calibration-Free Motion Capture in Open Environments  [[PDF](https://arxiv.org/abs/2411.04469)]

[arxiv 2025.04]  EMO-X: Efficient Multi-Person Pose and Shape Estimation in One-Stage [[PDF](https://arxiv.org/abs/2504.08718)]

[arxiv 2025.04] CoMotion: Concurrent Multi-person 3D Motion  [[PDF](https://arxiv.org/abs/2504.12186),[Page](https://github.com/apple/ml-comotion)] ![Code](https://img.shields.io/github/stars/apple/ml-comotion?style=social&label=Star)


[arxiv 2025.10]   [[PDF](),[Page]()] ![Code](https://img.shields.io/github/stars/xxx?style=social&label=Star)
