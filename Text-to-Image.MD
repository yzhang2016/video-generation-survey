# Text-to-image Generation 

## AIGC Datasets 
 * **CommonCanvas** CommonCanvas: An Open Diffusion Model Trained with Creative-Commons Images [[PDF](https://arxiv.org/abs/2310.16825)]

    Feature: 70 millions of high-quality images with high-quality synthetic captions

* **JDB** JourneyDB: A Benchmark for Generative Image Understanding [[PDF](https://arxiv.org/abs/2307.00716), [Page](https://journeydb.github.io/)]

    Feature: 4 millions of Midjourney images 

* **DiffusionDB** DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models [[PDF](https://arxiv.org/abs/2210.14896), [Page](https://github.com/poloclub/diffusiondb)]

    Feature: 14 millions of Stable Diffusion images 
  



## Diffusion-based 

*[ICML 2021; OpenAI ] **---GLIDE---** GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models \[[PDF](https://arxiv.org/pdf/2112.10741.pdf), [Code](https://github.com/openai/glide-text2im)\]

[arxiv 2022; Microsoft] Vector Quantized Diffusion Model for Text-to-Image Synthesis \[[PDF](https://arxiv.org/pdf/2111.14822.pdf), [Code](https://github.com/cientgu/VQ-Diffusion)\]

[CVPR 2022; SUNY] Towards Language-Free Training for Text-to-Image Generation \[[PDF](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Towards_Language-Free_Training_for_Text-to-Image_Generation_CVPR_2022_paper.pdf), [Code](https://github.com/drboog/Lafite)\]

[ECCV 2022; UIUC ] Compositional Visual Generation with Composable Diffusion Models \[[PDF](https://arxiv.org/pdf/2206.01714.pdf), [Code](https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch)\]

[arxiv 2022; ByteDance] CLIP-GEN: Language-Free Training of a Text-to-Image Generator with CLIP \[[PDF](https://arxiv.org/pdf/2203.00386.pdf), [Code](https://github.com/HFAiLab/clip-gen)\]

*[arxiv 2022; OpenAI ]  **---DALL-E2---** Hierarchical Text-Conditional Image Generation with CLIP Latents \[[PDF](https://arxiv.org/pdf/2204.06125.pdf), Code\]

*[CVPR 2022] **---LDM---** High-Resolution Image Synthesis with Latent Diffusion Models \[[PDF](https://arxiv.org/pdf/2112.10752.pdf), [Code](https://github.com/CompVis/latent-diffusion)\]

*[arxiv 2022; Goole] **---Imagen---**  Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding \[[PDF](https://arxiv.org/pdf/2205.11487.pdf), Code\]

[arxiv 2023.01] Simple diffusion: End-to-end diffusion for high resolution images [[PDF](https://arxiv.org/pdf/2301.11093.pdf) ]

[arxiv 2023.07]SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesi [[PDF](https://arxiv.org/abs/2307.01952), [Page](https://github.com/Stability-AI/generative-models/tree/main)]

[arxiv 2023.09]Emu: Enhancing Image Generation Models Using Photogenic Needles in a Haystack [[PDF](https://arxiv.org/abs/2309.15807),[Page](https://ai.meta.com/research/publications/emu-enhancing-image-generation-models-using-photogenic-needles-in-a-haystack/)]

[tech report] DALLE-3: Improving Image Generation with Better Captions [[PDF](https://cdn.openai.com/papers/dall-e-3.pdf),[Page](https://openai.com/dall-e-3)]

[arxiv 2023.10]Matryoshka Diffusion Models [[PDF](https://arxiv.org/abs/2310.15111)]

[arxiv 2023.12]Kandinsky-3: Text-to-image diffusion model [[PDF](https://arxiv.org/abs/2312.03511),[Page](https://github.com/ai-forever/Kandinsky-3)]

[arxiv 2024.01]Taiyi-Diffusion-XL: Advancing Bilingual Text-to-Image Generation with Large Vision-Language Model Support [[PDF](https://arxiv.org/abs/2401.14688)]


[arxiv 2024.02]Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in Text-to-Image Generation [[PDF](https://arxiv.org/abs/2402.17245),[Page](https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic)]

[arxiv 2024.03]SD3: Scaling Rectified Flow Transformers for High-Resolution Image Synthesis [[PDF](https://arxiv.org/abs/2403.03206)]

[arxiv 2024.03]PixArt-Σ: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation [[PDF](https://arxiv.org/abs/2403.04692),[Page](https://pixart-alpha.github.io/PixArt-sigma-project/)]

[arxiv 2024.03]CogView3: Finer and Faster Text-to-Image Generation via Relay Diffusion [[PDF](https://arxiv.org/abs/2403.05121)]

[arxiv 2024.03]Multistep Consistency Models [[PDF](https://arxiv.org/abs/2403.06807)]

[arxiv 2024.04]CosmicMan: A Text-to-Image Foundation Model for Humans [[PDF](https://arxiv.org/abs/2404.01294),[Page](https://cosmicman-cvpr2024.github.io/)]

[arxiv 2024.05] Hunyuan-DiT : A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding [[PDF](https://tencent.github.io/HunyuanDiT/asset/Hunyuan_DiT_Tech_Report_05140553.pdf),[Page](https://github.com/Tencent/HunyuanDiT?tab=readme-ov-file)]


[arxiv 2024.05] Improving the Training of Rectified Flows[[PDF](https://arxiv.org/abs/2405.20320),[Page](https://github.com/sangyun884/rfpp)]

[arxiv 2024.05] [[PDF](),[Page]()]



## GAN/VAE/Transformer-based 

[ICML 2021; OpenAI ] Zero-Shot Text-to-Image Generation \[[PDF](https://arxiv.org/pdf/2102.12092.pdf), [Code 3](https://github.com/YoadTew/zero-shot-image-to-text)\]

[CVPR 2021; Google ] Cross-Modal Contrastive Learning for Text-to-Image Generation \[[PDF](https://arxiv.org/pdf/2101.04702.pdf), [Code](https://github.com/google-research/xmcgan_image_generation)\]

[KDD, 2021; Alibaba ] **---M6---**  M6 : A Chinese Multimodal Pretrainer \[[PDF](https://arxiv.org/pdf/2103.00823.pdf), Code\]

[arxiv 2021; Baidu] **---ERNIE-ViLG---** ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation \[[PDF](https://arxiv.org/pdf/2112.15283.pdf), Code\]

[ECCV 2022] **---DT2I---** DT2I: Dense Text-to-Image Generation from Region Descriptions \[[PDF](https://arxiv.org/pdf/2204.02035.pdf), Code\]

*[arxiv 2022; Meta ] **---Make-a-scene---** Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors \[[PDF](https://arxiv.org/pdf/2203.13131.pdf), [Code 3](https://github.com/CasualGANPapers/Make-A-Scene)\]

*[arxiv 2022; Google] **---Parti---** Scaling Autoregressive Models for Content-Rich Text-to-Image Generation \[[PDF](https://arxiv.org/pdf/2206.10789.pdf), Code\]

[arxiv 2022; Tsinghua ] **---CogView2---** CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers \[[PDF](https://arxiv.org/pdf/2204.14217.pdf), [Code](https://github.com/THUDM/CogView2)\]

[ECCV 2022; Microsoft] **---NÜWA--** NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion \[[PDF](https://arxiv.org/pdf/2111.12417.pdf), code \]

[NIPS 2022; Microsoft] **---NÜWA-Infinity--** NUWA-Infinity: Autoregressive over Autoregressive Generation for Infinite Visual Synthesis \[[PDF](https://arxiv.org/pdf/2207.09814.pdf), code \]

*[arxiv 2023.1; Google] **---Muse---** Muse: Text-To-Image Generation via Masked Generative Transformers [[PDF](https://arxiv.org/abs/2301.00704), [Page](https://muse-model.github.io/)]

[arxiv 2023.1]Attribute-Centric Compositional Text-to-Image Generation [[PDF](https://arxiv.org/pdf/2301.01413.pdf)]

[arxiv 2023.01]**---StyleGAN-T---** StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis [[PDF](https://arxiv.org/abs/2301.09515), [Page](https://sites.google.com/view/stylegan-t/)]

[arxiv 2023.03]Scaling up GANs for Text-to-Image Synthesis[[PDF](https://arxiv.org/abs/2303.05511), [Page](https://mingukkang.github.io/GigaGAN/)]

[arxiv 2023.07]Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning [[PDF](https://ai.meta.com/research/publications/scaling-autoregressive-multi-modal-models-pretraining-and-instruction-tuning/)]

[arxiv 2023.10]PIXART-α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis [[PDF](https://arxiv.org/abs/2310.00426), [Page](https://pixart-alpha.github.io/)]

[arxiv 2024.04]Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction [[PDF](https://arxiv.org/abs/2404.02905), [Page](https://github.com/FoundationVision/VAR)]



# Generation & Super-resolution 

[TPAMI 2022; Google ] Image Super-Resolution via Iterative Refinement \[[PDF](https://arxiv.org/pdf/2104.07636.pdf), [Code](https://github.com/Janspiry/Image-Super-Resolution-via-Iterative-Refinement)\]

[CVPR 2022; POSTECH ]Autoregressive Image Generation using Residual Quantization \[[PDF](https://arxiv.org/pdf/2203.01941.pdf), [Code](https://github.com/kakaobrain/rq-vae-transformer)\]

[SIGGRAPH 2022; Goolge ] **---Palette---** Palette: Image-to-Image Diffusion Models\[[PDF](https://arxiv.org/pdf/2111.05826.pdf), [Code](https://github.com/Janspiry/Palette-Image-to-Image-Diffusion-Models)\]

[arxiv 2022; Google] Cascaded Diffusion Models for High Fidelity Image Generation\[[PDF](https://arxiv.org/pdf/2106.15282.pdf), Code\]

[arxiv 2023.06]Designing a Better Asymmetric VQGAN for StableDiffusion [[PDF](https://arxiv.org/abs/2306.04632), [code](https://github.com/buxiangzhiren/Asymmetric_VQGAN)]

## Scene 
[arxiv 2022.12]Generative Scene Synthesis via Incremental View Inpainting using RGBD Diffusion Models [[PDF](https://arxiv.org/pdf/2212.05993.pdf)]

[arxiv 2022.12]Benchmarking Spatial Relationships in Text-to-Image Generation [[PDF](https://arxiv.org/pdf/2212.10015.pdf)]


# privacy 
[arxiv 2023.02, DeepMind]Differentially Private Diffusion Models Generate Useful Synthetic Images [[PDF](https://arxiv.org/abs/2302.13861)]


# Transformer Related 
*[ICLR 2022, Google]**---ViT-VQGAN---** Vector-quantized Image Modeling with Improved VQGAN [[PDF](https://arxiv.org/abs/2110.04627)]

*[CVPR 2021, HEIDELBERG] **---VQGAN---** Taming transformers for high-resolution image synthesis[[PDF](https://arxiv.org/abs/2012.09841), [Page](https://compvis.github.io/taming-transformers/), [code](https://github.com/CompVis/taming-transformers)]

*[arxiv 2022.02]MaskGIT: Masked Generative Image Transformer [[PDF](https://arxiv.org/pdf/2202.04200.pdf)]

# Diffusion related 
*[arxiv 2022.12] Scalable Diffusion Models with Transformers [[PDF](https://arxiv.org/abs/2212.09748), [Page](https://www.wpeebles.com/DiT)]

[arxiv 2024.01]Scalable High-Resolution Pixel-Space Image Synthesis with Hourglass Diffusion Transformers [[PDF](https://arxiv.org/abs/2401.11605),[Page](https://crowsonkb.github.io/hourglass-diffusion-transformers/)]


## Benchmark 
[arxiv 2023.10]DEsignBench: Exploring and Benchmarking DALL-E 3 for Imagining Visual Design [[PDF](https://arxiv.org/abs/2310.15144), [Page](https://design-bench.github.io/)]




## Study 
[arxiv 2023.02] A Pilot Evaluation of ChatGPT and DALL-E 2 on Decision Making and Spatial Reasoning [[PDF](https://arxiv.org/abs/2302.09068)]

[arxiv 2023.02] Exploring the Representation Manifolds of Stable Diffusion Through the Lens of Intrinsic Dimension [[PDF](https://arxiv.org/abs/2302.09301)]

[arxiv 2023.02]Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness [[PDF](https://arxiv.org/abs/2302.10893)]

[arxiv 2023.02] Unsupervised Discovery of Semantic Latent Directions in Diffusion Models [[PDF](https://arxiv.org/pdf/2302.12469.pdf)]

[arxiv 2023.03]A Prompt Log Analysis of Text-to-Image Generation Systems [[PDF](https://arxiv.org/abs/2303.04587)]

[arxiv 2023.10]A Picture is Worth a Thousand Words: Principled Recaptioning Improves Image Generation[[PDF](https://arxiv.org/abs/2310.16656)]

[arxiv 2023.10]Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation
[[PDF](https://arxiv.org/abs/2310.18235),[Page](https://google.github.io/DSG)]


### Image caption 
[arxiv 2023.07]SITTA: A Semantic Image-Text Alignment for Image Captioning [[PDF](https://arxiv.org/abs/2307.05591), [Page](https://github.com/ml-jku/semantic-image-text-alignment)]
